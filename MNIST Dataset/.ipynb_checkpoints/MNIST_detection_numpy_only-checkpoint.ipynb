{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577069d9",
   "metadata": {},
   "source": [
    "# Project\n",
    "- Build a complete neural network using Numpy. \n",
    "- Implement all the steps required to build a network - feedforward, loss computation, backpropagation, weight updates etc\n",
    "- Dataset: MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82ceec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c39437",
   "metadata": {},
   "source": [
    "- The MNIST dataset we use here is 'mnist.pkl.gz' which is divided into training, validation and test data.\n",
    "- We must read data first before making any progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68872b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8691a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='mnist.pkl.gz'\n",
    "file = gzip.open(filename,'rb')\n",
    "file.seek(0) # Position pointer to 0\n",
    "train_set, validation_set, test_set = pickle.load(file,encoding=\"latin1\")\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e86deac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([5, 0, 4, ..., 8, 4, 8], dtype=int64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b7dc9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01171875, 0.0703125 , 0.0703125 ,\n",
       "       0.0703125 , 0.4921875 , 0.53125   , 0.68359375, 0.1015625 ,\n",
       "       0.6484375 , 0.99609375, 0.96484375, 0.49609375, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.1171875 , 0.140625  , 0.3671875 , 0.6015625 ,\n",
       "       0.6640625 , 0.98828125, 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.98828125, 0.87890625, 0.671875  , 0.98828125, 0.9453125 ,\n",
       "       0.76171875, 0.25      , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19140625, 0.9296875 ,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.36328125,\n",
       "       0.3203125 , 0.3203125 , 0.21875   , 0.15234375, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.0703125 , 0.85546875, 0.98828125, 0.98828125,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.7734375 , 0.7109375 ,\n",
       "       0.96484375, 0.94140625, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.3125    , 0.609375  , 0.41796875, 0.98828125, 0.98828125,\n",
       "       0.80078125, 0.04296875, 0.        , 0.16796875, 0.6015625 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.0546875 ,\n",
       "       0.00390625, 0.6015625 , 0.98828125, 0.3515625 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54296875,\n",
       "       0.98828125, 0.7421875 , 0.0078125 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04296875, 0.7421875 , 0.98828125,\n",
       "       0.2734375 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13671875, 0.94140625, 0.87890625, 0.625     ,\n",
       "       0.421875  , 0.00390625, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31640625, 0.9375    , 0.98828125, 0.98828125, 0.46484375,\n",
       "       0.09765625, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17578125,\n",
       "       0.7265625 , 0.98828125, 0.98828125, 0.5859375 , 0.10546875,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0625    , 0.36328125,\n",
       "       0.984375  , 0.98828125, 0.73046875, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.97265625, 0.98828125,\n",
       "       0.97265625, 0.25      , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.1796875 , 0.5078125 ,\n",
       "       0.71484375, 0.98828125, 0.98828125, 0.80859375, 0.0078125 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15234375,\n",
       "       0.578125  , 0.89453125, 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.9765625 , 0.7109375 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09375   , 0.4453125 , 0.86328125, 0.98828125, 0.98828125,\n",
       "       0.98828125, 0.98828125, 0.78515625, 0.3046875 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.08984375, 0.2578125 , 0.83203125, 0.98828125,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.7734375 , 0.31640625,\n",
       "       0.0078125 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.0703125 , 0.66796875, 0.85546875,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.76171875,\n",
       "       0.3125    , 0.03515625, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21484375, 0.671875  ,\n",
       "       0.8828125 , 0.98828125, 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.953125  , 0.51953125, 0.04296875, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53125   , 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.828125  , 0.52734375, 0.515625  , 0.0625    , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a37e52bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 8, 4, 8], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c5710d",
   "metadata": {},
   "source": [
    "- ok, so feature set is train_set[0] and target is train_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b11fa8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,)\n",
      "(10000, 784) (10000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Shape of all the data \n",
    "print(train_set[0].shape,train_set[1].shape)\n",
    "print(validation_set[0].shape,validation_set[1].shape)\n",
    "print(test_set[0].shape,test_set[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929b5a9",
   "metadata": {},
   "source": [
    "- The target variable is like labels for each feature\n",
    "- So, instead of having single label we prefer one hot encoding where the target is of the dimension 10 x (length of target)\n",
    "- Why we are doing this? Ans - The output of the model is softmax output of length 10\n",
    "- We have to do this for all the datasets - training, validation and test\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f352e73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eb35021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to create one hot encoding\n",
    "\n",
    "def onehot(val_list):\n",
    "    length = len(val_list)\n",
    "    # Create a blank array \n",
    "    barray = np.zeros((10,length))\n",
    "    # Now have to put 1 at the correct place based on input\n",
    "    col = 0\n",
    "    for i in val_list:\n",
    "        barray[i][col] = 1.0 # put 1 according to the value and move to next column\n",
    "        col +=1\n",
    "    return barray\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab1793ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets test it with only 1 value \n",
    "arr = np.array([4]) # 4th  position will be filled (starting with 0)\n",
    "onehot(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94bd8bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing will all digits\n",
    "arr = np.array([5,1,2,0,4,8,6,9,7,3])\n",
    "onehot(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5742a88",
   "metadata": {},
   "source": [
    "- This is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "204a73b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,)\n"
     ]
    }
   ],
   "source": [
    "# Now time to mould target sets \n",
    "train_set_X = np.array(train_set[0][:])\n",
    "train_set_Y = np.array(train_set[1][:])\n",
    "print(train_set_X.shape,train_set_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e1a2073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50000)\n"
     ]
    }
   ],
   "source": [
    "# Need to transpose the feature set\n",
    "train_set_X = train_set_X.T\n",
    "print(train_set_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07578d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do the same for other sets\n",
    "validation_set_X = np.array(validation_set[0][:])\n",
    "validation_set_Y = np.array(validation_set[1][:])\n",
    "\n",
    "test_set_X = np.array(validation_set[0][:])\n",
    "test_set_Y = np.array(validation_set[1][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8674b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying one hot encoding to target variables\n",
    "train_set_Y = onehot(train_set_Y)\n",
    "validation_set_Y = onehot(validation_set_Y)\n",
    "test_set_Y = onehot(test_set_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6ed2202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50000) (10, 10000) (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(train_set_Y.shape,validation_set_Y.shape,test_set_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcf2d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "300fac00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28d35d70e80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets check a random image \n",
    "img_no = random.randint(0,1000)\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(train_set_X[:,img_no].reshape(28,28),cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c55e27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward - Activation Functions\n",
    "def sigmoid(x):\n",
    "    out = 1/(1+np.exp(-x))\n",
    "    prev_input =x\n",
    "    return out, prev_input  # layer out put h and hprev\n",
    "\n",
    "def relu(x):\n",
    "    out = np.maximum(x,0)\n",
    "    prev_input=x\n",
    "    assert(out.shape==x.shape)\n",
    "    return out,prev_input\n",
    "\n",
    "def softmax(x):\n",
    "    prev_input=x\n",
    "    out = np.exp(x) /(np.sum(np.exp(x),keepdims=True,axis=0))\n",
    "    return out, prev_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d440dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   -1]\n",
      " [   0    3]\n",
      " [-100  100]\n",
      " [  -4    4]] (array([[  2,   0],\n",
      "       [  0,   3],\n",
      "       [  0, 100],\n",
      "       [  0,   4]]), array([[   2,   -1],\n",
      "       [   0,    3],\n",
      "       [-100,  100],\n",
      "       [  -4,    4]]))\n",
      "[[   2   -1]\n",
      " [   0    3]\n",
      " [-100  100]\n",
      " [  -4    4]] (array([[8.78878243e-01, 1.36853947e-44],\n",
      "       [1.18943236e-01, 7.47197234e-43],\n",
      "       [4.42477874e-45, 1.00000000e+00],\n",
      "       [2.17852136e-03, 2.03109266e-42]]), array([[   2,   -1],\n",
      "       [   0,    3],\n",
      "       [-100,  100],\n",
      "       [  -4,    4]]))\n",
      "[[   2   -1]\n",
      " [   0    3]\n",
      " [-100  100]\n",
      " [  -4    4]] (array([[8.80797078e-01, 2.68941421e-01],\n",
      "       [5.00000000e-01, 9.52574127e-01],\n",
      "       [3.72007598e-44, 1.00000000e+00],\n",
      "       [1.79862100e-02, 9.82013790e-01]]), array([[   2,   -1],\n",
      "       [   0,    3],\n",
      "       [-100,  100],\n",
      "       [  -4,    4]]))\n",
      "[2.68811714e+43]\n"
     ]
    }
   ],
   "source": [
    "z=np.array([2,-1,0,3,-100,100,-4,4]).reshape(4,2)\n",
    "print(z,relu(z))\n",
    "print(z,softmax(z))\n",
    "print(z,sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f5ca28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_count = [784,45,22,10] # Number of neurons in each count\n",
    "\n",
    "def init_parameters(list_neurons):\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(list_neurons)\n",
    "    \n",
    "    for i in range(1,L):\n",
    "        parameters[\"W\"+str(i)] = np.random.randn(list_neurons[i], list_neurons[i-1]) * 0.1\n",
    "        parameters['b' + str(i)] = np.zeros((list_neurons[i], 1))  \n",
    "        \n",
    "        assert(parameters['W' + str(i)].shape == (list_neurons[i], list_neurons[i-1]))\n",
    "        assert(parameters['b' + str(i)].shape == (list_neurons[i], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cbbde074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.02477442  0.01255666 -0.0366612  ... -0.00877862 -0.1307876\n",
      "  -0.10821573]\n",
      " [ 0.09278642 -0.03603296  0.04534459 ... -0.00056192  0.09561777\n",
      "   0.0366986 ]\n",
      " [ 0.19661937  0.14705427  0.10742961 ... -0.10857642 -0.09547231\n",
      "  -0.09690939]\n",
      " ...\n",
      " [-0.01742217  0.06095885 -0.01872798 ... -0.18210334 -0.08465396\n",
      "   0.04719205]\n",
      " [ 0.0214537  -0.05133838  0.04793492 ... -0.09233563 -0.17476924\n",
      "  -0.12192184]\n",
      " [ 0.09387316 -0.1448085  -0.06580721 ... -0.09010165 -0.04325293\n",
      "  -0.03749854]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-9.72807768e-02 -5.60932701e-02 -1.06751829e-02  1.13545386e-01\n",
      "   8.58594950e-02 -3.30799580e-02  1.32830912e-01 -1.70438704e-02\n",
      "  -1.06116542e-01 -4.35559676e-02  8.45691400e-02 -7.69627111e-02\n",
      "  -3.93939586e-02 -1.02329349e-01 -5.29320138e-02  4.09554725e-02\n",
      "  -1.09187653e-01 -3.97998364e-02  1.08080345e-01 -1.06552189e-01\n",
      "   4.87528725e-02  8.79432673e-02  2.77915283e-02 -1.08403634e-01\n",
      "   3.60226362e-02  6.22410144e-02  7.66082704e-02 -6.12911012e-02\n",
      "   7.25528908e-02 -5.53315712e-02  9.86582996e-02 -1.64563717e-01\n",
      "  -2.83199035e-02  1.02897513e-01  4.68650673e-02 -3.01568025e-02\n",
      "   4.24733886e-02  8.22229195e-02 -3.65085998e-02 -1.37844719e-01\n",
      "   1.02691073e-01  3.14931308e-03  2.30072664e-01  8.88373067e-03\n",
      "  -1.68854725e-02]\n",
      " [-2.09729264e-02 -3.41407140e-02 -1.37753606e-01  4.10455510e-02\n",
      "   7.09293909e-02  3.61595645e-02 -3.01547280e-02  1.01863251e-01\n",
      "  -7.39672189e-02 -2.03445853e-01  1.96764511e-01 -9.68467920e-02\n",
      "   6.81316589e-02  7.61121626e-02  7.59402630e-02  5.26158810e-02\n",
      "   1.04959565e-01  6.27689046e-02  2.90686996e-02  1.27959738e-01\n",
      "  -1.05527247e-01 -4.48710920e-02 -4.02898009e-02 -5.70393748e-02\n",
      "  -5.37835636e-02  2.26135343e-01 -1.99534671e-01  1.61750568e-01\n",
      "  -5.42542438e-02 -1.05973840e-02  8.09340448e-02 -1.44502132e-01\n",
      "   6.06272511e-02  1.66628266e-01 -3.29944370e-02 -5.93228149e-02\n",
      "  -1.06253249e-01  1.62379545e-01 -1.75093026e-01  8.82881883e-02\n",
      "   2.04047313e-02 -7.12545649e-02 -6.27526991e-02  5.09923385e-02\n",
      "   3.88987391e-02]\n",
      " [-6.55808014e-02 -8.65114837e-02  1.20063490e-01 -5.42186530e-02\n",
      "  -4.72219538e-02  1.19878938e-01 -5.20649568e-02 -2.35775920e-02\n",
      "  -1.82671653e-02  2.81882513e-02  4.80662551e-02 -3.46805044e-02\n",
      "  -7.22286408e-02 -2.69374772e-02 -7.97002748e-02  6.80836905e-02\n",
      "  -1.26015655e-01  7.99794299e-02 -8.82148008e-02  1.36663162e-01\n",
      "   1.27838084e-01  9.45511966e-02  5.65936601e-02 -1.05978826e-01\n",
      "   8.13598823e-02  6.47867011e-03 -6.65323487e-02 -7.25185559e-03\n",
      "   3.72290638e-03  1.77487317e-01 -4.21768637e-02  4.45717226e-02\n",
      "   1.57191604e-03  1.27276462e-02 -7.04330110e-02 -8.10075313e-02\n",
      "   1.43897800e-01  1.13580132e-01  1.14027729e-01 -1.63512324e-01\n",
      "  -1.06618586e-01 -8.38486609e-02 -1.44990358e-01  4.06922381e-02\n",
      "  -1.22743791e-01]\n",
      " [ 4.14022161e-02 -1.14738440e-01  1.58811495e-01 -5.99866438e-03\n",
      "  -1.19641528e-01  1.63698610e-01 -1.79509530e-01  1.53487369e-01\n",
      "   3.49621054e-02 -1.18685576e-01 -1.90040151e-01 -2.61201218e-01\n",
      "  -6.98900981e-02 -1.25789271e-01 -2.30705482e-02  6.07487996e-02\n",
      "   2.68127200e-02  5.74546022e-02 -5.77225801e-02 -2.93856210e-02\n",
      "   5.95400524e-02  4.19287365e-02 -4.95720269e-02  1.52556071e-01\n",
      "  -1.11354727e-01 -5.61563529e-02 -2.07870303e-01  6.84092569e-02\n",
      "   6.08616562e-02  2.00179043e-02 -3.94748367e-02  2.42029832e-01\n",
      "   5.93301604e-03  7.07826798e-03  1.59976877e-01  1.11429393e-01\n",
      "   6.11510425e-02 -1.70663128e-01 -2.41234518e-01 -1.24861178e-03\n",
      "   3.09831289e-02 -4.35615069e-02  1.37141997e-01  5.41358344e-04\n",
      "   5.22741587e-02]\n",
      " [ 1.12269771e-01  1.05684343e-01  6.23313758e-02  9.06561027e-02\n",
      "  -2.09162879e-01 -5.05782438e-02  2.06561101e-02  2.71981147e-02\n",
      "   2.16377599e-01 -1.94810196e-01 -1.60284661e-02  5.66418802e-02\n",
      "  -1.55734025e-01 -4.69327853e-02 -4.52884401e-02 -1.04387990e-01\n",
      "  -2.22997246e-01  2.96010664e-02  1.53961046e-01 -1.22774768e-01\n",
      "   3.37807582e-03 -1.48129715e-01 -1.54163708e-01 -7.70479848e-02\n",
      "   3.64954578e-02  1.35769549e-01  9.94930887e-02  2.59313980e-01\n",
      "  -1.38252189e-01 -1.57778326e-02 -4.47882029e-02  5.27479941e-03\n",
      "   6.68096310e-03  3.25333843e-02 -3.63012919e-02  3.81462541e-02\n",
      "   4.50665381e-02 -1.28925134e-01 -1.81337261e-01 -1.10325324e-01\n",
      "   9.94149340e-02 -8.59036111e-02 -1.29431117e-01 -7.12205092e-02\n",
      "   6.63228562e-02]\n",
      " [ 8.70189878e-02  3.64743776e-02 -1.80412130e-01  4.83942573e-02\n",
      "   7.04803857e-02 -3.09843722e-02  1.20216392e-01  5.52924940e-02\n",
      "  -1.16983540e-01 -6.32138959e-02  2.12731363e-01 -4.90060792e-04\n",
      "  -7.11408410e-02  1.17862812e-01  4.46711077e-02 -7.96921819e-02\n",
      "   6.45691507e-02 -8.15891752e-02  1.24953588e-02 -6.31362837e-03\n",
      "  -5.85670667e-03  1.18032345e-01  8.77138933e-02  6.36469171e-02\n",
      "  -6.46741604e-02 -2.97334010e-01  1.33280330e-02 -1.92940299e-01\n",
      "   1.69496311e-01 -3.03548723e-02 -9.07183944e-02 -2.34627022e-02\n",
      "  -1.65305680e-01 -1.44849247e-02  7.57786962e-02 -1.42546634e-03\n",
      "   2.79690246e-02 -3.61125183e-02 -1.58026241e-01  1.82503138e-01\n",
      "  -1.90804091e-01 -6.08640627e-02 -2.44902325e-03  2.50493656e-02\n",
      "   2.49164077e-01]\n",
      " [-2.02451432e-02 -2.03483235e-01  2.45998919e-02 -4.30329040e-02\n",
      "  -1.77086592e-02  1.01907160e-01 -7.28525225e-02  1.29596979e-01\n",
      "  -1.23745603e-01  6.20393536e-02  3.53217014e-02  9.34024198e-02\n",
      "  -1.07280401e-01 -1.44543490e-01  2.20684849e-01  1.22493177e-01\n",
      "   1.13296489e-01  4.00575673e-02 -1.28202861e-01  9.76268952e-02\n",
      "  -8.40290803e-02  5.28953982e-02 -9.11945632e-02  3.06129874e-03\n",
      "  -1.22205835e-01  5.08923808e-02  7.62554176e-02  1.26901793e-01\n",
      "  -1.70740599e-01  8.62715828e-02 -7.33175785e-02 -1.16922281e-01\n",
      "  -9.22265084e-03 -1.63778804e-02  4.63708698e-02  8.71968937e-02\n",
      "  -3.54201994e-02 -2.41653787e-01 -7.97047468e-03  5.11661418e-03\n",
      "  -1.72659312e-01  9.66365193e-02  9.04279030e-02 -1.83013652e-02\n",
      "  -5.35032199e-02]\n",
      " [ 1.57207754e-01 -7.97440240e-02 -1.82726603e-02 -2.66953045e-02\n",
      "   9.07159264e-02 -1.03087286e-01  5.49948531e-03 -5.89602510e-02\n",
      "  -5.84907797e-02  3.54551249e-02 -4.57317527e-02  1.70059404e-01\n",
      "  -7.43093248e-02  5.75424550e-02 -3.05188042e-02 -1.41408353e-01\n",
      "  -1.97099836e-02 -1.11477370e-02  9.08652532e-02 -1.46469345e-02\n",
      "  -8.60351833e-02 -7.81131698e-02 -8.26456086e-03 -4.87603095e-02\n",
      "  -3.14990401e-04 -1.25750479e-02  4.41929543e-02  3.90410304e-02\n",
      "   1.00728858e-01  1.08323478e-02  9.06332103e-02 -4.07856157e-02\n",
      "   1.00996003e-01 -2.94592717e-03  9.57987302e-02 -1.33608281e-01\n",
      "  -1.06676081e-01 -1.72773123e-01  1.10077626e-02  2.22204326e-02\n",
      "  -1.60311650e-01  8.52134036e-02  1.32212608e-03 -1.38790989e-02\n",
      "   8.56979582e-02]\n",
      " [ 2.18374374e-02 -2.29954117e-01  2.50225548e-01  6.88606303e-02\n",
      "   7.68380641e-02 -2.07182451e-02 -1.02109951e-01  1.57842468e-01\n",
      "   1.10680393e-02 -2.06826826e-02 -1.39272053e-01 -3.46153170e-02\n",
      "   3.18817692e-02  7.31320344e-02  1.48159844e-01 -1.68385599e-02\n",
      "   7.68968922e-02 -1.92815566e-01  7.86564401e-02 -2.01672797e-01\n",
      "   7.78643486e-02  2.85243070e-01 -1.40843402e-01  2.66737292e-01\n",
      "   3.69086679e-02  4.35545052e-02  1.23225596e-01  4.65562044e-02\n",
      "   1.68608871e-01 -1.32670746e-01  9.39028688e-03  8.58073917e-02\n",
      "   9.64328929e-02 -3.58764492e-02  6.30326620e-02 -6.16695311e-02\n",
      "   6.44509090e-03  2.59831744e-01 -4.41233814e-02  2.42175610e-02\n",
      "  -1.24597039e-01 -8.77674410e-05 -6.77223322e-02  6.11216962e-02\n",
      "  -1.42574393e-01]\n",
      " [-8.23551987e-02  2.10534674e-02  4.33530516e-02  6.77441135e-02\n",
      "   1.23550712e-01 -1.29563914e-01 -1.48527086e-02 -3.99877615e-02\n",
      "   1.03460526e-01 -7.27215986e-02  3.02990378e-02 -1.15937503e-01\n",
      "   5.29500993e-02  4.81406353e-02 -6.48562324e-02 -5.47484563e-02\n",
      "  -3.37672403e-03  1.01479968e-01 -1.69030616e-01 -7.85495909e-03\n",
      "   9.48766022e-02 -7.10496992e-03 -4.45911032e-02  5.52146277e-02\n",
      "   1.45943827e-01  4.65655442e-02  3.24494871e-02 -2.56612060e-02\n",
      "   1.48239748e-01  7.47778699e-02 -2.54616528e-02 -5.99316046e-03\n",
      "   1.40964120e-02 -2.44401454e-01  1.17222654e-01  2.68214141e-03\n",
      "  -2.66418875e-02 -4.03192220e-02  3.41614296e-02 -3.53337406e-02\n",
      "   1.14686344e-01  2.00410363e-02  2.11845823e-02  6.20095299e-02\n",
      "   6.88658620e-02]\n",
      " [ 1.63024486e-01 -4.11016919e-02  8.36004168e-02  3.58432962e-02\n",
      "  -1.64360934e-01  4.34973679e-02  8.60565566e-02  4.65837913e-02\n",
      "  -9.18686167e-02  1.01400523e-01 -2.73140289e-02 -1.00480679e-01\n",
      "   7.34885113e-03 -1.40284686e-02 -1.13685523e-01  8.02882410e-02\n",
      "   1.57898190e-02  6.24842494e-02  8.57318003e-02 -4.41019200e-04\n",
      "  -3.47966211e-02  1.45408962e-01 -1.50590388e-01 -8.52617751e-02\n",
      "  -7.51063197e-02 -5.55374351e-02  5.25810051e-02 -1.44514973e-02\n",
      "   7.37795314e-02 -1.36920871e-02  2.36014313e-01  1.80057799e-03\n",
      "  -1.57049972e-01  5.58828102e-02  1.63683090e-01  8.22709233e-02\n",
      "   4.06065724e-02  1.32752775e-01  1.42215326e-02  8.64975652e-02\n",
      "  -5.73684256e-03  2.45527127e-04  9.28528603e-02  1.23917973e-01\n",
      "   1.50129893e-02]\n",
      " [-1.88422983e-01  3.15819836e-01 -4.25419805e-02 -8.32971873e-02\n",
      "   9.73143456e-02  8.85978248e-02  1.53062465e-01 -4.83436933e-03\n",
      "   8.23976723e-02 -6.66409899e-02 -1.09930217e-01  7.04438911e-02\n",
      "   3.46593859e-02  9.65775053e-02 -1.23699364e-01 -7.91973577e-02\n",
      "   2.53710495e-02 -7.22138956e-04  2.11390022e-01 -1.02914809e-01\n",
      "   1.23593278e-01 -1.45293446e-01 -1.55979820e-02  1.69860112e-01\n",
      "   4.15004438e-02 -1.20217807e-01  3.08613758e-03 -5.78910358e-02\n",
      "  -1.72742314e-01 -3.53411994e-02  4.26264039e-02 -8.95719522e-02\n",
      "  -2.87974148e-02 -1.45681119e-01  2.80120836e-01 -6.34363263e-02\n",
      "   9.64958098e-02 -1.56612087e-01  1.18824463e-01  1.38560730e-02\n",
      "  -1.32534680e-02 -2.93129574e-02 -9.00342609e-02  7.03859665e-02\n",
      "  -7.96308352e-02]\n",
      " [ 6.40006917e-03  4.53070202e-02  1.56190342e-01  4.29832904e-02\n",
      "   4.61484102e-02  8.00945473e-02  3.11008456e-03  1.35897926e-02\n",
      "   1.71795366e-01  7.20688807e-02  6.81913710e-02  7.09076470e-02\n",
      "   4.34322169e-02  1.30215698e-01  7.27689910e-02  5.95262069e-02\n",
      "   2.11655715e-01  5.65896550e-02 -7.28150923e-03  9.34405894e-03\n",
      "  -2.77201002e-02  4.56345359e-02  2.88057138e-02 -3.00785382e-02\n",
      "  -6.10616362e-02  1.47699515e-02 -1.25367153e-01  5.57798801e-02\n",
      "  -2.39492673e-01  1.33067556e-01  7.03365534e-04 -1.28618346e-01\n",
      "   1.56317690e-01  7.93333826e-03  4.64974435e-02 -1.21910403e-02\n",
      "   6.01566954e-02  9.07350736e-03 -8.13249788e-02  3.84882254e-02\n",
      "  -4.96756577e-02 -1.95616975e-01  1.59275037e-01  9.68476988e-02\n",
      "   3.16571809e-02]\n",
      " [-5.80290996e-02 -1.10060447e-01  4.33254336e-03  6.99287423e-02\n",
      "   7.95790326e-02 -1.04628846e-03 -1.32055265e-01 -2.56039688e-02\n",
      "   1.61131945e-02  1.16966318e-01  1.38030305e-01  3.92774353e-02\n",
      "   2.79655697e-01  1.80120369e-03  4.78977224e-02 -4.20654091e-02\n",
      "   2.93629429e-02 -4.72914877e-02  2.36769455e-01 -1.44026625e-01\n",
      "   2.61070540e-02 -1.60051510e-04  6.48294889e-02  5.93697218e-03\n",
      "  -4.21793967e-02 -2.19029184e-01 -1.20791651e-01  1.94579463e-01\n",
      "  -1.72437982e-01 -2.34799978e-02  1.81644491e-01  6.15645316e-02\n",
      "   1.27133534e-01  1.60730316e-01  9.31033825e-02  4.29173431e-03\n",
      "  -1.08863855e-02  9.05216827e-04  1.24051496e-01 -1.46137805e-01\n",
      "   1.25474742e-01 -4.11492529e-02 -7.04254245e-02 -1.47762771e-02\n",
      "  -1.21470172e-02]\n",
      " [-6.41665999e-02 -9.90755171e-03  6.47829126e-02 -1.23957646e-01\n",
      "   3.63956997e-02  2.16765497e-01  1.04506735e-01  3.61666068e-02\n",
      "   2.22540616e-01 -2.54031756e-01  1.90084193e-01 -2.53514822e-01\n",
      "   1.94632444e-01  1.52285010e-02  7.91575720e-02 -1.31520682e-02\n",
      "  -1.14833229e-01  2.55198318e-02 -9.92584786e-02  1.54995624e-01\n",
      "  -8.56952317e-02 -1.32154709e-01 -4.43926291e-02  1.43914629e-01\n",
      "   3.75677812e-02  1.81460603e-02  1.05914048e-01 -4.17935762e-02\n",
      "  -4.88183744e-02  2.86158079e-02  7.01429893e-02  1.85061906e-01\n",
      "  -1.28433133e-01 -1.34682997e-01  7.04159054e-02 -3.28063979e-02\n",
      "  -6.27224579e-02  7.16127232e-02  8.63384442e-02 -2.80837867e-02\n",
      "  -6.27703753e-02 -1.35147877e-03 -2.65073857e-02 -3.51821829e-02\n",
      "   1.76079339e-02]\n",
      " [-7.26230873e-04  1.20491713e-01  9.84686188e-02  9.68401027e-02\n",
      "   9.98035459e-02  6.94631736e-02 -2.39338053e-01 -1.63583945e-01\n",
      "  -8.90764728e-02  3.10879583e-02  8.24602715e-02  8.23539346e-02\n",
      "   1.65193419e-01 -1.91333334e-02  1.99924647e-01  1.39077428e-01\n",
      "  -2.38561460e-01  1.07979550e-01  8.08657039e-02 -3.63907777e-02\n",
      "  -1.39637082e-02  1.02900643e-01 -1.80849528e-01  2.51082975e-02\n",
      "  -1.05660454e-01  5.36064736e-02 -2.55003908e-02  9.95521107e-02\n",
      "  -1.83126750e-02  2.20083957e-02 -2.90292148e-02  1.04763403e-01\n",
      "  -8.79132254e-02 -7.33023653e-02  1.65262368e-01 -5.62450652e-02\n",
      "  -4.40447342e-02 -1.16825436e-02 -7.07756412e-02  8.84059304e-02\n",
      "   8.63741216e-03 -1.15475989e-01  1.15825393e-01 -1.83510342e-02\n",
      "  -3.00782581e-02]\n",
      " [ 4.30667370e-02  2.99910434e-02 -1.40326384e-01 -3.75067274e-03\n",
      "  -7.55311036e-02 -1.38365457e-01  2.60106321e-01 -1.58516702e-01\n",
      "   5.22474836e-02 -6.63474831e-02 -7.57245099e-02  9.11846573e-02\n",
      "  -5.86611427e-02  1.69844551e-03 -1.34833145e-01 -1.47898895e-01\n",
      "   2.88969090e-02  3.44636706e-02 -1.28376967e-01 -1.22569133e-01\n",
      "   1.08743398e-01  5.00125661e-03  8.67539814e-02  2.03565633e-01\n",
      "  -1.25678595e-01  1.67061657e-01 -1.24767193e-01  1.03000605e-02\n",
      "  -9.65507514e-02 -5.80254179e-02  6.19761808e-02  1.90638837e-03\n",
      "  -5.42316515e-02  1.51124114e-01 -8.99998827e-03  5.20381626e-02\n",
      "   4.92929422e-02  1.72641166e-01 -2.73175711e-01 -9.85955565e-02\n",
      "   7.76908680e-02 -1.48829640e-02  2.20377885e-02  9.16267351e-02\n",
      "  -1.93971452e-02]\n",
      " [-2.09025029e-03  1.21576624e-01 -2.03503873e-01  1.03436404e-04\n",
      "  -7.65191671e-02 -7.44921713e-02  1.74978182e-01 -3.83478165e-03\n",
      "   8.20698379e-02 -1.60225950e-01 -1.04949514e-01  7.45692940e-03\n",
      "  -3.69243832e-02  4.55831353e-02 -2.53255829e-02  2.80496899e-02\n",
      "  -8.47141921e-02 -3.16961845e-02 -1.28564154e-01 -2.08950126e-01\n",
      "  -3.05681674e-02  5.06975908e-02  3.18920708e-02  1.68876677e-02\n",
      "  -4.59194307e-02 -2.95412237e-02  9.11693742e-02  3.63253486e-02\n",
      "  -1.23659609e-01  1.93585669e-02  1.82381511e-02 -1.22185550e-02\n",
      "  -3.78755198e-02  2.54067861e-02 -9.46314886e-02  2.20490858e-02\n",
      "   2.67997954e-02 -7.13727012e-02 -4.82551091e-03  1.27204617e-04\n",
      "  -3.18459061e-02 -6.71465618e-02  3.49712183e-01 -5.34365401e-03\n",
      "   6.12988991e-02]\n",
      " [-1.65222432e-02 -9.19668779e-03 -1.01467655e-01  1.37205841e-01\n",
      "  -1.66068538e-01 -6.20593958e-02  4.13734642e-02 -1.04764368e-01\n",
      "   7.86956785e-02 -1.35870019e-01  3.81823117e-02  2.80932133e-02\n",
      "  -1.92149659e-02 -1.29483432e-01 -1.72416545e-01  3.18759590e-02\n",
      "  -7.30049446e-02  9.08538473e-02  1.26769595e-02  5.04613742e-02\n",
      "  -1.19940223e-01 -1.18658881e-01 -6.01070127e-02 -5.42487851e-02\n",
      "   2.08768613e-01  2.37920203e-03  4.84475407e-02 -1.74449042e-01\n",
      "  -1.46030086e-01  1.66947011e-01 -5.40078788e-02  1.24307339e-02\n",
      "   3.08817581e-02 -1.06642747e-01 -1.27887493e-01 -5.88445509e-03\n",
      "   7.73148481e-02 -8.08180629e-02 -1.69939983e-01  7.16541557e-02\n",
      "   1.06470635e-01 -1.29036843e-01  2.23604348e-02 -5.73166073e-02\n",
      "   1.12127218e-01]\n",
      " [ 1.57814002e-01 -1.43673480e-02  1.65567952e-01 -1.03589890e-01\n",
      "   8.27867576e-02  1.19506474e-01  2.23269240e-01  7.33530375e-02\n",
      "  -2.00555687e-01 -1.84069191e-01  1.95924946e-02 -1.20933014e-01\n",
      "  -5.83823438e-02  2.16736499e-02 -1.94306187e-01  2.02227007e-01\n",
      "  -4.92814560e-02  2.17803861e-02  3.88355404e-02  1.59053434e-02\n",
      "   8.74013751e-02 -9.67147677e-02 -1.36893490e-01  1.00748316e-01\n",
      "  -1.49764577e-01 -5.67860484e-02  2.46502774e-02  6.80123542e-02\n",
      "   9.20010320e-02 -1.30043192e-01  1.06630108e-01  6.70477465e-02\n",
      "   3.86419020e-02  1.06422965e-02 -1.61189514e-02  4.27125845e-02\n",
      "   8.08039887e-03 -5.79680849e-02  5.59511195e-02  5.22818541e-02\n",
      "   1.28877027e-01 -1.69737373e-02 -4.31241862e-02  1.61992222e-01\n",
      "   1.38671406e-01]\n",
      " [ 8.79347118e-03 -1.35854001e-01  2.63105389e-02  2.28995480e-01\n",
      "   1.14059193e-01  2.25393915e-01  7.85214004e-02  5.38115003e-02\n",
      "   8.23238765e-02 -1.58591152e-02  1.52668417e-02 -3.39113652e-02\n",
      "  -1.14381731e-01 -6.41849630e-02  2.24926088e-01 -1.39057613e-01\n",
      "  -3.48915859e-02  1.42682275e-02 -9.25640867e-02 -2.91041507e-02\n",
      "   3.07159520e-02  7.74195014e-02 -1.16381768e-01 -6.83338542e-02\n",
      "   1.53470017e-02 -1.15501982e-01  1.02246225e-01  8.46729288e-02\n",
      "  -5.17593144e-02 -6.20448282e-02  7.36169514e-02  2.61317882e-05\n",
      "  -4.94864550e-02 -3.02474813e-02 -4.39747696e-02  4.23440807e-02\n",
      "  -1.48648541e-01 -8.69367039e-02 -1.14502413e-02 -3.33553244e-02\n",
      "  -5.60032654e-02  1.11409194e-01  8.44062809e-03 -9.81014563e-02\n",
      "   1.48098604e-01]\n",
      " [ 1.08492908e-01  2.53376271e-01 -2.08807755e-02 -8.77619822e-02\n",
      "   1.62796047e-01  1.76785144e-02 -1.42180205e-01 -5.19288929e-02\n",
      "  -1.59402573e-01 -2.67217910e-02  1.51978813e-02  6.85606464e-02\n",
      "   4.37674849e-03 -3.45455967e-02  4.17527140e-02  1.16750799e-01\n",
      "   3.07310068e-02 -5.34568691e-03  3.26766251e-02  4.65743526e-02\n",
      "   2.86855080e-01 -5.41844184e-02  7.55084308e-02 -6.87648466e-02\n",
      "  -1.11129009e-01 -1.26813352e-02  2.30764701e-01 -1.18424109e-01\n",
      "  -7.74651045e-02 -4.81976793e-02 -1.99386207e-02 -4.82495262e-02\n",
      "  -2.98423579e-02 -1.79952783e-01 -8.07973294e-02  1.33241060e-01\n",
      "   8.32814504e-02  8.13430826e-02 -3.99475165e-02  7.13003767e-02\n",
      "   5.98632881e-02 -3.73144405e-02  3.05462431e-02 -5.40842956e-03\n",
      "   4.11733358e-02]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W3 = [[-0.26877219  0.06216644 -0.10964597  0.05672867  0.05729129 -0.00512988\n",
      "  -0.0470502   0.00724812  0.03222766  0.14424377 -0.04746525  0.1050536\n",
      "  -0.03179447  0.0824625   0.08488934  0.08400656  0.07139554  0.13901\n",
      "   0.12955489  0.05694621  0.10310227  0.08788276]\n",
      " [-0.0237179   0.04608168  0.01747946  0.02747001  0.14475305  0.01789365\n",
      "  -0.02300449  0.14579558  0.07819306 -0.11658383 -0.04642248 -0.12383482\n",
      "   0.10176332  0.01602232 -0.0574096   0.0243065  -0.17470041 -0.13717385\n",
      "  -0.20800531  0.07086334  0.11022764  0.01947718]\n",
      " [-0.18721632 -0.10772969 -0.03019308 -0.05907043 -0.04406846 -0.05172301\n",
      "  -0.2687664  -0.02227212  0.00458011  0.20728006  0.05657303  0.09651088\n",
      "   0.10748625 -0.2352599   0.04259319 -0.03206221  0.05153181 -0.00452749\n",
      "  -0.01904648  0.05950858 -0.08951839 -0.05533846]\n",
      " [-0.05265424 -0.19103625 -0.11914807 -0.05937264 -0.05448466  0.0789454\n",
      "   0.08647624  0.06549636 -0.05512866  0.03772753 -0.04879097  0.11541233\n",
      "   0.11301812  0.12277721  0.05000576  0.0527235  -0.00346981  0.08895851\n",
      "  -0.03291357  0.03605151  0.04832431 -0.07931467]\n",
      " [ 0.08893819 -0.00957869  0.25126446  0.06556382 -0.02200913  0.02320989\n",
      "  -0.10464697 -0.07814888 -0.08706819 -0.15479976  0.04412538 -0.05353931\n",
      "   0.08292965  0.00471712 -0.03360342 -0.10271273 -0.00938385 -0.11814882\n",
      "  -0.01037757  0.05048048 -0.00656244 -0.04947935]\n",
      " [-0.10322193 -0.01472556  0.00172179 -0.0327493   0.09505311  0.06562264\n",
      "  -0.03220903 -0.00761471 -0.12921019 -0.1745683  -0.10660529  0.04121589\n",
      "   0.04543832  0.04780531 -0.08635242  0.00284104 -0.2101267   0.11645265\n",
      "   0.0340585   0.10646056 -0.06620822  0.04291495]\n",
      " [-0.27425887  0.09915125 -0.0193486  -0.10173946 -0.11382998  0.04086272\n",
      "   0.00482905  0.04856943 -0.03095923 -0.09885875  0.1268325   0.00722124\n",
      "  -0.19216715 -0.00849514  0.20956348 -0.10329732 -0.13227214 -0.08458162\n",
      "  -0.15469059  0.13383535 -0.04513767  0.10695345]\n",
      " [ 0.26916692 -0.02057023 -0.11300713 -0.07712011 -0.13314542  0.0170427\n",
      "   0.06326556  0.16326739 -0.01857996 -0.04685393 -0.0073793  -0.13191987\n",
      "  -0.09034486  0.08726895  0.14145011 -0.04151351  0.01277011  0.00863778\n",
      "  -0.07818237 -0.14083089  0.187896   -0.26544253]\n",
      " [-0.23882663  0.0520539   0.0646486  -0.09719851  0.02665498 -0.08021705\n",
      "   0.00820293 -0.18387859 -0.05382432 -0.04397677 -0.21576069 -0.06638302\n",
      "  -0.02640482  0.05114056  0.15370567  0.01920395  0.01098646 -0.02255764\n",
      "  -0.21334242 -0.1402774  -0.05270704 -0.11473249]\n",
      " [ 0.0432327  -0.07805819  0.12187285 -0.10004375 -0.04094462  0.01341823\n",
      "  -0.00849125 -0.04632521  0.02521142  0.18326468  0.10617961  0.04498567\n",
      "   0.02169822 -0.04422976 -0.2330107  -0.0900814  -0.02227052  0.12781296\n",
      "   0.06490601 -0.07252434  0.1216557   0.04497239]]\n",
      "b3 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = init_parameters(neuron_count)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "print(\"b3 = \" + str(parameters[\"b3\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6f564c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single layer forward\n",
    "def single_layer_forward (H_prev,W,b,activation='relu'):\n",
    "    \n",
    "    # activation = sigmoid\n",
    "    if activation == \"sigmoid\":\n",
    "        Z = np.dot(W, H_prev) + b \n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = sigmoid(Z)\n",
    " \n",
    "    elif activation == \"softmax\":\n",
    "        Z = np.dot(W, H_prev) + b \n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = softmax(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        print(\" shape w\", W.shape, \" shape H\", H_prev)\n",
    "        Z = np.dot(W, H_prev) + b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = relu(Z)\n",
    "        \n",
    "    assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
    "    memory = (linear_memory, activation_memory)\n",
    "\n",
    "    return H, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b92e3be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 30,  35,  75, 160,  40],\n",
       "       [  7,   5,  15,  25,   9],\n",
       "       [  1,   0,   5,  10,   2]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Test\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "Z = np.dot(W_sample, H_prev) + b_sample\n",
    "Z\n",
    "#H = single_layer_forward(H_prev, W_sample, b_sample)[0]\n",
    "#H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9631ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now  for L_layer\n",
    "def multi_layer_forward(X,parameters):\n",
    "    memories = []\n",
    "    H = X\n",
    "    L = len(parameters)//2\n",
    "    #print(L)\n",
    "    \n",
    "    for i in range(1,L):\n",
    "        H_prev = H\n",
    "        H,memory = single_layer_forward(H_prev,parameters[\"W\"+str(i)],parameters[\"b\"+str(i)])\n",
    "        memories.append(memory)\n",
    "        \n",
    "    # Implement the final softmax layer\n",
    "    # HL here is the final prediction P as specified in the lectures\n",
    "    HL, memory = single_layer_forward(H, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)],activation=\"softmax\")\n",
    "    memories.append(memory)\n",
    "\n",
    "    assert(HL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return HL, memories    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20115ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "[[0.10371976 0.12258791 0.1649387  0.0999841  0.08709493]\n",
      " [0.07402441 0.11582099 0.07242176 0.08147696 0.10412131]\n",
      " [0.09386632 0.07235654 0.07712258 0.1312524  0.1062463 ]\n",
      " [0.08477888 0.09045667 0.13321834 0.12364141 0.11268957]\n",
      " [0.09635722 0.05629885 0.0795772  0.07192288 0.08279234]\n",
      " [0.05801445 0.08154273 0.08376579 0.07109354 0.07559287]\n",
      " [0.09910775 0.14440686 0.10055592 0.12596101 0.14421615]\n",
      " [0.17716308 0.11502742 0.13851701 0.13325697 0.08375003]\n",
      " [0.08733756 0.09878753 0.06789748 0.08696642 0.12060508]\n",
      " [0.12563058 0.1027145  0.08198521 0.07444431 0.08289143]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_X[:, 10:20]\n",
    "print(x_sample.shape)\n",
    "HL = multi_layer_forward(x_sample, parameters=parameters)[0]\n",
    "print(HL[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51728816",
   "metadata": {},
   "source": [
    "### Compute loss\n",
    "\n",
    "- Compute loss function after every forward pass\n",
    "- Keep checking whether it is decreasing with training\n",
    "- Loss function : Cross Entropy Loss Function (-1) * sum(y*log(p)) -P is probability matrix\n",
    "- For m data points average loss  : (-1/m) * sum(y*log(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7041c18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11 <class 'numpy.ndarray'>\n",
      "11 <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "zr = np.array([[11]])\n",
    "pt = np.squeeze(zr)\n",
    "print(np.squeeze(zr))\n",
    "print(pt,type(pt))\n",
    "assert(pt.shape==())\n",
    "print(pt,type(pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "484f371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss (HL,Y):\n",
    "    #HL is probability matrix like previous output\n",
    "    #Another argument is true label that is Y\n",
    "    # Using cross entropy loss function\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    loss = (-1) * (1/m) * np.sum(np.multiply(Y, np.log(HL)))\n",
    "    \n",
    "    loss = np.squeeze(loss)\n",
    "    assert(pt.shape==())\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b2c378b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]\n",
      " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]\n",
      " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]\n",
      " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]\n",
      " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]\n",
      " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]\n",
      " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]\n",
      " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]\n",
      " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]\n",
      " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "0.8964600261334037\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "# HL is (10, 5), Y is (10, 5)\n",
    "np.random.seed(2)\n",
    "HL_sample = np.random.rand(10,5)\n",
    "Y_sample = train_set_Y[:, 10:15]\n",
    "print(HL_sample)\n",
    "print(Y_sample)\n",
    "\n",
    "print(compute_loss(HL_sample, Y_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3884b0d0",
   "metadata": {},
   "source": [
    "### Backpropagation of errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1055e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need a function similar to compute dz\n",
    "def propagate_backward(dh,memory,activation):\n",
    "    # Gradient =dh \n",
    "    # Sigmoid\n",
    "    Z= memory\n",
    "    if activation==\"sigmoid\":\n",
    "        H = 1/(1+np.exp(-Z))\n",
    "        dz = dh * H * (1-H)\n",
    "    elif activation == \"relu\":\n",
    "        dz = np.array(dh, copy=True) # dZ will be the same as dA wherever the elements of A weren't 0\n",
    "        dz[Z <= 0] = 0\n",
    "        \n",
    "    assert (dz.shape == Z.shape)\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "498d5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need backward output for single layer\n",
    "def single_layer_backward(dh, memory,activation=\"relu\"):\n",
    "    linear_memory,activation_memory = memory\n",
    "    \n",
    "    dz = propagate_backward(dh,activation_memory,activation)\n",
    "    H_prev, W, b = linear_memory\n",
    "    m = H_prev.shape[1]\n",
    "    dw = (1.0/m) * np.dot(dz, H_prev.T)\n",
    "    db = (1. / m) * np.sum(dz, axis=1, keepdims=True)\n",
    "    dh_prev = np.dot(linear_memory[1].T, dz)\n",
    "    \n",
    "    return dh_prev,dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cbd4aa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dH_prev is \n",
      " [[1.22723679e-01 1.35006555e-01 8.95073976e-04 2.33143020e-05\n",
      "  1.94308447e-02]\n",
      " [2.03785715e-13 8.63516986e-17 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]]\n",
      "dW is \n",
      " [[8.15142859e-15 1.63201275e-14]\n",
      " [7.34933850e-05 1.49406888e-03]\n",
      " [3.31117895e-02 1.89463900e-01]]\n",
      "db is \n",
      " [[8.15488266e-15]\n",
      " [3.38860777e-04]\n",
      " [5.49381719e-02]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "H, memory = single_layer_forward(H_prev, W_sample, b_sample)\n",
    "np.random.seed(2)\n",
    "dH = np.random.rand(3,5)\n",
    "dH_prev, dW, db = single_layer_backward(dH, memory)\n",
    "print('dH_prev is \\n' , dH_prev)\n",
    "print('dW is \\n' ,dW)\n",
    "print('db is \\n', db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8dafcb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For L Layers keep on calculate gradient \n",
    "def multi_layer_backward(HL, Y, memories):\n",
    "    \n",
    "    # Takes the predicted value HL and the true target value Y and the \n",
    "    # memories calculated by L_layer_forward as input\n",
    "    \n",
    "    # returns the gradients calulated for all the layers as a dict\n",
    "\n",
    "    gradients = {}\n",
    "    L = len(memories) # the number of layers\n",
    "    #print(L)\n",
    "    m = HL.shape[1]\n",
    "    Y = Y.reshape(HL.shape) # after this line, Y is the same shape as HL\n",
    "    \n",
    "    # Perform the backprop for the last layer that is the softmax layer\n",
    "    current_memory = memories[-1]\n",
    "    linear_memory, activation_memory = current_memory\n",
    "    dZ = HL - Y\n",
    "    H_prev, W, b = linear_memory\n",
    "    gradients[\"dH\" + str(L-1)] = np.dot(linear_memory[1].T, dZ)\n",
    "    gradients[\"dW\" + str(L)] = (1. / m) * np.dot(dZ, H_prev.T) \n",
    "    gradients[\"db\" + str(L)] = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "     \n",
    "    # Perform the backpropagation l-1 times\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
    "        current_memory = memories[l]\n",
    "        \n",
    "        dH_prev_temp, dW_temp, db_temp = single_layer_backward(gradients[\"dH\" + str(l + 1)], current_memory,activation=\"relu\")\n",
    "        gradients[\"dH\" + str(l)] = dH_prev_temp\n",
    "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
    "        gradients[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6771e578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['W1', 'b1', 'W2', 'b2'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79ec4a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW3 is \n",
      " [[ 0.01835666  0.00323199  0.00968218  0.01453862  0.00467873  0.00618915\n",
      "   0.0208162   0.0037565   0.02671775  0.01027468  0.0131307   0.00295632\n",
      "   0.02603656  0.00405461  0.03312186  0.00533466  0.00390259  0.02639974\n",
      "   0.00163343  0.01502369  0.04084336  0.01593622]\n",
      " [-0.00506001 -0.00027949  0.00988383  0.00179633  0.00386307  0.00524962\n",
      "   0.01000929  0.00366608  0.00873288 -0.00118346  0.00040588  0.00250885\n",
      "   0.01945508  0.00437419  0.02458737 -0.00763539  0.000281    0.01668395\n",
      "   0.00130971 -0.00356257  0.0197628   0.00198979]\n",
      " [-0.00519915  0.00285754  0.00833639 -0.01514697  0.00371193 -0.02559013\n",
      "  -0.00070843  0.00060512 -0.03045865 -0.00184511 -0.0308424   0.00255271\n",
      "  -0.02378553  0.0035254  -0.00116141  0.00190895  0.00321515 -0.02494578\n",
      "   0.00130422 -0.04948407 -0.01445744 -0.00853608]\n",
      " [-0.00852069 -0.00683213 -0.02129794 -0.07701725  0.00411538  0.00570838\n",
      "  -0.05820556  0.00067817 -0.017248    0.00969177  0.00689542  0.00277473\n",
      "  -0.06951538 -0.00602925 -0.04468312 -0.02804259  0.00340595 -0.00499599\n",
      "   0.00143149 -0.02169911 -0.09530578  0.00807395]\n",
      " [ 0.0171      0.00303397  0.00962106  0.01316209  0.00340942  0.00480376\n",
      "   0.01854953  0.00338424  0.02178265  0.008704    0.01110679  0.0024266\n",
      "   0.02380023  0.00445533  0.0275211   0.00492156  0.00274237  0.02161213\n",
      "   0.00117426  0.01295753  0.03628305  0.01321175]\n",
      " [ 0.01605117 -0.00738833 -0.00370788  0.01211994  0.00358993  0.0036161\n",
      "   0.01798892 -0.00317371  0.00127498 -0.00220073  0.010514    0.00234959\n",
      "   0.02225815  0.00389998  0.00869273  0.00454288  0.00269869  0.02099254\n",
      "   0.00122035  0.00727688  0.0338774   0.01257948]\n",
      " [-0.0402435  -0.00373613 -0.02796996  0.01192271  0.00361916  0.00507873\n",
      "   0.00096376  0.00260832  0.01680269 -0.00408539 -0.00888145 -0.0133983\n",
      "  -0.0356177  -0.02747353 -0.00747039  0.00440852  0.00307917 -0.01138977\n",
      "   0.00126569  0.0125886  -0.05171207  0.01417239]\n",
      " [ 0.00357172  0.00319977  0.01025885  0.01413662 -0.00350095 -0.01466318\n",
      "   0.02007876  0.00352617 -0.03010836 -0.00765022 -0.02422662 -0.00658894\n",
      "   0.00810577  0.00494213 -0.03483484  0.00532428 -0.02382702 -0.04347513\n",
      "  -0.00332426  0.0022178  -0.0036313  -0.00729638]\n",
      " [ 0.00826906  0.00293547 -0.00455674  0.01133762  0.00351943  0.00398307\n",
      "  -0.03176622 -0.01703793  0.01976065 -0.00151682  0.00923244  0.00247473\n",
      "   0.00899557  0.003465   -0.00048131  0.00434446  0.0026765   0.01726334\n",
      "   0.00120069  0.0107508   0.03223016 -0.03958747]\n",
      " [-0.00432528  0.00297735  0.0097502   0.01315028 -0.02700611  0.0056245\n",
      "   0.00227375  0.00198703 -0.01725658 -0.01018871  0.01266523  0.0019437\n",
      "   0.02026726  0.00478615 -0.00529199  0.00489265  0.00182558 -0.01814502\n",
      "  -0.00721557  0.01393045  0.00210982 -0.01054365]]\n",
      "db3 is \n",
      " [[ 0.10945336]\n",
      " [ 0.00383809]\n",
      " [-0.00692902]\n",
      " [-0.09369391]\n",
      " [ 0.09761903]\n",
      " [-0.00667033]\n",
      " [-0.10487684]\n",
      " [ 0.00696679]\n",
      " [-0.01026066]\n",
      " [ 0.00455349]]\n",
      "dW2 is \n",
      " [[ 1.02261849e-02  0.00000000e+00  2.41707517e-03 -3.72746324e-03\n",
      "   1.03963287e-02  3.15018469e-02 -2.52747560e-02  1.61682978e-02\n",
      "  -3.94007826e-02  2.32157080e-02  3.54094942e-03  1.53710022e-02\n",
      "  -1.99983765e-02  1.76507154e-02 -2.91441607e-04  8.61807340e-03\n",
      "  -5.37390948e-04  0.00000000e+00  4.26082632e-05  2.14133765e-02\n",
      "  -1.37477679e-03 -1.73927212e-03  2.25312288e-02 -9.32910358e-03\n",
      "   3.10475921e-02  8.61700212e-03 -5.89072065e-02 -8.09174057e-03\n",
      "   1.53044540e-02  1.58759601e-02  0.00000000e+00 -1.56487859e-02\n",
      "   9.62868735e-03  0.00000000e+00  0.00000000e+00  1.71627015e-03\n",
      "  -3.53810582e-03 -1.91781263e-02  1.99983346e-02 -3.94669963e-02\n",
      "   2.05786803e-03  1.28917441e-02  3.13919107e-02 -2.29204825e-03\n",
      "  -8.40360692e-03]\n",
      " [ 0.00000000e+00 -7.19690563e-06  2.60215524e-02  1.66255941e-02\n",
      "   2.22104464e-03  6.40491545e-02  0.00000000e+00  0.00000000e+00\n",
      "  -7.78560627e-06 -9.80319517e-04  0.00000000e+00  7.83992480e-04\n",
      "  -8.17982113e-06  7.82459043e-03 -2.91054771e-04  0.00000000e+00\n",
      "   6.55034947e-03  0.00000000e+00  0.00000000e+00 -6.94796492e-03\n",
      "   0.00000000e+00 -3.51092957e-03  3.41945493e-02  2.20013777e-03\n",
      "  -2.87741551e-03  9.25470730e-03  1.07619775e-02  3.45419091e-02\n",
      "   1.72510761e-02  1.42045984e-02  0.00000000e+00  0.00000000e+00\n",
      "   7.63392371e-03  0.00000000e+00 -3.26186974e-03 -2.43106045e-03\n",
      "   0.00000000e+00 -1.29353011e-05 -9.35040035e-04 -2.15547948e-03\n",
      "  -4.12970076e-03  1.85730351e-02  3.12472257e-02 -1.99034622e-04\n",
      "   3.07613848e-03]\n",
      " [ 0.00000000e+00  1.09805170e-04  1.83005550e-02  1.31497230e-02\n",
      "   4.43870955e-03  6.14407353e-02  2.09205428e-03  7.75657222e-04\n",
      "  -2.73202500e-03  3.16619613e-03  0.00000000e+00 -6.09587971e-03\n",
      "  -1.66918381e-03  1.26412869e-02  4.28832049e-03 -1.29532918e-03\n",
      "   4.72709824e-03  0.00000000e+00 -1.58226286e-05 -6.32403208e-03\n",
      "  -2.70955522e-03  4.28078160e-03  2.80865788e-02  5.16216286e-04\n",
      "   1.73807352e-02  3.70282386e-04 -6.26474752e-04  3.10915727e-02\n",
      "   1.08295905e-02  5.97476245e-03  0.00000000e+00  0.00000000e+00\n",
      "   5.24462732e-03  0.00000000e+00  6.55172543e-04  4.88297873e-04\n",
      "   0.00000000e+00  1.97357448e-04  4.23349342e-03  9.16806378e-03\n",
      "   1.67189672e-03  1.68091781e-02  2.57831868e-02  1.83757865e-03\n",
      "   2.16589666e-03]\n",
      " [ 2.13299436e-03  0.00000000e+00  4.61285515e-03  3.25500664e-03\n",
      "  -9.49997513e-04  1.00777344e-02  1.38845703e-03  2.10745503e-03\n",
      "   1.64950698e-03  0.00000000e+00  7.38577017e-04  9.42733613e-04\n",
      "   0.00000000e+00  3.52687657e-03  6.73921858e-04  1.07000748e-03\n",
      "   8.02849229e-04  0.00000000e+00  0.00000000e+00 -3.19329253e-04\n",
      "   3.84568144e-04 -2.90025008e-03  5.66609816e-03 -1.29814286e-04\n",
      "   1.96867511e-03 -2.58888592e-04  2.73313722e-03  6.98281531e-03\n",
      "   4.87362179e-03  1.99682855e-03  0.00000000e+00  0.00000000e+00\n",
      "   2.67080379e-03  0.00000000e+00  0.00000000e+00  2.08081150e-03\n",
      "   0.00000000e+00  0.00000000e+00  1.66603383e-03  1.10959404e-03\n",
      "  -4.17473514e-03 -3.88219135e-04  5.42595479e-03 -2.48235907e-03\n",
      "   4.43522769e-04]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.94607896e-04  5.40891490e-03\n",
      "   0.00000000e+00  5.33362249e-03  1.69610204e-02  0.00000000e+00\n",
      "   1.86190442e-02  0.00000000e+00  0.00000000e+00  3.23883934e-03\n",
      "   8.72117085e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   3.69333010e-03  1.25631941e-02  9.52523518e-03  5.80855181e-03\n",
      "   0.00000000e+00  2.75784420e-03  3.01764037e-02  8.32096044e-03\n",
      "   0.00000000e+00  2.69432021e-04  0.00000000e+00  5.49655013e-03\n",
      "   6.73392258e-04  0.00000000e+00  0.00000000e+00  2.90118663e-03\n",
      "   1.00424950e-03  6.67142419e-03  2.04120377e-03  1.56664229e-02\n",
      "   5.50600971e-03  2.52932255e-03  7.69248195e-03  0.00000000e+00\n",
      "   2.88435064e-03]\n",
      " [ 5.88199512e-03 -1.43525769e-03  2.34579248e-03  2.73781847e-03\n",
      "   3.09214369e-03  4.73993252e-03  3.16953408e-03  5.81156727e-03\n",
      "   2.38596830e-03  0.00000000e+00  2.03671725e-03 -1.24975673e-03\n",
      "  -1.97028632e-03  3.83274712e-03  0.00000000e+00  2.95067764e-03\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00 -5.04688895e-03\n",
      "   9.16927259e-04  1.30124875e-03  2.51873239e-03  3.27974051e-05\n",
      "   0.00000000e+00 -3.00605027e-05  4.33857045e-03  3.16750444e-03\n",
      "   9.02296569e-04 -2.69412250e-04  0.00000000e+00 -2.13661476e-04\n",
      "  -8.34157622e-05  0.00000000e+00  0.00000000e+00  5.62531974e-03\n",
      "   0.00000000e+00 -2.82837543e-03  2.79907778e-03  3.81436731e-03\n",
      "  -2.14029188e-04 -1.12751883e-03  9.39937722e-03  0.00000000e+00\n",
      "  -1.12120257e-04]\n",
      " [ 2.09396847e-02  0.00000000e+00 -9.40654992e-03 -3.54220929e-03\n",
      "   7.41878253e-03 -3.64811917e-02  1.36305341e-02  2.06889642e-02\n",
      "   1.26112460e-02 -2.93315402e-04  7.25063796e-03 -1.49934072e-03\n",
      "  -1.10206465e-03  3.78748677e-03 -4.54376106e-03  9.70856843e-03\n",
      "  -4.41329041e-03  0.00000000e+00 -9.72001095e-06 -6.95405695e-03\n",
      "   2.11081081e-03  5.92673431e-03 -2.40548834e-02  2.07587337e-03\n",
      "  -1.42140614e-02 -1.32524704e-02  1.46715648e-03 -1.63080006e-02\n",
      "   1.17762420e-02 -1.23159494e-02  0.00000000e+00  0.00000000e+00\n",
      "   3.05905185e-03  0.00000000e+00 -9.75964078e-04  1.97000209e-02\n",
      "  -6.28733711e-04 -1.70799037e-04  4.27633687e-03  5.55954777e-03\n",
      "  -9.70388897e-04 -2.78212840e-02  3.03224855e-03 -2.06083656e-03\n",
      "  -2.01264398e-03]\n",
      " [ 3.39698667e-03  4.55687041e-04 -1.31819376e-03  7.08467220e-03\n",
      "   1.08278497e-03  5.27855866e-03  2.21124355e-03  3.35631297e-03\n",
      "   1.76807844e-02 -3.20155428e-04  1.17625078e-03  2.88946689e-02\n",
      "   6.34992599e-03 -2.48480084e-03 -2.95418482e-03  5.91502313e-03\n",
      "  -1.91415886e-03  0.00000000e+00  5.14372176e-05  2.55724527e-02\n",
      "   9.42085576e-03  1.03353738e-03  1.71681049e-02  1.30879573e-02\n",
      "  -6.81908875e-03  2.58525281e-02  4.02899012e-02 -5.32438694e-03\n",
      "   9.15278875e-03  1.51802384e-02  0.00000000e+00  0.00000000e+00\n",
      "   9.51270758e-03  0.00000000e+00 -1.06527034e-03  2.51993837e-03\n",
      "   1.81786111e-03  1.31285764e-03  5.83862929e-03 -3.72944527e-04\n",
      "   2.37030859e-03  2.21032042e-02  1.74078298e-02  0.00000000e+00\n",
      "  -8.77043395e-04]\n",
      " [-2.17180882e-03  2.82721606e-03  4.01494536e-03 -1.26137219e-03\n",
      "  -4.21884891e-03  1.12023687e-02 -9.42061062e-04 -1.90881152e-03\n",
      "  -2.84518729e-03  9.07233096e-04 -7.52017028e-04  4.50040351e-03\n",
      "   3.12719439e-03  2.32343194e-03  9.16154093e-04 -1.08947859e-03\n",
      "   1.05820492e-03  0.00000000e+00  0.00000000e+00  9.44318297e-03\n",
      "  -4.28049007e-04 -5.01576553e-03  3.42810164e-03 -1.35392853e-03\n",
      "   3.96238537e-03 -3.47568002e-03 -1.04269683e-02  5.55052463e-03\n",
      "   1.23946533e-02  3.11768554e-03  0.00000000e+00 -5.42950671e-05\n",
      "   6.66678652e-03  0.00000000e+00  0.00000000e+00 -2.14733438e-03\n",
      "  -1.39745968e-03  4.63864084e-03 -1.91926545e-04 -3.29740886e-03\n",
      "  -5.83578813e-03 -1.65101642e-03 -3.53712377e-03 -3.93935325e-03\n",
      "   5.74449429e-04]\n",
      " [-1.83306647e-02  4.48329156e-03 -7.81056152e-03 -1.66943867e-02\n",
      "  -1.63919944e-03  1.69380226e-02  2.47583212e-03 -1.55439892e-02\n",
      "  -1.69832210e-02  1.06201633e-02 -6.34723086e-03  7.50281035e-03\n",
      "   1.00317192e-02 -2.06699036e-03  4.01488382e-03 -8.40995691e-03\n",
      "   2.89244050e-04  0.00000000e+00  9.59551173e-06  2.65405562e-02\n",
      "  -3.20734992e-05  2.03507201e-02  7.74593692e-03 -6.23400737e-03\n",
      "   2.73349167e-02 -3.02020072e-03 -3.60875185e-02 -3.21330315e-03\n",
      "  -1.57920236e-03  4.47550102e-03  0.00000000e+00  2.42532620e-03\n",
      "  -8.63740297e-05  0.00000000e+00  2.63757847e-03 -1.46362993e-02\n",
      "  -5.60196466e-03  9.35956091e-03  1.66464664e-03  2.72788215e-03\n",
      "   1.81302211e-02  1.41979985e-02 -1.51181702e-02  4.98894997e-03\n",
      "   1.27270579e-03]\n",
      " [-5.73127157e-03  0.00000000e+00  2.80264370e-03 -4.71640254e-03\n",
      "  -5.14623468e-03 -9.10112294e-03 -1.42265018e-02 -9.42467783e-03\n",
      "  -4.75516414e-03 -1.44014138e-02 -1.98452726e-03 -1.23799346e-03\n",
      "  -1.79483598e-04 -8.43155060e-03  8.91256744e-04 -2.87506782e-03\n",
      "   1.36956234e-03  0.00000000e+00  0.00000000e+00  1.72860357e-04\n",
      "  -1.10932895e-03 -2.08433243e-02 -1.47967257e-02 -6.20030361e-04\n",
      "  -1.72694073e-02  3.10489527e-03 -2.93609729e-03 -4.01701604e-03\n",
      "  -3.25930554e-03 -5.90788937e-04  0.00000000e+00 -1.13120200e-04\n",
      "  -1.43565166e-03  0.00000000e+00  0.00000000e+00 -5.65076534e-03\n",
      "   0.00000000e+00 -1.31684876e-04 -1.19910381e-02 -1.62142787e-03\n",
      "  -9.49870647e-03  2.96806353e-03 -1.79019919e-02  2.05085111e-03\n",
      "   5.27195064e-04]\n",
      " [ 0.00000000e+00  0.00000000e+00 -2.51793830e-04  2.33285701e-04\n",
      "   0.00000000e+00  3.20671710e-03  1.97772555e-02  0.00000000e+00\n",
      "   1.47656152e-02 -3.72571656e-05  0.00000000e+00  2.22301389e-03\n",
      "   1.01692481e-02 -2.37587117e-04 -1.88701943e-04  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.51091249e-04\n",
      "   4.30657658e-03  1.46492071e-02  5.97305205e-03  1.90342693e-03\n",
      "  -6.42464418e-04 -1.49753140e-03  1.94683307e-02  6.99892766e-03\n",
      "   0.00000000e+00 -3.25739107e-04  0.00000000e+00  6.40920616e-03\n",
      "  -9.01982536e-04  0.00000000e+00 -1.23967767e-04  3.29051158e-03\n",
      "  -1.21412197e-03  7.13122735e-03  1.14864504e-03  1.70210621e-02\n",
      "   6.42023643e-03 -2.05026476e-03  3.72814343e-03  0.00000000e+00\n",
      "   3.36327286e-03]\n",
      " [-8.09892895e-03  0.00000000e+00 -1.71790641e-02 -2.60672644e-03\n",
      "  -3.88519093e-03  2.29243918e-06  2.43232323e-02 -2.28626453e-03\n",
      "   7.80814429e-03  2.36166447e-02 -2.80435940e-03  6.23529402e-03\n",
      "   8.40122775e-03  8.14599320e-03  6.17589899e-03 -3.26099807e-03\n",
      "  -3.98787495e-03  0.00000000e+00  9.79403808e-06  1.62397392e-02\n",
      "   3.30455896e-03  3.47188686e-02  2.23426373e-02 -2.66509853e-03\n",
      "   5.23619358e-02 -4.06523102e-03  8.40451040e-03  9.16006354e-04\n",
      "  -1.80870578e-02 -1.20329157e-04  0.00000000e+00  4.59503461e-03\n",
      "  -4.86984053e-03  0.00000000e+00  5.77768382e-03 -1.16935460e-03\n",
      "  -2.79562376e-04  5.27320213e-03  2.06240855e-02  1.66609180e-02\n",
      "   1.70257581e-02  1.29644496e-03  8.70733336e-04 -5.38861569e-03\n",
      "   6.87779562e-04]\n",
      " [ 0.00000000e+00  0.00000000e+00 -8.67349864e-03 -3.40050866e-04\n",
      "  -6.50349537e-04 -1.91490635e-02  1.81870371e-03  6.74308830e-04\n",
      "   0.00000000e+00  2.58131966e-03  0.00000000e+00 -1.02393302e-03\n",
      "   0.00000000e+00 -4.94113042e-03 -2.79336322e-03  0.00000000e+00\n",
      "  -4.01227493e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  3.72144905e-03 -1.08052327e-02  0.00000000e+00\n",
      "   8.61492158e-04 -3.85192401e-03 -6.93077373e-03 -1.39746835e-02\n",
      "  -5.27675020e-03 -8.73663731e-04  0.00000000e+00  0.00000000e+00\n",
      "  -4.55050776e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00 -8.11909225e-04 -3.84322398e-04\n",
      "   2.08730700e-03 -3.56682215e-03 -1.50394223e-02  0.00000000e+00\n",
      "  -1.83837366e-03]\n",
      " [-1.84345443e-03  3.05973493e-03 -4.34066109e-03  1.22900534e-02\n",
      "   1.90174810e-03 -3.07990812e-02 -1.69415722e-02 -1.82138189e-03\n",
      "   4.42113622e-03 -1.52710821e-03 -6.38320052e-04 -1.19750361e-02\n",
      "  -8.36346698e-03 -1.23400083e-02 -8.43883657e-03 -3.63019528e-03\n",
      "  -8.00408767e-04  0.00000000e+00 -3.30472610e-05 -1.59108395e-02\n",
      "  -9.41935698e-03 -7.68253843e-03 -1.80732691e-02  1.22813559e-02\n",
      "  -2.81693306e-02  2.53710219e-04  9.64414938e-03 -1.40236479e-02\n",
      "   4.35956465e-03 -8.95396809e-03  0.00000000e+00 -5.10136957e-03\n",
      "   1.47282496e-03  0.00000000e+00 -5.08122911e-03 -8.27798046e-03\n",
      "   7.91864135e-03  1.71195480e-03 -1.19636930e-02 -2.77352372e-02\n",
      "  -1.29490142e-03 -5.78184573e-03 -9.09338897e-03  3.31987994e-03\n",
      "  -3.14046803e-03]\n",
      " [ 1.31194987e-03  0.00000000e+00 -9.88377762e-03 -6.49062310e-03\n",
      "  -3.11653037e-03 -3.40046495e-02  8.54004141e-04  1.29624129e-03\n",
      "   1.01456924e-03  0.00000000e+00  4.54279692e-04 -5.46985817e-04\n",
      "   0.00000000e+00 -4.60423213e-03 -1.83649264e-03  6.58134028e-04\n",
      "  -2.74800595e-03  0.00000000e+00  0.00000000e+00 -2.12972839e-04\n",
      "   2.36537956e-04 -1.96794510e-03 -1.64307205e-02 -1.98163454e-03\n",
      "  -5.53619413e-03 -5.57765679e-03 -3.94780556e-03 -1.50354234e-02\n",
      "  -5.33881907e-03 -5.59361118e-03  0.00000000e+00  0.00000000e+00\n",
      "  -3.15159485e-03  0.00000000e+00  0.00000000e+00  1.27985353e-03\n",
      "   0.00000000e+00  0.00000000e+00 -1.07403008e-03 -4.26227401e-03\n",
      "  -2.78428984e-03 -1.49547285e-02 -1.52531913e-02 -2.93132838e-03\n",
      "  -1.20863612e-03]\n",
      " [ 0.00000000e+00  0.00000000e+00 -9.06102301e-05 -1.73673449e-03\n",
      "   5.83999683e-03  7.07192397e-03 -6.46727440e-03  0.00000000e+00\n",
      "  -7.34599504e-03  0.00000000e+00  0.00000000e+00 -1.28803943e-03\n",
      "  -3.32540164e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   3.47348089e-04  0.00000000e+00  0.00000000e+00  6.71642099e-04\n",
      "  -1.40827489e-03  2.67465831e-03 -1.21985545e-03  6.14370595e-04\n",
      "   3.00718638e-05  2.63998110e-03 -1.02883531e-02 -3.26115006e-03\n",
      "   1.13467053e-04 -1.25448648e-04  0.00000000e+00 -2.09584666e-03\n",
      "  -3.13534182e-04  0.00000000e+00  0.00000000e+00 -1.10622885e-03\n",
      "  -4.67582663e-04 -2.56682712e-03 -8.14886877e-04 -2.00013692e-03\n",
      "   6.68122603e-03  8.61527628e-03  4.33379576e-03  5.99114221e-03\n",
      "  -1.09980924e-03]\n",
      " [ 1.73166956e-03  0.00000000e+00 -6.07251792e-03 -3.67076487e-03\n",
      "   9.61565306e-03  9.64470872e-03  9.81831888e-03  4.40789058e-03\n",
      "  -5.58909803e-03  1.03242059e-02  5.99613088e-04  2.58629701e-03\n",
      "   1.75858706e-03  1.76393198e-03 -2.03779079e-03  1.61235645e-03\n",
      "  -2.54112284e-03  0.00000000e+00  9.08405640e-06  5.36042924e-03\n",
      "   2.17638242e-03  2.47538301e-02  5.11978730e-03 -1.30049071e-03\n",
      "   1.23781864e-02 -9.05337990e-04 -1.68733534e-02 -7.29229036e-03\n",
      "  -6.84822985e-04  3.11556462e-03  0.00000000e+00  4.59220222e-04\n",
      "  -3.14931089e-03  0.00000000e+00  0.00000000e+00  1.93169036e-03\n",
      "  -3.39180053e-03 -3.86816666e-04  5.90627286e-03  5.53208089e-03\n",
      "   1.89816674e-02  8.66758088e-03  6.82329083e-03  6.65571783e-03\n",
      "  -1.10013604e-03]\n",
      " [ 0.00000000e+00  0.00000000e+00 -6.45112271e-04 -7.54734974e-03\n",
      "   0.00000000e+00 -2.08595981e-03  5.26233975e-03  0.00000000e+00\n",
      "  -4.82390308e-03  0.00000000e+00  0.00000000e+00 -1.27695256e-03\n",
      "   2.70583744e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.14589555e-03  3.89786665e-03 -4.54025619e-03 -5.61983557e-03\n",
      "   0.00000000e+00 -6.33862903e-03 -1.46300347e-02 -1.21758625e-03\n",
      "   0.00000000e+00 -8.93149285e-04  0.00000000e+00  1.70536404e-03\n",
      "  -2.23225069e-03  0.00000000e+00  0.00000000e+00  9.00124484e-04\n",
      "  -3.32902051e-03  1.08089207e-03 -9.39356847e-04  3.55927160e-03\n",
      "   1.70829898e-03 -6.44197158e-03 -5.41110595e-03  0.00000000e+00\n",
      "   8.94900935e-04]\n",
      " [-3.45421680e-03 -2.38873953e-03 -4.65445222e-03 -3.25834394e-04\n",
      "  -4.80703333e-03 -1.67525149e-02  1.96005294e-02 -3.41285786e-03\n",
      "   1.49626136e-02  0.00000000e+00 -1.19606746e-03 -2.61751139e-03\n",
      "   8.51954902e-03 -4.78398232e-03 -5.97349674e-04 -1.73279305e-03\n",
      "  -9.99453916e-04  0.00000000e+00  0.00000000e+00 -8.67318013e-03\n",
      "   4.13493510e-03  1.20929774e-02  1.77727676e-03 -1.28221919e-03\n",
      "  -1.72395118e-03 -1.94953305e-03  2.50564336e-02  2.33183202e-03\n",
      "  -1.59942450e-02 -2.16145985e-03  0.00000000e+00  7.08060403e-03\n",
      "  -9.11907939e-03  0.00000000e+00  0.00000000e+00  3.67570501e-04\n",
      "   0.00000000e+00  3.94925246e-03 -1.66784684e-04  1.42913596e-02\n",
      "   3.51717937e-03 -8.77559184e-03 -6.36101911e-03 -2.75149407e-03\n",
      "   3.32246435e-03]\n",
      " [ 1.11799313e-02  0.00000000e+00  2.85045504e-03 -2.02879461e-03\n",
      "   5.24837810e-03  1.91510715e-02 -7.40502768e-03  1.34122794e-02\n",
      "  -1.83135328e-02  9.72902412e-03  3.87119651e-03 -7.24239902e-04\n",
      "  -1.08311441e-02  1.59412688e-02  3.17938364e-03  5.60836462e-03\n",
      "  -5.07580845e-04  0.00000000e+00  0.00000000e+00  4.14803801e-03\n",
      "  -2.57119771e-03 -3.29583970e-03  4.09721604e-03 -7.66485663e-03\n",
      "   2.33331923e-02 -8.75933108e-03 -3.77025538e-02  1.79875615e-03\n",
      "   1.19272731e-02  5.82036729e-04  0.00000000e+00 -6.82636857e-03\n",
      "   5.88595808e-03  0.00000000e+00  2.23245566e-03  8.96716954e-03\n",
      "  -2.56448747e-03 -8.64333263e-03  1.20436363e-02 -7.28574094e-03\n",
      "  -4.39914993e-03 -4.76906713e-03  1.33544768e-02 -3.49337103e-03\n",
      "  -3.72619714e-03]\n",
      " [ 2.96840602e-03 -1.81677115e-03  5.53266446e-03  2.87146172e-03\n",
      "   2.93515990e-04  2.14351384e-02  3.55653377e-02  2.93286393e-03\n",
      "   2.94822444e-02  0.00000000e+00  1.02784916e-03  1.42038191e-02\n",
      "   1.79680093e-02  4.66686545e-03  1.34632245e-03  3.46686126e-03\n",
      "   1.81620610e-03  0.00000000e+00  2.41587773e-05  5.65573256e-03\n",
      "   1.19960094e-02  2.32881435e-02  2.75270941e-02  4.21113514e-03\n",
      "   1.30145593e-03  9.11048141e-03  5.12187638e-02  2.13426159e-02\n",
      "   1.22944949e-03  6.97674940e-03  0.00000000e+00  1.08994554e-02\n",
      "   1.83370428e-03  0.00000000e+00  0.00000000e+00  8.64873084e-03\n",
      "  -2.17153631e-03  8.83294588e-03  7.94142354e-03  3.06327851e-02\n",
      "   9.05871855e-03  4.83846396e-03  2.57968115e-02 -2.02834935e-03\n",
      "   6.60560490e-03]]\n",
      "db2 is \n",
      " [[-0.00332068]\n",
      " [ 0.0169753 ]\n",
      " [ 0.02403814]\n",
      " [ 0.00095837]\n",
      " [ 0.01537996]\n",
      " [ 0.00078385]\n",
      " [-0.00895617]\n",
      " [ 0.02238714]\n",
      " [ 0.00118079]\n",
      " [ 0.01676357]\n",
      " [-0.0103582 ]\n",
      " [ 0.00971819]\n",
      " [ 0.0166192 ]\n",
      " [-0.00779716]\n",
      " [-0.00297276]\n",
      " [-0.01648813]\n",
      " [ 0.00733389]\n",
      " [ 0.01272275]\n",
      " [-0.00710148]\n",
      " [-0.00712832]\n",
      " [-0.00355928]\n",
      " [ 0.02299196]]\n"
     ]
    }
   ],
   "source": [
    "##### verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_X[:, 10:20]\n",
    "y_sample = train_set_Y[:, 10:20]\n",
    "\n",
    "HL, memories = multi_layer_forward(x_sample, parameters=parameters)\n",
    "gradients  = multi_layer_backward(HL, y_sample, memories)\n",
    "print('dW3 is \\n', gradients['dW3'])\n",
    "print('db3 is \\n', gradients['db3'])\n",
    "print('dW2 is \\n', gradients['dW2'])\n",
    "print('db2 is \\n', gradients['db2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4845cb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b844227a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dH2', 'dW3', 'db3', 'dH1', 'dW2', 'db2', 'dH0', 'dW1', 'db1'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6738bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Updates\n",
    "def update_parameters(parameters, gradients,learning_rate):\n",
    "    # what is parameters? dictionary of weights and biases\n",
    "    length = len(parameters)//2 # equal number of weights and biases combination\n",
    "    \n",
    "    #Now for each parameters we need to update weights and biases\n",
    "    for l in range(length):\n",
    "        parameters[\"W\"+str(l+1)] = parameters[\"W\"+str(l+1)] - learning_rate * gradients[\"dW\"+str(l+1)]\n",
    "        parameters[\"W\"+str(l+1)] = parameters[\"b\"+str(l+1)] - learning_rate * gradients[\"db\"+str(l+1)]\n",
    "        \n",
    "    # Return the updated parameters\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270704b2",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d1ce9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "def multi_layer_model(X,Y, dimensions, learning_rate=0.0075,num_iterations=3000,displayLoss=True):\n",
    "    np.random.seed(2)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    parameters = init_parameters(dimensions)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Forward\n",
    "        HL, memories = multi_layer_forward(X,parameters)\n",
    "        \n",
    "        #What is the loss after cycle i\n",
    "        loss = compute_loss(HL,Y)\n",
    "        losses.append(loss)\n",
    "        if displayLoss:# and i%100==0 :\n",
    "            print(\"Loss after iteration : \",i,\" is: \",loss)\n",
    "        \n",
    "        # do backward\n",
    "        gradients = multi_layer_backward(HL,Y,memories)\n",
    "        print(\" Gradients Updated\")\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        print(\" Gradients Updated\")\n",
    "    \n",
    "    # plotting the loss\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    return parameters, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfdc7b0",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b662f413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if it runs first for taking 5000 samples\n",
    "train_set_x_check = train_set_X[:,0:5000]\n",
    "train_set_y_check = train_set_Y[:,0:5000]\n",
    "dimensions = [784, 45, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "47b231d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration :  0  is:  2.422624104830477\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (45,1) and (784,5000) not aligned: 1 (dim 1) != 784 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9148/1465684797.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrained_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulti_layer_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set_x_check\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set_y_check\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9148/2542914548.py\u001b[0m in \u001b[0;36mmulti_layer_model\u001b[1;34m(X, Y, dimensions, learning_rate, num_iterations, displayLoss)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# Forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mHL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulti_layer_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m#What is the loss after cycle i\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9148/1593515036.py\u001b[0m in \u001b[0;36mmulti_layer_forward\u001b[1;34m(X, parameters)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mH_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msingle_layer_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH_prev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mmemories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9148/431693048.py\u001b[0m in \u001b[0;36msingle_layer_forward\u001b[1;34m(H_prev, W, b, activation)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH_prev\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mlinear_memory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mH_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_memory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (45,1) and (784,5000) not aligned: 1 (dim 1) != 784 (dim 0)"
     ]
    }
   ],
   "source": [
    "trained_parameters = multi_layer_model(train_set_x_check, train_set_y_check, dimensions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
