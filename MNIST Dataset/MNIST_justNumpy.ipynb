{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data in .pkl.gz format\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    f.seek(0)\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> 2\n"
     ]
    }
   ],
   "source": [
    "print(type(training_data),len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([5, 0, 4, ..., 8, 4, 8], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ok, so we have two numpy arrays in the tuple\n",
    "- First features set then labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,)\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0].shape,training_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784) (10000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(validation_data[0].shape,validation_data[1].shape)\n",
    "print(test_data[0].shape,test_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is training data ? I believe it is 28x28 image. LEts check it out\n",
    "samplearr = training_data[0][0]\n",
    "samplearr = samplearr.reshape(28,28)\n",
    "samplearr.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try imshow from matplotlib to check if it shows anything\n",
    "def showImage(arr):\n",
    "    #wrote these lines before but making it function for future usage\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(arr,cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFkAAABYCAYAAACeV1sKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMfUlEQVR4nO2cW0xc13qAv3/23C8MMAOYq4dLhGNwGCcYYleJIzWWkiqSayk6OechSpQjnb4cqUfKwzk6T308D22lPlVy1SO1UpW2UqvUiiw5iZXGauTEgA0uHgjGAcw4MwyDuQxjmOs6DzBbxjYYc9kGMp+EYPbea++fb7bWWnutfy9RSlFkdzE97wB+ChQlG0BRsgEUJRtAUbIBFCUbwLYki8hbIvK9iIyKyO92KqiDhmy1nywiGjACnAHCQA/wC6VUaOfCOxiYt1G2CxhVSv0AICL/DpwF1pXs9/tVIBDYxiX3JuPj48TjcVlv/3Yk1wKTD30OA92PHiQivwJ+BdDQ0EBvb+82Lrk36ezs3HD/rjd8SqnzSqlOpVRnRUXFbl9uT7IdyfeA+oc+161uK/II25HcA7wgIo0iYgV+DlzYmbAOFluuk5VSWRH5NXAJ0IA/KqVu7VhkB4jtNHwopS4CF3colgNL8YnPAIqSDWBb1cVeRCnFk55il5eXSafT+v5sNks+n6ekpASr1UoymWRpaYlkMkkmk0FEMJlMlJaWYrVacblcmM1b03VgJBfEZrNZcrncY/tHRkYYGBggnU6TTqeJRqMkEgk+/PBDmpubuXjxIr29vVy+fJmJiQmsVitOp5MPPviAtrY23nrrLTwez5Zi2zeSlVLkcjldZi6XI5PJkM1mSaVS5PN5crkci4uLzM3NPVb++vXr3L59W/8SEokEmUyG27dvo2kaoVCIO3fuEIvFWFxcxOv1YjKZcDgcuN1uRNZ9an4q+0ZyNptlaWkJpRT5fJ65uTnC4TDhcJiRkRGWl5dZWlri1q1bfPfdd4+Vz+Vy5PN5/XNZWRkejwcRwefz8eWXXzI2NobVasXtdtPS0kJdXR3BYJCmpqYtVxWwDyTn83kymQzz8/MMDQ2RyWRYXl5mZmaGcDjMzMwMkUhE3z4xMUEymdzwnCJCfX09zc3N1NTUUFJSwokTJzh8+DAlJSXYbDZqa2spLy+nrq4Or9eLpmlb/h/2vORMJsPMzAxXr17l448/JplM8uDBA70By+fzaxq7pw3dFhq0N954gzNnzmCz2TCbzbz55psAeL1erFYrDocDq9VKaWkpFovlYFcXIoLNZsNqtSIi5HI5lpeXn1rO6XTicrnQNA2z2cz8/DzJZBKTyYTZbKa6uprDhw+jadqau9Rut2OxWDCbzZhMJjRN25Zg2AeSNU3D6XTi8/koLS0lnU4zPz//1HIVFRW0tLTg8Xiw2Wz09/czNjaG2WzGarXS0dFBS0vLhufYrtwCe16yiKBpGj6fj9dee41YLEYoFMLhcOD1epmYmGB0dFQ/3uFw4HK5OHXqFCdPnsRut+NwOGhtbSUcDhOLxZiZmdl2j+FZ2POSTSYTVquVmpoa3n//fe7evcu1a9fw+Xw0NTVx+fLlNZJLS0tpamri3LlzvPPOO3p1sLi4SCKR4JtvvuH69euUl5cb9j/seckFLBYLtbW1uN1uXC4XHo8Hn89HLBajo6ODqakpotEo9fX1nDhxgrq6OsxmMyKCiGC1WvF4PBw7doyKigrKysoMi33fSLbb7dTU1FBVVUVjY6PeoAFMTk5y48YNotEoL774ImfPnqWhoWFNg2az2bDZbBw5coTW1lbDqgrYhwNEhd6ByWTS6+tCjwAgFovR39+/YeNopGDYh5JFBLPZrN+lhd6HzWYDVsYorly5QiQSeWqf2Sj2neRHKSsr4/Tp03R0dBAIBBARhoaGGB0dZX5+flN96t1m30v2er0Eg0FOnz7N8ePH0TSN4eFhhoeHCYfDe0Lyvmn41qNQLzc0NHDu3DlcLhdjY2OMjIzw6aefcvToURobG3G73djtdnw+H06n09AYD4zkQ4cO8fbbbzM9Pc2FCxfo6+sjFAoRDAYJBAIEAgHq6+t59dVXi5K3itlsxul00t3dzUcffcTQ0BD9/f0MDw8zNTXFnTt38Hg8ZLNZgsEgFRUVOBwOvaeyq7Ht6tkNxGw2YzabeeGFF3jvvff45JNP+PbbbxkbG2N4eFg/zmaz8eDBA06ePEl1dTUmk6ko+Vlxu900Nzfz7rvvUl9fz8DAADdu3ODHH39kdnaWa9euMTc3x8LCAi+99BKtra36cOZuyT5wku12O3a7XZ/RKC8vJ5VK8eDBA2ZnZ7l16xah0EriaTwep6ysDKfTiaZpRcnPSmHA/fXXX6epqYmBgQFGR0f5+uuvGRwc5MaNG8RiMZRSHDlyhDNnzuD3+3cllgMr2WKxYLFYaGhooKamhtraWiYnJ4lGowwODjI+Ps7ExASpVIrR0VE6OzuLkreKyWTCYrFQVVVFSUkJhw4d0sculFLcvXuXxcVFxsfH8fl8uN1urFbrjsbwk5CslMLlcuF0OikrK0NE9HGNeDyuSw4EAjgcjh2P4cBKTqVSpFIpZmZmmJ6eZnZ2lvv379PX16dPvgL4/X78fj+tra1UV1fro3k7yYGTXJCXTqdZWFhgYmKCmzdvMjY2RjQaJRQKrRmd83g8VFZW4vf7cblcuzIM+lTJIlIP/CtQBSjgvFLqH0SkHPgPIACMAz9TSs3ueISbpJB+FYvFGB8fJxQKcfv2bSKRCHfv3mVhYYFkMsns7EqIhdSAkpISKioqtj3tvxGbuZOzwMdKqesi4gH6ROQL4EPgslLqD6vv8P0O+O2uRLkBDycQFurWr776ilAoRE9PDwsLCyQSiTVlRASLxYKmaXi9Xjwez7aSV57GUyUrpSJAZPXvhIgMsfLm01ngjdXD/gX4XwyUXMiFW1hY4N69ewwNDdHT00M4HCYUCjE7O8v8/DyZTGZNuaqqKvx+P6dPn6a5uZmuri4OHTpEdXX1rsX6THWyiASA48B3QNXqFwAQZaU6eVKZNa+YbZeHszdTqRTxeJybN2/S19fHpUuXuH//Pvfv33/4+vpInaZpVFZWEggEOH78OC+//DLNzc1bztbcLJuWLCJu4L+A3yilFh6uv5RSSkSeONejlDoPnAfo7Ozc8nxQ4c7NZDIkEgnu3LnDZ599RiQSYWhoSO9FZLPZNeUqKyupqqri1KlTtLa2EgwGqa6upry8HLvdrk9b7SabkiwiFlYE/5tS6r9XN0+JSLVSKiIi1UBsNwLM5/Pk83n9zk0mk8RiMQYGBvjiiy+IxWJEIhH9+EKDVpgHrKqqoqWlhWPHjtHV1UVTUxNer9fQydTN9C4E+GdgSCn19w/tugB8APxh9ff/7GRgBbHxeJzPP/+c6elpwuEw8/Pz3Lt3j2g0yuTkJOl0ek25yspK6uvreeWVV2hvb6etrY2amhpKS0txOp16Tp2RbOZO/jPgfeD/RaR/ddvvWZH7nyLyS2AC+Nl2gyk8JBQSulOpFLFYjKtXrxKLxfj++++Zm5tjamrqsbKapmGxWKisrKS5uZm2tjZOnTpFbW0tZWVlhowbr8dmehf/B6z31f/5TgVSyEGemZnhypUrxONxfvjhB6anp+nt7SWVSrG0tPTYqwqFsYmOjg66u7s5efIkXV1deDwe/T2PncjM3A575okvn8+TSCSYmpqip6eH6elpBgYGSCaTxONxvZdgNpv18QURwW6343K5aGlpob29nfb2durq6vS0173AnpGcTqfp6+tjcHCQS5cusbi4uObOdTqdek5xW1ubnkHU3t5Od3c3brdbT+DezqsHu8GeiUYpRTqdJpfLYbFYcLlcuFwufb/X66WpqYm6ujqOHj2q36nBYJDGxkZ9jm8vsuWVW7ZCZ2enWm+9i1wux9LSEtls9rHHYECXarFY1oz3Fj4XqpPnQWdnJ729vbuyqMiOomkabrcbWMkxPkjs+zSt/UBRsgEUJRtAUbIBGNq7EJFpIAnEDbvozuPn8fgPK6XWXcXKUMkAItKrlNp4ja89zFbiL1YXBlCUbADPQ/L553DNneSZ4ze8Tv4pUqwuDKAo2QAMk7wfF7QWkXoR+UpEQiJyS0T+enX734jIPRHpX/35iw3PY0SdvF8XtF6dha9+OHsK+EtW5jMXlVJ/u5nzGHUn6wtaK6XSQGFB6z2NUiqilLq++ncCKGRPPRNGSX7SgtbPHOzz5JHsKYBfi8hNEfmjiGy4rkOx4dsEj2ZPAf8INANBVvIE/26j8kZJ3rcLWj8pe0opNaWUyiml8sA/sVIdrotRkvflgtbrZU+tNogFzgGDG53HkDm+fbyg9XrZU78QkSArSfHjwF9tdJLiY7UBFBs+AyhKNoCiZAMoSjaAomQDKEo2gKJkA/gTAa34xNMhutIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFkAAABYCAYAAACeV1sKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAM8UlEQVR4nO2cWWxbV3rHf4c7bZISaWqjTEqWRUWmZMVakjZREtepjcQBguk8ZJB5CAJkgGmADtACfeigSIA+BX1oC/QhKOAmBlqgQFsnATKGDdSJUSeulTimNzreKFmLTS3mJjqkFi6Xpw8SGVu2FMmkaFHhDyBIXvLe890/D7/7ne989wgpJRXWF9WTNuDnQEXkElARuQRURC4BFZFLQEXkElCQyEKIV4UQN4UQQ0KI3xfLqM2GeNw4WQihBvzAASAAnAN+LaW8VjzzNgeaAvZ9FhiSUg4DCCH+E/gFsKzIdrtdNjc3F9DkxmR0dJRwOCyW+7wQkRuBO/e9DwB/tPRLQojfAr8FcLlceL3eAprcmPT19a34+bpf+KSUh6SUfVLKvpqamvVubkNSiMjjgPO+99sXt1VYQiEinwPcQogdQggd8Cbwh+KYtbl4bJ8spcwIIX4H/A+gBg5LKa8WzbJNRCEXPqSUx4HjRbJl01KQyOWAoigoikIymSSVSjE7O0s6ncZoNKLX6zEYDGg0GjQaDSrV+sQBm17kubk5fvjhB86fP4/X6+Wbb77B5/Oxf/9+2traeOWVV3C5XFitVgwGw7rYsGlFzvXgYDCI3+/n4sWLDA4OcuvWLcLhMENDQwDY7XYSiQS9vb3rJjJSypI9ent7ZalIJBJyYmJCfvDBB7KpqUnabDZpNBqlVquVarVa6nQ6aTQaZX19vWxvb5cXL1587LYWz2vZ8950PTmTyZDJZLh79y6Dg4MMDQ0RjUZJp9Ok02lUKhVqtTrvfxOJBNlslng8zvz8PFqtFpVKhRDLjpLXzKYTeX5+nunpaT777DMOHz5MKBRiZmYGACEEOp0OnU6HVqtFCEE8HicejzM8PIzL5aKmpga9Xl9Uocte5NxfUlEUMpkMU1NTXL16Fb/fTzQaZW5u7oHvGwwGTCYTLS0t2O12zp49y9TUFJcvX0ar1dLb24vNZqOqqgqdTlcUGzeFyIqiMDs7SyQS4ejRo3z00UeEw2EikQhySSrXZrPhdDp5/fXX6enpIZlMcuzYMT788EP0ej3vvvsuPT09vPrqqxWRc8zMzDA1NcXk5CQ3btzA5/M9sgfnSCQSBINB0uk0AFVVVdTV1RGLxUgmk8RiMUKhEIqiFM3Gshc5EAjw8ccf8/333/PVV1+hKArZbHbZ7weDwXwvT6VS1NfX8+yzz3L27FnC4TDBYJDbt2+TSqWKZmPZipzJZEin04RCIUZHRxkfH39AGIvFgtlsZtu2bdhsNm7evMnk5CR6vR6dTkdVVRXV1dW43W70ej03b94kFAo9EHoVi7IVOZ1OE4lEuHLlCqdPn2Z2dvaBz2tra+ns7MTj8dDe3s6RI0c4evQoRqMRi8XC9u3baWhowGg00tnZidfrxe/3k81miyowlKHIiqKQSqUYHx/n1KlTXLx4kbm5OTKZDADV1dVYrVb27t3L888/j9lsxmQy0dbWxssvv0xraysNDQ3s3r0bs9mMVqvFZDJhsViQUhIIBDAYDMRiMaxWaz6cK4SyEzmdTjM9Pc3XX3/Ne++9x+zsbL4XCyFwOBx0d3fz5ptv0t/fTyQSIRgMsm/fPp566ilefPFFGhsb87GyxWIhk8lQXV0NgN/vZ2pqiuHhYaqrq7Hb7QVHGWUjsqIoeYGvX7+O3+9ndnaWbDaL0WhEq9Wi1+tpb2+ns7MTu92ORqPBbDajUqmwWCw4nU7sdjtarRa1Wp0/9v2DjkwmQzKZ5Nq1a5jNZiwWy89L5JmZGW7fvs3Ro0e5ceMGMzMzGI3G/N/dZrPh8Xh46aWXqK+vR61WYzabMZvNq24nk8kwNzfHuXPniEajeDweTCZTQbaXhcjZbJZEIoHX68Xn8+Hz+fKRQkdHBy+88AI2m426ujrcbje1tbUFZdRyuYx4PF6UeHnDi5wLp0KhEEeOHGFwcJCBgQE0Gg0Gg4He3l7efvvtfKiWzWZRFAW9Xl9Qm9PT00xPT/88RM79fXM5icnJSaSUtLe3c+DAAfr7+2lsbESv16PRaBZSixrNAz73SbPhRVYUhWg0yuDgIJcuXcoPh59++mneeecdamtrsdlsT9jKldnwIsfjcb744gsuXLiAoiiYzWbq6+vZsWMH9fX1BbmFpUgp85HGSkPztbLhRY7FYhw/fpxbt26RzWbZunUrHo+H1tZWqqqqipbzvX8ovfS5UDasyKlUinv37jE2NsbQ0BDhcBiAmpoaOjo6cDqdP3GEtSGEyD9UKhWNjY04HA40msIl2rAiZzIZJiYm8Pv9BAIB5ufngYV88K5du7Db7UVvM/evUKvVWK1W6urq0Gq1BR93w4qcS8anUikymQxCCCwWC01NTXR3d2O32wtyFVJKstksExMTTE1NMT4+jhACp9OJw+Hgtddeo7Ozky1bthR8LhtWZPhxUlRRFFQqFVu2bKGmpgaHw1HwUDcn8sjICN9+++0DIu/cuZOenh6cTufm7slLEUKg1WrzlT+P6ytzF7jx8XEmJib4/PPP8Xq93Lt3j5qaGvr7+9mzZw9Wq7Uo/hjK6MYctVqdTwItTfCshdyIMBAIMDAwkH8kk0nsdjudnZ10dXVhNBqLNmNdNj3ZZrPR399PS0vLY534/Pw8yWSSkZERxsbGOHHiBFeuXMFgMLBv3z7eeOMNurq6aGpqwmQyFcVN5CgbkU0mEy6Xi7q6ujXtl4t1cyHhpUuXOHfuHKdOnWJoaIh9+/bhdrs5cOAA63U/y0+KLIRwAv8O1AESOCSl/GchhA34L6AZGAV+JaWcLpZhSwcHoVCI7777joaGBvbu3bvq44TDYcbHxxkYGODy5csMDw9z+/Zt6uvraWtr46233qKrq4va2tpimf4Qq/HJGeCvpZQe4I+BvxBCeIDfAyellG7g5OL7opMTOpFIcOPGDYLBIIqiPDThmXudzWbzfldRFCKRCD6fj9OnT/Ppp58yMDDAyMgIFouFtrY2uru7aW1tLUqothw/2ZOllJPA5OLruBDiOgt3Pv0C+JPFr/0bcAr4m2Iad/8oLJVKMT09jc/n48svv8ThcOByuVCr1Wg0Gubm5ojFYkxOTjI4OEgkEmF6eprR0VFGRkaIxWKYTKZ8vnn//v35KGK9WZNPFkI0A93AWaBu8QcAmGLBnTxqnwduMVtDWw+8z1UJDQ8Pc+bMGTo6OvJTQwaDgWg0ysTEBOfPn+fChQsEAgHGxsaIxWLE43HsdjtWq5Xm5mYcDgcdHR24XK6iJpiWY9UiCyFMwKfAX0kpf7hfBCmlFEI8MpsipTwEHALo6+srKOOSGzx88sknnDlzhhMnTmAwGDAajYTDYe7cuUM0Gs1XEM3Pz7Nt2zZaWlpob2/H4XBw8OBB3G53/gcqZhSxHKsSWQihZUHg/5BSfra4+a4QokFKOSmEaACCxTYuV+aqVqsRQiClzE8LBQIB/H4/er2erVu35qt/cuT2s1qt7Ny5kx07dtDU1ERrayvbt28vtqkrsproQgAfA9ellP9030d/AN4G/n7x+fNiGqbVanG5XHR1ddHV1cXExAR37vx4A2wymSQajaJSqdBoNPnqodx9IB0dHbjdbg4ePMgzzzyTrxzaunVrMc1cFavpyf3AW8AVIcSlxW1/y4K4/y2E+A0wBvyqmIblchU2m43m5maEEEQikXwxdy56gAX/rVar0ev1WCwWqquraWlpwePxsHv3bpqamopp2ppZTXTxf8ByQ6w/La45P6JWqzEajbjdbt5//33GxsY4efIkfr+fY8eOPZRQ93g8PPfcc3R0dNDd3Z2fllrP0Gy1bNgRX653GgwGnE4nBoOBcDiMoijU1tbma9ZyF2C3282uXbvo6emhvb0do9G4fjfarJHHXu/icejr65NrXSUgN8DIVfakUikSicRDPdlgMLBlyxa0Wi06nQ6VSrVu9+Utpa+vD6/Xuy5LMZSEXI/O+VxgXWZF1pOySXWWMxWRS0BF5BJQEbkEVEQuARWRS0BJ42QhRAiYAcIla7T42HnY/iYp5bKrWJVUZAAhhFdKufIaXxuYx7G/4i5KQEXkEvAkRD70BNosJmu2v+Q++edIxV2UgIrIJaBkIpfjgtZCCKcQ4n+FENeEEFeFEH+5uP3vhBDjQohLi4/XVjxOKXxyuS5ovTgL3yClvCCEMAPngT9jYT4zIaX8h9Ucp1Q9Ob+gtZQyBeQWtN7QSCknpZQXFl/HgVz11JoolciPWtB6zcY+SZZUTwH8TgjhE0IcFkKsWOtVufCtgqXVU8C/ADuBPSzUCf7jSvuXSuSyXdD6UdVTUsq7UkpFSpkF/pUFd7gspRK5LBe0Xq56avGCmOOXwPcrHacks9WyfBe0Xq566tdCiD0sFMWPAn++0kEqw+oSULnwlYCKyCWgInIJqIhcAioil4CKyCWgInIJ+H94s5IOReA7oAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "samplearr = training_data[0][0]\n",
    "samplearr = samplearr.reshape(28,28)\n",
    "showImage(samplearr) # It should be 5 \n",
    "samplearr = training_data[0][1]\n",
    "samplearr = samplearr.reshape(28,28)\n",
    "showImage(samplearr) # It should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In case of training the 784 inputs would come together to form input for the network\n",
    "- The target is a number / digit labeled for each cases\n",
    "- For each 784 inputs, given a label from 10 digits - it is better to compute probability of the output labels being a certain class\n",
    "- Hence, converting the target datasets into one hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining one hot encoded vector  # label Binarizer can be used\n",
    "def onehot(data):\n",
    "    num = data.shape[0]\n",
    "    arr = np.zeros((10,num))\n",
    "    \n",
    "    col=0\n",
    "    for i in data:\n",
    "        arr[i][col] =1\n",
    "        col+=1\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot(np.array([5,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50000) (10, 50000)\n"
     ]
    }
   ],
   "source": [
    "#Let us format the datasets correctly\n",
    "train_df_X = training_data[0].T\n",
    "train_df_y = onehot(training_data[1])\n",
    "print(train_df_X.shape,train_df_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will process 1 input set as a single batch and 1 column in the target is the output of that batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dataset : (784, 10000) (10, 10000)\n",
      "Test Dataset : (784, 10000) (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Doing the same for validation and test sets\n",
    "validation_X = validation_data[0].T\n",
    "validation_y = onehot(validation_data[1])\n",
    "\n",
    "test_X = test_data[0].T\n",
    "test_y = onehot(test_data[1])\n",
    "\n",
    "print(\"Validation Dataset :\", validation_X.shape,validation_y.shape)\n",
    "print(\"Test Dataset :\",test_X.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In our case we would like to keep input layer having 784 neurons for each input datapoint\n",
    "- 1 hidden layer having 45 neurons taking inputs from all input layer\n",
    "- Output layer with 10 neurons which would give probabilities of each labels\n",
    "<img src=\"feedforward.jpg\" alt=\"feedforward\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let us define activation functions first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu\n",
    "def relu(arr):\n",
    "    out = np.maximum(0,arr)\n",
    "    assert(out.shape == arr.shape)\n",
    "    return out,arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1,   3,   0, 100,   0]), array([   1,    3,   -3,  100, -100]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing relu fn\n",
    "relu(np.array([1,3,-3,100,-100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "def softmax(arr):\n",
    "    Z_exp = np.exp(arr)\n",
    "    Z_sum = np.sum(Z_exp,axis=0,keepdims=True)\n",
    "    out = Z_exp/Z_sum\n",
    "    return out,arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.01122149e-43, 7.47197234e-43, 1.85211677e-45, 1.00000000e+00,\n",
       "        1.38389653e-87]),\n",
       " array([   1,    3,   -3,  100, -100]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(np.array([1,3,-3,100,-100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def sigmoid(arr):\n",
    "    Z_exp= np.exp(-arr)\n",
    "    out = 1/(1+Z_exp)\n",
    "    return out,arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.31058579e-01, 9.52574127e-01, 4.74258732e-02, 1.00000000e+00,\n",
       "        3.72007598e-44]),\n",
       " array([   1,    3,   -3,  100, -100]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.array([1,3,-3,100,-100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "dimensions = [784,45,10]\n",
    "def initialize_parameters(dimensions):\n",
    "    \n",
    "    # dimensions is a list containing the number of neuron in each layer in the network\n",
    "    # It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "\n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    L = len(dimensions)            # number of layers in the network + 1\n",
    "\n",
    "    for l in range(1, L): \n",
    "        parameters['W' + str(l)] = np.random.randn(dimensions[l], dimensions[l-1]) * 0.1\n",
    "        parameters['b' + str(l)] = np.zeros((dimensions[l], 1)) \n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (dimensions[l], dimensions[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (dimensions[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.04167578 -0.00562668 -0.21361961 ... -0.06168445  0.03213358\n",
      "  -0.09464469]\n",
      " [-0.05301394 -0.1259207   0.16775441 ... -0.03284246 -0.05623108\n",
      "   0.01179136]\n",
      " [ 0.07386378 -0.15872956  0.01532001 ... -0.08428557  0.10040469\n",
      "   0.00545832]\n",
      " ...\n",
      " [ 0.08905062  0.07040399 -0.08219343 ... -0.01449245 -0.06122183\n",
      "   0.02644657]\n",
      " [-0.08982216 -0.03764587 -0.05486391 ...  0.11571805 -0.06069458\n",
      "   0.01906126]\n",
      " [ 0.07903911 -0.0149561  -0.09171207 ...  0.17660684 -0.13286932\n",
      "  -0.05818293]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 1.17468571e-02 -5.02481234e-02 -3.14525825e-02 -2.27898280e-03\n",
      "   6.72031394e-02 -1.06401759e-01  5.14211713e-02 -9.71999970e-02\n",
      "  -1.02584659e-01  1.41976359e-01  1.37019480e-01 -6.54097115e-02\n",
      "   4.63439070e-02  4.03645330e-02  1.64252688e-01 -9.52313728e-02\n",
      "   1.16095163e-01 -1.76317067e-02 -2.86496769e-02  7.66775765e-02\n",
      "  -1.02404903e-01 -2.36651853e-02  4.09731037e-02 -7.24532419e-02\n",
      "  -6.68931735e-02  9.44072417e-02 -8.55873279e-03 -1.58553611e-01\n",
      "   1.71542248e-02  9.68529587e-02  1.27510698e-02 -5.24635935e-04\n",
      "   5.23205963e-02  1.62744929e-01 -7.08710158e-03 -2.65825985e-03\n",
      "  -5.76752354e-02  1.55725916e-01  1.63671210e-02 -9.52033570e-02\n",
      "  -1.75059488e-01  5.18128607e-02 -1.08119332e-01 -4.92621545e-02\n",
      "   8.67032483e-03]\n",
      " [-2.78670294e-02 -1.07551900e-01 -3.97779847e-02 -4.00289467e-03\n",
      "   3.95466542e-02  8.87982350e-02  3.18781471e-01  2.70050434e-02\n",
      "   6.85241256e-02 -5.71720630e-02  1.61929164e-01 -1.34277899e-01\n",
      "  -8.20919123e-02  7.25384349e-02  1.16387882e-02  2.09997026e-01\n",
      "   6.14133540e-02  3.17821068e-02  1.24141366e-01  7.76974309e-02\n",
      "  -5.36746363e-02  1.11783490e-01 -8.48828598e-02  8.67489778e-02\n",
      "  -4.46943756e-02  3.51478534e-02 -1.47481584e-01 -2.46845704e-03\n",
      "  -3.13504520e-02 -1.90559110e-01 -4.21285891e-02  8.42485726e-02\n",
      "  -3.96459858e-02  1.86409000e-02 -6.39043639e-02  7.22876257e-02\n",
      "  -1.04538216e-02  2.07921364e-01  9.33976394e-02  5.49094925e-02\n",
      "   1.24090624e-01  1.94141251e-01  8.20174847e-02 -8.23990419e-02\n",
      "   1.46233976e-01]\n",
      " [ 1.65573434e-01 -1.31628082e-01  1.24382195e-01 -2.31025735e-02\n",
      "   7.34636653e-02  7.96334175e-04 -1.69301997e-01  4.03871110e-02\n",
      "  -6.67962792e-02  1.23780706e-01 -5.99633238e-02  4.68280006e-02\n",
      "  -6.29731748e-02 -5.64673686e-02 -5.55650307e-02 -1.76906691e-01\n",
      "   2.62123061e-02  5.98060017e-03 -1.70950026e-01  7.28689726e-02\n",
      "   3.72909344e-02 -9.34966746e-03 -1.60002925e-02  1.41544832e-01\n",
      "  -7.91624273e-03 -1.00556501e-02 -1.24106201e-01  2.25360693e-01\n",
      "   3.69036801e-02  2.11926494e-01  3.44650711e-02 -2.17067840e-01\n",
      "   2.91405270e-02 -1.32552800e-02 -3.69571127e-02 -1.08721404e-01\n",
      "   1.97907896e-01 -6.51110392e-02 -1.15964378e-01  4.89294837e-02\n",
      "  -3.29564991e-02 -4.73630570e-02  8.68248295e-02  8.37658673e-02\n",
      "   2.65478439e-02]\n",
      " [ 2.42399939e-02  7.74917560e-02 -1.53057806e-02  4.30680344e-02\n",
      "   2.21837295e-01 -3.37245038e-03  1.37591899e-01 -8.18984826e-02\n",
      "   1.46472698e-02  1.98655671e-01 -6.06158144e-02 -9.83731484e-02\n",
      "  -1.23723975e-01 -5.95485271e-02  8.74151907e-02  1.02498395e-02\n",
      "   2.57753943e-01  1.05127577e-02 -6.09519128e-02 -2.75761319e-02\n",
      "   7.42130784e-02  2.11124104e-01  6.39451661e-02 -2.48462819e-01\n",
      "   4.61297044e-02 -1.83848428e-01  5.58823147e-02 -3.02666027e-03\n",
      "  -3.68302320e-02 -6.37798629e-02 -1.56212155e-01 -1.82728658e-03\n",
      "  -9.54494736e-02 -4.39781015e-02 -2.39488457e-02  1.21209190e-02\n",
      "  -7.69092768e-02 -4.42867523e-02  2.89189322e-02 -8.46854236e-03\n",
      "   1.37142853e-01 -1.17974885e-01  2.79751618e-02 -2.76806325e-01\n",
      "   4.57697833e-02]\n",
      " [ 2.20713595e-01 -2.19508206e-01 -1.73211112e-01 -1.08378890e-01\n",
      "   1.56919434e-01 -5.80832926e-02 -5.32426957e-02 -5.39463530e-02\n",
      "   7.39409555e-03 -4.82465065e-02  4.96517731e-02 -1.09491153e-03\n",
      "  -9.88668440e-02  1.70385104e-01  9.81652474e-02 -2.76657296e-02\n",
      "  -8.06750619e-02  1.28950798e-01  1.90159872e-01  5.78450490e-03\n",
      "  -9.52038002e-02 -9.93999925e-02  8.12549508e-02  6.61792439e-02\n",
      "  -1.33767169e-01  1.28165612e-01 -6.83306451e-02  6.88296736e-02\n",
      "  -2.74543752e-01 -4.35678110e-02 -8.31210672e-03 -2.51918552e-02\n",
      "   8.80097723e-03  7.13910081e-02 -1.08070683e-02 -5.64108116e-03\n",
      "   2.37397283e-03  6.60923889e-02 -4.22619579e-02 -6.85011404e-02\n",
      "  -6.55088830e-03 -2.52960049e-04 -6.72041228e-02 -6.51261124e-02\n",
      "  -1.45828628e-01]\n",
      " [-4.68929886e-02  5.51847767e-02  8.99896556e-02 -9.60256727e-02\n",
      "   3.03991332e-02 -2.59400242e-01  1.39264898e-01 -4.32447491e-02\n",
      "   1.41237547e-01 -9.19622643e-02 -5.26207354e-03 -4.80459887e-02\n",
      "   6.84505369e-02  4.37385361e-02 -3.07263474e-02  2.11564804e-01\n",
      "  -4.21788498e-02 -2.37746935e-01  1.93241957e-02 -4.88869640e-02\n",
      "   1.00558949e-01  1.95185844e-01  1.25969923e-01 -1.14670384e-01\n",
      "  -2.67955777e-01  4.63538288e-02  1.86490745e-01 -7.22167479e-02\n",
      "  -2.88437428e-01  1.17425261e-01 -2.69145672e-02 -1.37284921e-01\n",
      "   7.39880937e-02 -1.18205555e-01  3.58697101e-02  1.97767272e-02\n",
      "  -7.36163280e-02 -2.78778782e-02  7.63559766e-03 -1.62872866e-01\n",
      "   8.52474404e-02 -6.62827827e-02 -4.87831271e-02  5.90749340e-02\n",
      "  -1.03912874e-01]\n",
      " [ 7.29400387e-02 -1.06188620e-01 -8.27463660e-02  2.83603186e-02\n",
      "  -1.61271204e-02  1.83226773e-01 -4.37424602e-03  9.58566979e-02\n",
      "  -7.48522423e-03  3.90388364e-02 -2.40499769e-02 -4.90883231e-02\n",
      "   3.00264696e-02  3.12113671e-02 -2.11842727e-03 -1.18526413e-01\n",
      "   1.01435480e-01 -1.26424762e-02 -2.92140496e-02 -5.56176671e-03\n",
      "  -3.22008301e-01  5.20908205e-02 -4.71172909e-02  5.86113772e-02\n",
      "   5.65369027e-02  2.54738440e-01 -2.76608769e-02  8.18348955e-02\n",
      "  -1.97283960e-01  4.83730198e-02 -1.35420977e-02  1.37301955e-01\n",
      "   1.93918011e-02 -1.34225136e-01  8.61647741e-02  1.02261896e-02\n",
      "   2.24105209e-02  6.11262599e-02 -2.11749784e-01 -7.44406364e-02\n",
      "   5.13221581e-02 -8.93365434e-02  5.84747670e-02 -4.12704336e-03\n",
      "  -7.26467633e-02]\n",
      " [ 1.75653846e-01 -6.02911037e-02  5.08530745e-02  1.31736067e-02\n",
      "   1.32675927e-01 -6.67404923e-02 -7.96595448e-02 -1.06695275e-01\n",
      "   7.88566001e-02 -2.06228611e-01 -1.41531384e-01 -1.22936178e-01\n",
      "   2.19202793e-01 -3.65152911e-02 -8.10892359e-02 -2.77010940e-02\n",
      "  -3.56220551e-02  3.03396063e-02 -4.02587599e-02 -3.82370752e-02\n",
      "  -5.85964930e-03  4.92123887e-02 -5.27242405e-02  7.28431084e-02\n",
      "  -1.98280232e-02 -6.38777435e-03  2.95402292e-01 -4.86079498e-02\n",
      "   3.65600644e-02 -3.75605075e-02 -6.92493905e-02 -1.64852357e-01\n",
      "   5.01686909e-02 -1.28597628e-01 -6.24944937e-02  1.62617897e-01\n",
      "  -7.02863866e-02 -7.47393517e-02 -2.92637059e-02 -4.30466657e-03\n",
      "  -1.36377420e-01  1.57786861e-02 -8.21840337e-04 -2.92139597e-02\n",
      "  -2.19356065e-01]\n",
      " [ 1.80867662e-01 -4.71554666e-02 -3.68802539e-02  1.12520154e-02\n",
      "  -9.83153805e-02  1.79981422e-01  8.64730531e-02  9.80463441e-02\n",
      "   5.60155408e-02 -5.80603142e-02 -7.10474922e-02 -1.00656387e-01\n",
      "  -2.80132536e-01 -2.35446559e-01 -3.68889700e-02  1.90167689e-02\n",
      "   1.18240663e-01  5.58046631e-02 -6.92324340e-02 -1.39480840e-01\n",
      "  -2.68880237e-02 -5.30293378e-02  7.44982617e-02 -4.94277032e-02\n",
      "   9.48383920e-02 -2.45617224e-02  1.19252456e-01  6.93441857e-02\n",
      "   6.44770143e-02  1.15331592e-01 -4.34313873e-02 -8.71531161e-02\n",
      "   1.01318326e-01 -2.40273213e-01  9.83025672e-04 -1.33137496e-01\n",
      "  -7.86227257e-03 -2.33932028e-02 -4.60976883e-02  1.87238032e-01\n",
      "  -9.36857897e-02  9.70701960e-02  5.36373929e-02  4.15847120e-02\n",
      "   3.07210681e-02]\n",
      " [-7.16249391e-03 -1.54542362e-01 -2.10555033e-02  6.91346651e-02\n",
      "  -1.04585060e-01  1.09196992e-01 -1.46668833e-01 -8.71395825e-02\n",
      "  -8.78697266e-02 -4.38859472e-02  2.92219623e-02 -2.26228659e-01\n",
      "  -2.57662637e-02  2.41598646e-01 -1.19606119e-01  3.22168704e-03\n",
      "   1.20934029e-01 -1.73575945e-02  1.78757597e-01 -4.91688186e-02\n",
      "  -6.75263480e-03  3.12008865e-02  4.93504731e-02  4.14321583e-02\n",
      "   3.94023340e-02  2.36190503e-02 -9.96595657e-02  1.45815997e-01\n",
      "  -5.53806124e-02  3.86756133e-02  4.75905501e-02 -1.31741692e-01\n",
      "   1.36827055e-03  1.92497750e-02 -1.40097484e-02  4.64840158e-02\n",
      "   2.65023563e-02  7.12400307e-02  1.43209523e-02 -1.30550897e-02\n",
      "  -1.69419590e-01 -1.07772516e-01 -4.43441273e-02  2.98939288e-02\n",
      "  -1.19915626e-01]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "dimensions  = [784, 45,10]\n",
    "parameters = initialize_parameters(dimensions)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward(H_prev, W, b, activation = 'relu'):\n",
    "\n",
    "    # H_prev is of shape (size of previous layer, number of examples)\n",
    "    # W is weights matrix of shape (size of current layer, size of previous layer)\n",
    "    # b is bias vector of shape (size of the current layer, 1)\n",
    "    # activation is the activation to be used for forward propagation : \"softmax\", \"relu\", \"sigmoid\"\n",
    "\n",
    "    # H is the output of the activation function \n",
    "    # memory is a python dictionary containing \"linear_memory\" and \"activation_memory\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z = np.dot(W, H_prev) + b \n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = sigmoid(Z)\n",
    " \n",
    "    elif activation == \"softmax\":\n",
    "        Z = np.dot(W, H_prev) + b \n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = softmax(Z)\n",
    "    elif activation == \"relu\":\n",
    "        #print(\" shape w\", W.shape, \" shape H\", H_prev)\n",
    "        Z = np.dot(W, H_prev) + b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = relu(Z)\n",
    "        \n",
    "    assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
    "    memory = (linear_memory, activation_memory)\n",
    "\n",
    "    return H, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_forward(X, parameters):\n",
    "\n",
    "    # X is input data of shape (input size, number of examples)\n",
    "    # parameters is output of initialize_parameters()\n",
    "    \n",
    "    # HL is the last layer's post-activation value\n",
    "    # memories is the list of memory containing (for a relu activation, for example):\n",
    "    # - every memory of relu forward (there are L-1 of them, indexed from 1 to L-1), \n",
    "    # - the memory of softmax forward (there is one, indexed L) \n",
    "\n",
    "    memories = []\n",
    "    H = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    #print(L)\n",
    "    # Implement relu layer (L-1) times as the Lth layer is the softmax layer\n",
    "    for l in range(1, L):\n",
    "        H_prev = H \n",
    "        \n",
    "        H, memory = single_layer_forward(H_prev, \n",
    "                                 parameters[\"W\" + str(l)], \n",
    "                                 parameters[\"b\" + str(l)], \n",
    "                                 activation='relu')\n",
    "        memories.append(memory)\n",
    "    \n",
    "    # Implement the final softmax layer\n",
    "    # HL here is the final prediction P as specified in the lectures\n",
    "    HL, memory = single_layer_forward(H,\n",
    "                              parameters[\"W\" + str(L)], \n",
    "                              parameters[\"b\" + str(L)], \n",
    "                              activation='softmax')\n",
    "    memories.append(memory)\n",
    "\n",
    "    assert(HL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return HL, memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(HL, Y):\n",
    "\n",
    "\n",
    "    # HL is probability matrix of shape (10, number of examples)\n",
    "    # Y is true \"label\" vector shape (10, number of examples)\n",
    "\n",
    "    # loss is the cross-entropy loss\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    loss = (-1./ m) * np.sum(np.multiply(Y, np.log(HL)))\n",
    "    \n",
    "    loss = np.squeeze(loss)      # To make sure that the loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]\n",
      " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]\n",
      " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]\n",
      " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]\n",
      " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]\n",
      " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]\n",
      " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]\n",
      " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]\n",
      " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]\n",
      " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "0.8964600261334037\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "HL_sample = np.random.rand(10,5)\n",
    "Y_sample = train_df_y[:, 10:15]\n",
    "print(HL_sample)\n",
    "print(Y_sample)\n",
    "\n",
    "print(compute_loss(HL_sample, Y_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dh,memory):\n",
    "    Z = memory\n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    dZ = dH * H * (1-H)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dh, memory):\n",
    "    Z=memory\n",
    "    dZ= np.array(dh,copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward(dH, memory, activation = 'relu'):\n",
    "    \n",
    "    # takes dH and the memory calculated in layer_forward and activation as input to calculate the dH_prev, dW, db\n",
    "    # performs the backprop depending upon the activation function\n",
    "    \n",
    "\n",
    "    linear_memory, activation_memory = memory\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dH, activation_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = (1. / m) * np.dot(dZ, H_prev.T) \n",
    "        db = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dH_prev = np.dot(linear_memory[1].T, dZ)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dH, activation_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = (1. / m) * np.dot(dZ, H_prev.T) \n",
    "        db = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dH_prev = np.dot(linear_memory[1].T, dZ)\n",
    "    \n",
    "    return dH_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dH_prev is \n",
      " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]\n",
      " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]\n",
      "dW is \n",
      " [[1.67565336 1.56891359]\n",
      " [1.39137819 1.4143854 ]\n",
      " [1.3597389  1.43013369]]\n",
      "db is \n",
      " [[0.37345476]\n",
      " [0.34414727]\n",
      " [0.29074635]]\n"
     ]
    }
   ],
   "source": [
    "# Lets test\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "H, memory = single_layer_forward(H_prev, W_sample, b_sample, activation=\"relu\")\n",
    "np.random.seed(2)\n",
    "dH = np.random.rand(3,5)\n",
    "dH_prev, dW, db = single_layer_backward(dH, memory, activation = 'relu')\n",
    "print('dH_prev is \\n' , dH_prev)\n",
    "print('dW is \\n' ,dW)\n",
    "print('db is \\n', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_backward(HL,Y,memories):\n",
    "    gradients = {}\n",
    "    L = len(memories) # the number of layers\n",
    "    #print(L)\n",
    "    m = HL.shape[1]\n",
    "    Y = Y.reshape(HL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Perform the backprop for the last layer that is the softmax layer\n",
    "    current_memory = memories[-1]\n",
    "    linear_memory, activation_memory = current_memory\n",
    "    dZ = HL - Y\n",
    "    H_prev, W, b = linear_memory\n",
    "    gradients[\"dH\" + str(L-1)] = np.dot(linear_memory[1].T, dZ)\n",
    "    gradients[\"dW\" + str(L)] = (1. / m) * np.dot(dZ, H_prev.T) \n",
    "    gradients[\"db\" + str(L)] = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "     \n",
    "    # Perform the backpropagation l-1 times\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
    "        current_memory = memories[l]\n",
    "        \n",
    "        dH_prev_temp, dW_temp, db_temp = single_layer_backward(gradients[\"dH\" + str(l + 1)], current_memory, activation=\"relu\")\n",
    "        gradients[\"dH\" + str(l)] = dH_prev_temp\n",
    "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
    "        gradients[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW3 is \n",
      " [[ 2.97223118e-04  1.92239282e-02  1.08132699e-01  3.75994318e-02\n",
      "   8.11324658e-03  4.34206842e-02  1.52914283e-02  0.00000000e+00\n",
      "   1.33793457e-02  6.77858775e-03  2.58199644e-02  1.04253648e-02\n",
      "   2.29850493e-02  4.69443953e-02  6.75200373e-03  1.19886261e-01\n",
      "   4.30647178e-02  6.16379238e-02  6.78481416e-02  4.95798877e-02\n",
      "   2.75532444e-03  3.19768445e-02  5.93744529e-02  1.62289613e-02\n",
      "   6.02748527e-02  7.52515266e-02  3.88898986e-02  1.15426911e-02\n",
      "   7.83670359e-02  4.06326662e-02  0.00000000e+00  3.85225798e-02\n",
      "   9.68157205e-02  4.95705562e-02  5.59745638e-02  3.87598333e-03\n",
      "   0.00000000e+00  6.88041161e-03  6.51068962e-03  1.24615641e-02\n",
      "   0.00000000e+00  2.07803381e-02  1.50352126e-02  3.04744028e-02\n",
      "   2.53949815e-02]\n",
      " [ 5.52259382e-04  2.66265169e-02  1.44289265e-01  4.93487608e-03\n",
      "  -7.80501140e-02 -3.85146679e-02  2.18903276e-02  0.00000000e+00\n",
      "  -5.42160801e-03 -1.20391825e-02 -3.41857997e-03  1.50016603e-02\n",
      "   2.24588006e-02  1.69117801e-02  7.12500150e-03  9.78226373e-02\n",
      "   5.47578544e-02 -7.10482465e-02  8.08905431e-02  8.03492875e-04\n",
      "   3.92182957e-03  3.89643919e-02  7.36497065e-02 -6.32326462e-03\n",
      "   2.04984612e-02  3.05756555e-02  3.14915927e-02 -1.40428212e-02\n",
      "   9.74114690e-02  4.68191710e-02  0.00000000e+00 -2.66562532e-02\n",
      "   3.55697633e-02  5.77046900e-02 -1.38550289e-02  6.11332982e-03\n",
      "   0.00000000e+00  1.06706029e-02  1.13540558e-02  1.70877348e-02\n",
      "   0.00000000e+00  2.56182207e-02  1.88047138e-02 -3.45586177e-02\n",
      "   3.25377256e-02]\n",
      " [ 1.48054234e-04  1.27293643e-02 -8.85849239e-02  2.74004787e-02\n",
      "   7.47677781e-03 -4.66696989e-02  1.00715075e-02  0.00000000e+00\n",
      "   1.22430948e-02 -1.25488623e-02 -2.01799685e-02  7.81958787e-03\n",
      "  -6.81852481e-02  3.36370053e-02  4.41794786e-03 -2.16935677e-02\n",
      "  -5.51154006e-02 -7.08078679e-02  1.99838358e-02 -2.34017286e-02\n",
      "   1.75730856e-03 -2.57551855e-03 -1.51312826e-02  1.27050339e-02\n",
      "   4.50613117e-02  4.80567629e-02 -3.75168930e-02  7.89987589e-03\n",
      "  -4.19163324e-02 -5.06798368e-02  0.00000000e+00 -3.26453584e-02\n",
      "  -7.44291380e-03  3.29325896e-02  4.14116985e-02  2.48439354e-03\n",
      "   0.00000000e+00 -2.59141972e-03  4.66879922e-03  1.08961872e-02\n",
      "   0.00000000e+00 -6.60909538e-02 -3.24773345e-02 -4.05131050e-02\n",
      "   1.68159698e-02]\n",
      " [ 2.44639432e-04 -3.56114734e-03 -8.95025701e-02 -7.90073241e-02\n",
      "   6.42616175e-03 -9.54410619e-02 -3.87320685e-02  0.00000000e+00\n",
      "   1.06091901e-02  5.24670530e-03 -8.15685249e-03  1.13293048e-02\n",
      "   2.05756056e-02 -2.47922578e-01  7.24531790e-03 -1.57899542e-01\n",
      "  -5.40664021e-02 -4.80835847e-02 -9.69944948e-02 -1.74864306e-01\n",
      "  -1.73476965e-02 -3.76905174e-04  8.62022921e-03 -6.93874956e-02\n",
      "   1.25609658e-03 -1.73023701e-01  3.42494495e-02  1.20903185e-02\n",
      "  -4.46863147e-01 -1.79849221e-01  0.00000000e+00 -5.98492441e-02\n",
      "  -1.90222235e-01 -4.85877707e-02 -2.38938581e-01  4.88306480e-03\n",
      "   0.00000000e+00  5.16203188e-03  5.38397299e-03  9.16042174e-03\n",
      "   0.00000000e+00 -2.32406501e-02 -5.69878859e-02 -5.65890638e-02\n",
      "   2.84913842e-02]\n",
      " [ 2.47865794e-04  1.41257320e-02  6.69317058e-02  2.73625729e-02\n",
      "   1.14063016e-02  3.18364713e-02  8.91199449e-03  0.00000000e+00\n",
      "   1.18977648e-02  6.20578893e-03  1.82524126e-02  8.22402142e-03\n",
      "   1.24723218e-02  2.76324936e-02  4.55892179e-03  8.33048215e-02\n",
      "   2.51042892e-02  4.85963271e-02  4.64219911e-02  3.38368596e-02\n",
      "   1.28589286e-03  2.21933039e-02  4.00776747e-02  1.11810348e-02\n",
      "   4.64723754e-02  5.17432940e-02  2.51222281e-02  1.00139411e-02\n",
      "   3.68999081e-02  1.98414093e-02  0.00000000e+00  2.97706985e-02\n",
      "   6.55953370e-02  3.28145854e-02  3.77529712e-02  3.55916304e-03\n",
      "   0.00000000e+00  4.72730962e-03  5.73030221e-03  8.98980860e-03\n",
      "   0.00000000e+00  1.00713516e-02  6.67587145e-03  2.25894995e-02\n",
      "   1.89895364e-02]\n",
      " [ 2.98847348e-04  2.29431968e-02  6.84376145e-02 -2.08839668e-02\n",
      "   4.65729742e-03 -2.09553745e-02  1.52156818e-02  0.00000000e+00\n",
      "  -5.05632587e-02 -2.43576287e-02  1.94338619e-02 -1.54414833e-04\n",
      "   1.84104708e-02  1.31400013e-02  4.05138946e-03  3.88634120e-02\n",
      "   4.47477127e-02  5.81760778e-02  1.70389230e-02 -7.32200103e-02\n",
      "   1.40567935e-03  3.09701265e-02  5.55383229e-02 -2.69958332e-02\n",
      "  -3.65862349e-02  1.75401888e-02  4.62869176e-02  1.01277969e-02\n",
      "   4.20923512e-02  2.40905440e-02  0.00000000e+00  2.92440845e-02\n",
      "  -7.96107295e-02  4.50800996e-02  3.94113122e-02  4.72683859e-03\n",
      "   0.00000000e+00  7.87251199e-03 -2.11371486e-02 -4.11499733e-02\n",
      "   0.00000000e+00  1.84419701e-02  8.84115628e-03  2.54187888e-02\n",
      "   3.09285602e-02]\n",
      " [-2.59058987e-03  1.25718145e-02 -3.02378228e-01 -8.25942564e-03\n",
      "   1.44990077e-02  3.03803770e-02 -4.84508191e-02  0.00000000e+00\n",
      "  -2.95208449e-02  8.29123852e-03 -1.09166705e-01 -1.19600120e-03\n",
      "   1.57850263e-02 -1.45214840e-02  4.64855419e-03 -1.03554845e-01\n",
      "  -9.74984860e-02  1.22412967e-02 -8.83920795e-02  4.73678035e-02\n",
      "  -8.98268310e-04 -5.29352462e-03 -2.94433991e-02  1.59193753e-02\n",
      "  -9.40339044e-02 -1.66930442e-01 -9.59488820e-02  1.07594613e-02\n",
      "   3.99927247e-02  2.98984961e-02  0.00000000e+00 -5.77213006e-02\n",
      "  -1.66464188e-01  3.53015582e-02  5.00142805e-02  3.17607854e-03\n",
      "   0.00000000e+00 -5.34312378e-02 -3.23830980e-02 -6.12704193e-02\n",
      "   0.00000000e+00 -3.13150684e-02  1.05196740e-02  2.15707251e-02\n",
      "   3.06319438e-03]\n",
      " [ 2.12347737e-04 -2.02758546e-02  9.19433061e-02 -3.53446390e-02\n",
      "   7.18313995e-03  2.62531082e-02  7.87779352e-03  0.00000000e+00\n",
      "   1.36138641e-02  6.75045468e-03  2.18825787e-02 -2.88640648e-03\n",
      "  -8.35002818e-02  3.01940286e-02 -3.39937184e-02  3.30454946e-02\n",
      "   3.70242974e-02  5.23710729e-02 -1.13246054e-02  3.84498769e-02\n",
      "   1.64026923e-03 -7.07315357e-02 -7.36106051e-02  1.17166732e-02\n",
      "  -6.86814548e-02  5.90499020e-02 -3.74258883e-02 -3.88646445e-02\n",
      "   5.12813214e-02  5.29106036e-05  0.00000000e+00  2.98531359e-02\n",
      "   8.22715232e-02 -1.40273956e-01  3.74652015e-02  2.63391703e-03\n",
      "   0.00000000e+00  6.09094621e-03  5.45156920e-03  1.32316863e-02\n",
      "   0.00000000e+00  1.18279391e-02  2.66767345e-04  2.52450429e-02\n",
      "  -5.79647897e-02]\n",
      " [ 2.42450702e-04 -4.73027731e-02 -1.04450769e-01  3.14780745e-02\n",
      "   9.13159894e-03  2.35898143e-02 -8.64035554e-03  0.00000000e+00\n",
      "   9.35011461e-03  8.42385789e-03  2.94303540e-02 -4.72449852e-02\n",
      "   1.90344499e-02  3.90327023e-02 -1.11975948e-02 -7.09202807e-02\n",
      "  -3.71710469e-02 -7.13027509e-02 -5.20315735e-02  4.79187991e-02\n",
      "   2.14874252e-03  2.55268161e-03 -8.38161805e-02  1.54093708e-02\n",
      "   1.11446700e-02  4.34389444e-03 -4.09632868e-02  8.63769545e-03\n",
      "   6.49070547e-02  3.42116237e-02  0.00000000e+00  3.62058801e-02\n",
      "   7.11039924e-02 -8.84966665e-03 -1.57197332e-02  3.23190903e-03\n",
      "   0.00000000e+00  7.66256642e-03  6.65332652e-03  1.80276865e-02\n",
      "   0.00000000e+00  1.45193810e-02  1.34876783e-02 -2.44706900e-02\n",
      "  -5.84878538e-02]\n",
      " [ 3.46902127e-04 -3.70807776e-02  1.05181900e-01  1.47199217e-02\n",
      "   9.15658222e-03  4.61003482e-02  1.65645099e-02  0.00000000e+00\n",
      "   1.44123375e-02  7.24904044e-03  2.61029347e-02 -1.31813147e-03\n",
      "   1.99638057e-02  5.49516558e-02  6.39217673e-03 -1.88543907e-02\n",
      "   3.91524641e-02  2.82197517e-02  1.65593186e-02  5.35293256e-02\n",
      "   3.33091826e-03 -4.76798644e-02 -3.52589188e-02  1.95461442e-02\n",
      "   1.45938266e-02  5.33929194e-02  3.58148637e-02 -1.81643145e-02\n",
      "   7.78276140e-02  3.49822370e-02  0.00000000e+00  1.32757774e-02\n",
      "   9.23837297e-02 -5.56926859e-02  6.48331550e-03 -3.46846777e-02\n",
      "   0.00000000e+00  6.95627690e-03  7.76753109e-03  1.25653033e-02\n",
      "   0.00000000e+00  1.93874717e-02  1.58341466e-02  3.08330175e-02\n",
      "  -3.97687085e-02]]\n",
      "db3 is \n",
      " [[ 0.10775862]\n",
      " [ 0.04475268]\n",
      " [-0.01807172]\n",
      " [-0.1152066 ]\n",
      " [ 0.07711892]\n",
      " [-0.0009417 ]\n",
      " [-0.10281053]\n",
      " [-0.01319818]\n",
      " [ 0.0074315 ]\n",
      " [ 0.013167  ]]\n",
      "dW2 is \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "db2 is \n",
      " [[-0.00176379]\n",
      " [-0.00647535]\n",
      " [-0.01265247]\n",
      " [-0.01166367]\n",
      " [ 0.00025278]\n",
      " [ 0.00195575]\n",
      " [ 0.01203626]\n",
      " [ 0.        ]\n",
      " [-0.01689166]\n",
      " [ 0.0010036 ]\n",
      " [ 0.02371025]\n",
      " [ 0.01141598]\n",
      " [-0.01711318]\n",
      " [ 0.01050311]\n",
      " [ 0.01582594]\n",
      " [ 0.01155441]\n",
      " [-0.03154577]\n",
      " [-0.0106516 ]\n",
      " [ 0.03260061]\n",
      " [ 0.0061771 ]\n",
      " [ 0.01500451]\n",
      " [ 0.0043473 ]\n",
      " [-0.00434033]\n",
      " [ 0.03167533]\n",
      " [-0.02744072]\n",
      " [ 0.01364798]\n",
      " [ 0.00143968]\n",
      " [-0.00161699]\n",
      " [-0.0027219 ]\n",
      " [ 0.00268566]\n",
      " [ 0.        ]\n",
      " [-0.02784093]\n",
      " [ 0.01668644]\n",
      " [ 0.0272768 ]\n",
      " [ 0.0071973 ]\n",
      " [-0.00330519]\n",
      " [ 0.        ]\n",
      " [ 0.00703377]\n",
      " [ 0.01637545]\n",
      " [ 0.02270815]\n",
      " [ 0.        ]\n",
      " [ 0.02645898]\n",
      " [-0.01507931]\n",
      " [ 0.00899533]\n",
      " [ 0.03281862]]\n"
     ]
    }
   ],
   "source": [
    "# Lets test it\n",
    "x_sample = train_df_X[:, 10:20]\n",
    "y_sample = train_df_y[:, 10:20]\n",
    "\n",
    "HL, memories = multi_layer_forward(x_sample, parameters=parameters)\n",
    "gradients  = multi_layer_backward(HL, y_sample, memories)\n",
    "print('dW3 is \\n', gradients['dW2'])\n",
    "print('db3 is \\n', gradients['db2'])\n",
    "print('dW2 is \\n', gradients['dW1'])\n",
    "print('db2 is \\n', gradients['db1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dH1', 'dW2', 'db2', 'dH0', 'dW1', 'db1'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients.keys() # these are generated as we have only 1 hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "\n",
    "    # parameters is the python dictionary containing the parameters W and b for all the layers\n",
    "    # gradients is the python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    # returns updated weights after applying the gradient descent update\n",
    "\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * gradients[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * gradients[\"db\" + str(l+1)]\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we are ready to build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_model(X, Y, dimensions, learning_rate = 0.0075, num_iterations = 3000, print_loss=False):\n",
    "    \n",
    "    # X and Y are the input training datasets\n",
    "    # learning_rate, num_iterations are gradient descent optimization parameters\n",
    "    # returns updated parameters\n",
    "\n",
    "    np.random.seed(2)\n",
    "    losses = []                         # keep track of loss\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialize_parameters(dimensions)\n",
    " \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        HL, memories = multi_layer_forward(X, parameters)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(HL, Y)\n",
    "    \n",
    "        # Backward propagation\n",
    "        gradients = multi_layer_backward(HL, Y, memories)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "                \n",
    "        # Printing the loss every 100 training example\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
    "            losses.append(loss)\n",
    "            \n",
    "    # plotting the loss\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally We are ready, I'm so excited "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 5000) (10, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Lets test for first 5000 data\n",
    "train_df_X_1 = train_df_X[:,0:5000]\n",
    "train_df_y_1 = train_df_y[:,0:5000]\n",
    "print(train_df_X_1.shape,train_df_y_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784, 45, 10]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 2.422624\n",
      "Loss after iteration 100: 2.129232\n",
      "Loss after iteration 200: 1.876095\n",
      "Loss after iteration 300: 1.604213\n",
      "Loss after iteration 400: 1.350205\n",
      "Loss after iteration 500: 1.144823\n",
      "Loss after iteration 600: 0.990554\n",
      "Loss after iteration 700: 0.876603\n",
      "Loss after iteration 800: 0.791154\n",
      "Loss after iteration 900: 0.725441\n",
      "Loss after iteration 1000: 0.673485\n",
      "Loss after iteration 1100: 0.631386\n",
      "Loss after iteration 1200: 0.596598\n",
      "Loss after iteration 1300: 0.567342\n",
      "Loss after iteration 1400: 0.542346\n",
      "Loss after iteration 1500: 0.520746\n",
      "Loss after iteration 1600: 0.501865\n",
      "Loss after iteration 1700: 0.485205\n",
      "Loss after iteration 1800: 0.470368\n",
      "Loss after iteration 1900: 0.457054\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ahahaha, lets go\n",
    "params = ann_model(train_df_X_1,train_df_y_1,dimensions, num_iterations = 2000, print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets write the prediction function while it runs\n",
    "def predict(X,y,parameters):\n",
    "    # Forward propagation\n",
    "    probas, memo = multi_layer_forward(X, parameters)\n",
    "    \n",
    "    p = np.argmax(probas, axis = 0)\n",
    "    act = np.argmax(y, axis = 0)\n",
    "    \n",
    "    acu = np.sum((p==act)/X.shape[1])\n",
    "    return p, acu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8774000000000002\n"
     ]
    }
   ],
   "source": [
    "train_df_y_pred, accuracy = predict(train_df_X_1,train_df_y_1,params)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pretty good for 5000 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8304000000000001\n"
     ]
    }
   ],
   "source": [
    "# lets try predict on test set using these params\n",
    "test_X_1 =test_X[:,0:5000]\n",
    "test_y_1 = test_y[:,0:5000]\n",
    "test_pred, accuracy = predict(test_X_1,test_y_1,params)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I am okay with it we will include more samples during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'True Label: 5 Predicted label: 0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAABlCAYAAAAI2qyuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUfElEQVR4nO2de3Bc1X3HP79d7VsrybIse5EsyZIlGxtjHpJNAtSeIXYT0kxa8iiE0jzIJJnC5DHQvCZtaQopbSGdzrQkIVNCSkiZZEJdQsmQkKkT3lgYGxv5gYVsaS3J0sqrh1e7q9Xu6R/37vZKlq5laVcS0vnMaLT3nnPP+d1z7/ec8zv33HNFKYVGo5kax0IboNEsZrRANBobtEA0Ghu0QDQaG7RANBobtEA0GhuWlEBEZKeIhOf72IVERD4lIi9Yts+JSP085LtXRD47Tdg9IvKTGabzqIjcO0sbZn3sTLEViFnY2b+MiMQt27cWyqjJF30xIiInJ5XHr23iPioiY2a8syLyGxHZWAi7lFLFSql37OKISJ2IKBEpKoQNixUR+YSInBKRmIjsEZHyCx1jKxCzsIuVUsVAJ/Ahy77HLRkvq4K2YC2P3ReI+49mOVYDfcCjkyOIwZJq1RcLIrIZ+AFwG7AaGAUeutBxs7oY2e6IiHxNRHqBH01V65u11Hrzt0dEHhCRThE5IyLfFxHfLPL+tIgcEZEREXlHRD4/RZxvikjErOVvtezPiw1zQSk1CvwUuMy0aa+I3CciL2JctHoR2Wi2MmdF5JiIfNxyDitF5CkRGRaR14AGa/qTytwnIg+ateaQiLxgnu/vzeiDZqv2HjP+Z8yyjYrIsyJSa0l3l4gcNdP5V0Bmes4i8nMR6TWP/b15s1qpMM93RER+NynfacviIrkV+KVS6vdKqXPAXwE3iUjQ7qC51FZrgHKgFvjcDOLfDzQBVwDrgSrgr2eRbx/wR0AJ8Gngn0Xkqkl2VZjpfxJ4WEQ2XKwNIvKQiFyohnlcRPpF5NcisnUmxotIMcbFesOy+zaMMgwC/cBvMERUCdwMPCQim8y4/wYkgBDwGfNvOh4Argbei3GtvgpkgD8ww8vM1u9lEfkw8E3gJmAV8Dzwn6bNFcCTwLcwyrYduHYm52vyK6DRPJ/9wOOTwm8F/s5M+0A2XEQCFyiLCYjIoIhcN40Nm4GD2Q2lVDswhnE/TI9SakZ/wEngfebvnWbiXkv4p4AXJh2jMG5EAWJAgyXsPUDHNHmdl5aNXXuAL1nsGgcClvCfYdQWtjaYx4YvojyuBXyAH/gG0Itxw00V91GMm3rQjPdU1g5gL/BtS9w/BZ6fdPwPgL8BnEAK2GgJ+461rCxl7gDiwNYp7Kkz4xVZ9v0KuN2y7cBo0WqBPwdesYQJEAY+O8353gP8ZJqwMjPvUkvZPGEJLwbSwFq7srAce+8Mr9dvgS9M2nca2Gl33Fx8h36lVGKGcVdh3Eivi+RaZsG44BeFiHwA42ZpwriIfuCQJUpUKRWzbJ8CLsmnDQBKqRctm38vIp8Ergd+Oc0hDyilvjVNWJfldy2wXUQGLfuKgMcwzqFoUvxT06RZAXgxavuZUAv8i4g8aNknGK3sJdY8lVJKRLqYASLiBO4DPmban7HYN2T+tqZ9TkTOmnnalcXFcg6j12GlBBixO2guApk8DTiGcQMCICJrLGERjNpss1Lq9GwzFBEP8AuMGu2/lVIpEdnDxP7wChEJWERSAxzOlw02KC6iXz7FsVm6gN8ppXZNjmTebOMYtetRc3fNNGlGMFqtBixdiynys+Z7n7IMvljybTTzzG6LdfsCfAL4MPA+jF5IKRBlYllZ0y7G6A52Y1MWs+AtINcNFmMo3AMctzsonyMmB4HNInKFiHgxmlkAlFIZ4IcY/kKlaWCViPyhTXoiIl7rH+DGOKl+YNxsTaYaPfpbEXGLyPUY/srPZ2nDdIbViMi1Zh5eEflLjBrxxQsdOwOeBppE5DYRcZl/LSJyqVIqjeEL3CMifrMv/smpEjHP9xHguyJyiYg4ReQ9ZiXTj1GTW5+XfB/4RtaBFpFSEfmYGfY/GNf2JjFGLL+I4evNhCCQBAYwKtDvTBHnRhG5TkTcGL7IK0qpLruymGHeVh4HPiQi15u+zbeBJ5VSti1I3gSilDpuZvoc8DYw+TnG14ATwCsiMmzG28D0vBejxp/890UMvyKKUTs9Nem4XjOsG6NQvqCUyta2M7ZBjBGu709jWxD4npnPaeD9wAeUUgM25zMjzAu2G8Mh7TbP5x8wKgaAOzH66b0YffAf2SR3N0b3cx9w1kzHoYyRtPuAF03H9hql1H+Z4U+YZXMY+IBpUwSji3Q/xo3eyMwrg//A6AaeBtqAV6aI81OMbvNZjEGFP5thWUzAHJG7fqowpdRbwBcw7ok+jGv4FxcyXpR+YUqjmRb9UEqjsUELRKOxYVkKRETebz6VPSEiX19oezSLl2Xng5hDpceBXRgPu/YBtyil2hbUMM2iZDlOMtwGnFDmjFcReQJjnH5agVRUVKi6urr5sW4eOXnyJJFIZLbPbpYFy1EgVUx8Eh0Gtk+OJCKfw5xjVlNTQ2tr6/xYN480NzcvtAmLnmXpg8wEpdTDSqlmpVTzqlWrFtoczQKxHAVymonTJKrNfRrNeSxHgewDGkVknTm14WbOfxqv0QDL0AdRSo2LyJ3AsxgzeR8xpyFoNOex7AQCoJR6Bnhmoe3QLH6WpUAWknQ6zfj4OOl0mlQqNW28eDxOMpnE4XAgIgSDQdxuNy6Xi6IifdnmC13S80wymaS/v5/u7m4OHDjAdA9q33jjDd544w2CwSCBQIAPfvCDbNu2jfXr11NaWjrPVi9ftEAKhFKKTCaTe3VzfHycRCJBNBqls7OT48eP89Zb07s+7e3thMNhvF4vHo8nF7eyspKSkhIsb0VqCogWSIFIp9Mkk0kymQzpdJqenh5eeOEFurq6OHLkCO3t7Rw5cmTa48fHx8lkMjkhdHZ24vF4aGxspKqqCkCLZB7QAskz2ZYjFotx/Phxkskk8XicU6dOcfDgQSKRCB0dHQwNDeHz+XL+iNPpxOVy5XyMFStWUF4+cV0zp9NJScnk16o1hUQLJM9kMhkSiQRHjhzhzjvvJBKJEIlEci1JVkCVlZVs2rSJWCzG0NAQpaWllJWVsXLlSoLBIFdddRVbt05cSUhEWL9+PSKiW495Qgskz2QFkHXGo9Eoo6Oj58Vbt24du3fvJplMcu7cOYLBICUlJQQCAQKBAE1NTaxZM/G1bxHB45nybVNNgdACyTNZh3xkZIRIJEI8Hp8y3vXXX8/dd9+dG+7NOuNZ7FoJ3XrMH1ogeUZEcLlcVFdX89GPfpRwOMzhw4eJx+OcO3cuF6+oqAin04nD4aCoqCi3rVlcLMe5WAXF6XTi9Xppamri3nvv5Stf+Qpbt24lFAqdFzcrJp/Ph8vlWgBrNRdCtyAFINsqlJaWUl5ejs/nw+12A+DxeHJ+hoiQSqVIJBKMj48zNjaWe9I+MjLC8PDweWkXFxfj8Xjw+/25ES+Hw5Hb1uQXLZA8k/Ud3G43breblStXTmghiouLaWhooKKiAhHJOfMDAwP09vYSjUYZHBzk9ddf59VXXz0v7SuvvJJQKMSGDRsIhUKUlpbi8/nYtGmTFkgB0AKZJxwOozfrdDpxu910dnZy6NAhenp6OHr0KENDQ7kRr0Qiwdtvv000Gj0vnePHjzM4OEg0GqWkpASfz4fH4+HkyZOsXr2ampoagsEgpaWluVZLM3u0QOYZh8OBx+Ph0KFDdHR0cOzYMd58800ymQyZTCYXb7o5WgMDxuKN2ZGsbIuV7brdcccdXHnllezYsUMLJA9ogRSITCbD+Pg4586dY2BgIOdPjI6OcurUqZxz3tvbm3uACMbo1sV0ldLpdG6oOBaLsX//fgYGBli7di1VVVWUlZXprtcc0AIpEOl0esLkxKEhY6X/4eHhKZ1vEck5236//7zw6YjFYoyNjTE2NkYqleKpp57C7/dTVVVFS0sL27Zt0wKZA1ogBUIpRTKZzAkiFotNCHc6nXg8HioqKqipqaGyspKysjIuueSSKYeEp8ujo6ODgYEB9u7dSzgcJp1Ok8lk6O7upr29nauuugqllH64OEu0QApEJpMhHo8zODjIwMAA4+PjE8LdbjfFxcVs2bKFlpYWGhsbaWxspLa2lpmuopIVSDgcpr+/n/7+/tyQcVdXFy6Xi2QyWYjTWzZogRQIEaGoqAiv14vX683duNl927dvZ8eOHTQ0NLBx48YJc7EuhoqKCjweDzt37mTFihU899xz9PX10dbWxujoKCdOnCCdTlNRUaG7WrNAC6SAuFwuvF4vfr8/57S73W7Kysq45ppruP322wkGgxQXF88qfRGhpKSE4uJidu/eTV1dHW1tbfT29nLs2DFOnz7Nyy+/TCqVorS0VAtkFmiBFIiioiICgQBbtmzhrrvuIhKJEA6HWbVqFQ0NDbS0tFBSUpK3mzYQCFBRUYHX683tU0oxPDxMNBqddthYY48WSIFwOp34/X5qamq47bbbGBgY4M0336S+vp6mpia8Xi8+X34+0S4ieL3e8x4OZjIZhoeHc++jaC4eLZACk52T5fV6CQaD+P1+fD6fnrn7LkELpMA4nU58Ph8+n4+ysrKFNkdzkWiBLAGUUrzzzju0trZy5syZ3H6n08maNWtYt26dbrFmiX4fZAmglOLYsWMcOHCAwcHB3H6n08mqVatYuXJlbrKk5uLQLUieyK5/lf0N/z+Dt5BPsVOpFKlUiu7ubk6dOkU8HsfpdHL11VdTX19PS0sLNTU1ejXGWaJLLY9kF4qzrmdV6BVI0uk0Y2NjRKNRuru7GRsbw+Fw0NTUxIYNG6iqqtIrMc4BLZA8MTo6yuHDhwmHw7z88svU1taya9cuysvLWbVqVUFEopQikUgwODjImTNnOHPmDMlkMve03u/3667VHNECyRPJZJLW1lYOHz7MI488QktLC7W1tTQ1NeXeHiwEiUSCkZERotFobpawy+XC4/Hg9Xr1JMU5smSrFxFZKyL/KyJtIvKWiHzJ3F8uIr8RkbfN/yvykZ/T6aSsrCw3Vb29vZ3HHnuMPXv20NXVRTQazc20zQeJRILh4WGeeeYZHnroIdrajG+QZh8YXnfdddxwww16Ha05smQFAowDdymlNgHXAHeIyCbg68BvlVKNwG/N7TkjIhQXF+P3+ykqKmJoaIi9e/dy8OBBTp8+TTQaza23m3XoZzP9w7ow3fDwMPv37+fZZ5+lv78fMBaFCAaDNDQ0EAqF9PyrObJku1hKqR6gx/w9IiJHML5w+2Fgpxntx8Be4Gtzzc/n83HttdcSCoXo7Oyks7OT1157jeeff57BwUEaGxvZsWMH1dXV1NTU5Lo+xcXFFzVZsauri3A4zEsvvURHRwd79+6lr6+PZDKJy+Xilltu4bLLLqO2thafz6d9kDmyZAViRUTqgCuBV4HVpngAeoHV0xwz4TPQF6KoqIjy8nLGx8fZtGkTAC+99BI9PT25IViXy8WGDRvO8w2y23b+QrbF6enpobW1lRdffJEDBw7Q39/P6OgoRUVF+P1+Nm/ezPbt2wkEAnpoNw8s+RIUkWLgF8CXlVLD1ptQKaVEZMp+jlLqYeBhgObm5gv2hbLplpaW8pGPfIQNGzbQ2dlJV1cXHR0ddHV1sWfPHkpKSnjyySdzgqiqqqK+vp7q6mpqa2spKSkhGAzm0h0eHmZkZIQTJ07Q09PDwYMHOXr0KH19fYyMjCAilJWVcfPNN3PppZeye/duKisr9YINeWJJC0REXBjieFwp9aS5+4yIhJRSPSISAvrymB9ut5tQKMTo6Cj19fWISO75RGdnZ84HyRIKhVi/fj0bN24kEokQCoUmvHIbDofp6+vj0KFDtLe3c+zYMfr6+nA4HDgcDlasWEEwGOTyyy+npaWFyspK/H6/nlqSJ2SpvicgRpX+Y+CsUurLlv3/BAwope4Xka8D5Uqpr9ql1dzcrFpbW2eUb9aJTiQS9PX10dnZydNPP00kEsl9NerkyZO5+B6PB5/Pl/NFJk+Dj8fjJBIJhoaGiMfjxGIxUqkUGzdupLa2lptuuoktW7awbt263KqLTqdzRsO7zc3NtLa26nFgG5ZyC3ItcBtwSEQOmPu+CdwP/ExEbgdOAR/PZ6YiknsXZO3atfh8Prq7uwmHw4DxjGJoaCjXimQ/zRaLxeju7s4JLJuWddqKw+HA7Xbj8/mor6+nrq6OlpYWGhoa8Pl82ucoAEu2RJVSLwDT1Y43FDr/rFDKy8u58cYbSaVSudYgFovlulptbW3s27ePrq4u2traiEQinD179rz01qxZw+rVq9m1axebN2/m8ssvp6KigvLycjwejx6tKhBLViCLAesavdmWIfulKTC6Y4FAgHg8jtvtJpVKTdsSrFu3jurqarZu3coVV1xBdXU1gUAg95loTWHQApknsgvDORyOCQ50XV0doVCIVCrF2NhY7puFk8mu5B4IBHLfMtTiKDxaIPOIdYZvluyyQJrFie64ajQ2aIFoNDZogWg0NmiBaDQ2aIFoNDZogWg0NizZuVj5RET6gRgQWWhb5kgFE8+hVik1s28tLFO0QGaIiLQqpZoX2o65sBTOYb7RXSyNxgYtEI3GBi2QmfPwQhuQB5bCOcwr2gfRaGzQLYhGY4MWiEZjgxbIBRCR94vIMRE5Yb7DvuixWVXyHhE5LSIHzL8bF9rWxY72QWwQESdwHNgFhIF9wC1KqbYFNewCmKu1hJRS+0UkCLwO/DHG+/fnlFIPLKR97yZ0C2LPNuCEUuodpdQY8ATGyoyLGqVUj1Jqv/l7BMiuKqm5SLRA7KkCuizbYd5lN9qkVSUB7hSRN0XkkXwt3L2U0QJZwkxeVRL4HtAAXIGxbvGDC2fduwMtEHtOA2st29XmvkXPVKtKKqXOKKXSSqkM8EOMLqTGBi0Qe/YBjSKyTkTcwM3AUwts0wUxV5X8d+CIUuq7lv0hS7Q/AQ7Pt23vNvSqJjYopcZF5E7gWcAJPKKUemuBzZoJ060qeYuIXAEo4CTw+YUw7t2EHubVaGzQXSyNxgYtEI3GBi0QjcYGLRCNxgYtEI3GBi0QjcYGLRCNxob/A0dg0J9AWmA8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets play with it in test set\n",
    "import random\n",
    "n= random.randint(0,5000)\n",
    "pict = test_X_1[:,n]\n",
    "truelabel = np.argmax(test_y_1, axis=0)[n]\n",
    "predictedlabel= int(test_pred[n])\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(pict.reshape(28,28),cmap=\"Greys\")\n",
    "plt.title(\"True Label: \"+str(truelabel)+\" Predicted label: \"+str(predictedlabel))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
