{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577069d9",
   "metadata": {},
   "source": [
    "# Project\n",
    "- Build a complete neural network using Numpy. \n",
    "- Implement all the steps required to build a network - feedforward, loss computation, backpropagation, weight updates etc\n",
    "- Dataset: MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82ceec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c39437",
   "metadata": {},
   "source": [
    "- The MNIST dataset we use here is 'mnist.pkl.gz' which is divided into training, validation and test data.\n",
    "- We must read data first before making any progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68872b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8691a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='mnist.pkl.gz'\n",
    "file = gzip.open(filename,'rb')\n",
    "file.seek(0) # Position pointer to 0\n",
    "train_set, validation_set, test_set = pickle.load(file,encoding=\"latin1\")\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e86deac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([5, 0, 4, ..., 8, 4, 8], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7dc9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01171875, 0.0703125 , 0.0703125 ,\n",
       "       0.0703125 , 0.4921875 , 0.53125   , 0.68359375, 0.1015625 ,\n",
       "       0.6484375 , 0.99609375, 0.96484375, 0.49609375, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.1171875 , 0.140625  , 0.3671875 , 0.6015625 ,\n",
       "       0.6640625 , 0.98828125, 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.98828125, 0.87890625, 0.671875  , 0.98828125, 0.9453125 ,\n",
       "       0.76171875, 0.25      , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19140625, 0.9296875 ,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.36328125,\n",
       "       0.3203125 , 0.3203125 , 0.21875   , 0.15234375, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.0703125 , 0.85546875, 0.98828125, 0.98828125,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.7734375 , 0.7109375 ,\n",
       "       0.96484375, 0.94140625, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.3125    , 0.609375  , 0.41796875, 0.98828125, 0.98828125,\n",
       "       0.80078125, 0.04296875, 0.        , 0.16796875, 0.6015625 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.0546875 ,\n",
       "       0.00390625, 0.6015625 , 0.98828125, 0.3515625 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54296875,\n",
       "       0.98828125, 0.7421875 , 0.0078125 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04296875, 0.7421875 , 0.98828125,\n",
       "       0.2734375 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13671875, 0.94140625, 0.87890625, 0.625     ,\n",
       "       0.421875  , 0.00390625, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31640625, 0.9375    , 0.98828125, 0.98828125, 0.46484375,\n",
       "       0.09765625, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17578125,\n",
       "       0.7265625 , 0.98828125, 0.98828125, 0.5859375 , 0.10546875,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0625    , 0.36328125,\n",
       "       0.984375  , 0.98828125, 0.73046875, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.97265625, 0.98828125,\n",
       "       0.97265625, 0.25      , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.1796875 , 0.5078125 ,\n",
       "       0.71484375, 0.98828125, 0.98828125, 0.80859375, 0.0078125 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15234375,\n",
       "       0.578125  , 0.89453125, 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.9765625 , 0.7109375 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09375   , 0.4453125 , 0.86328125, 0.98828125, 0.98828125,\n",
       "       0.98828125, 0.98828125, 0.78515625, 0.3046875 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.08984375, 0.2578125 , 0.83203125, 0.98828125,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.7734375 , 0.31640625,\n",
       "       0.0078125 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.0703125 , 0.66796875, 0.85546875,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.76171875,\n",
       "       0.3125    , 0.03515625, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21484375, 0.671875  ,\n",
       "       0.8828125 , 0.98828125, 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.953125  , 0.51953125, 0.04296875, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53125   , 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.828125  , 0.52734375, 0.515625  , 0.0625    , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37e52bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 8, 4, 8], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c5710d",
   "metadata": {},
   "source": [
    "- ok, so feature set is train_set[0] and target is train_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b11fa8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,)\n",
      "(10000, 784) (10000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Shape of all the data \n",
    "print(train_set[0].shape,train_set[1].shape)\n",
    "print(validation_set[0].shape,validation_set[1].shape)\n",
    "print(test_set[0].shape,test_set[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929b5a9",
   "metadata": {},
   "source": [
    "- The target variable is like labels for each feature\n",
    "- So, instead of having single label we prefer one hot encoding where the target is of the dimension 10 x (length of target)\n",
    "- Why we are doing this? Ans - The output of the model is softmax output of length 10\n",
    "- We have to do this for all the datasets - training, validation and test\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f352e73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eb35021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to create one hot encoding\n",
    "\n",
    "def onehot(val_list):\n",
    "    length = len(val_list)\n",
    "    # Create a blank array \n",
    "    barray = np.zeros((10,length))\n",
    "    # Now have to put 1 at the correct place based on input\n",
    "    col = 0\n",
    "    for i in val_list:\n",
    "        barray[i][col] = 1.0 # put 1 according to the value and move to next column\n",
    "        col +=1\n",
    "    return barray\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab1793ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets test it with only 1 value \n",
    "arr = np.array([4]) # 4th  position will be filled (starting with 0)\n",
    "onehot(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94bd8bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing will all digits\n",
    "arr = np.array([5,1,2,0,4,8,6,9,7,3])\n",
    "onehot(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5742a88",
   "metadata": {},
   "source": [
    "- This is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "204a73b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,)\n"
     ]
    }
   ],
   "source": [
    "# Now time to mould target sets \n",
    "train_set_X = np.array(train_set[0][:])\n",
    "train_set_Y = np.array(train_set[1][:])\n",
    "print(train_set_X.shape,train_set_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e1a2073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50000)\n"
     ]
    }
   ],
   "source": [
    "# Need to transpose the feature set\n",
    "train_set_X = train_set_X.T\n",
    "print(train_set_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07578d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do the same for other sets\n",
    "validation_set_X = np.array(validation_set[0][:])\n",
    "validation_set_Y = np.array(validation_set[1][:])\n",
    "\n",
    "test_set_X = np.array(validation_set[0][:])\n",
    "test_set_Y = np.array(validation_set[1][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8674b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying one hot encoding to target variables\n",
    "train_set_Y = onehot(train_set_Y)\n",
    "validation_set_Y = onehot(validation_set_Y)\n",
    "test_set_Y = onehot(test_set_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6ed2202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50000) (10, 10000) (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(train_set_Y.shape,validation_set_Y.shape,test_set_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcf2d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "300fac00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e2f35b0520>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets check a random image \n",
    "img_no = random.randint(0,1000)\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(train_set_X[:,img_no].reshape(28,28),cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c55e27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward - Activation Functions\n",
    "def sigmoid(x):\n",
    "    out = 1/(1+np.exp(-x))\n",
    "    prev_input =x\n",
    "    return out, prev_input  # layer out put h and hprev\n",
    "\n",
    "def relu(x):\n",
    "    out = np.maximum(x,0)\n",
    "    prev_input=x\n",
    "    assert(out.shape==x.shape)\n",
    "    return out,prev_input\n",
    "\n",
    "def softmax(x):\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp,axis = 0, keepdims = True)\n",
    "    out = x_exp/x_sum\n",
    "    prev_input=x\n",
    "    return out, prev_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cc6fecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   -1]\n",
      " [   0    3]\n",
      " [-100  100]\n",
      " [  -4    4]] (array([[  2,   0],\n",
      "       [  0,   3],\n",
      "       [  0, 100],\n",
      "       [  0,   4]]), array([[   2,   -1],\n",
      "       [   0,    3],\n",
      "       [-100,  100],\n",
      "       [  -4,    4]]))\n",
      "[[   2   -1]\n",
      " [   0    3]\n",
      " [-100  100]\n",
      " [  -4    4]] (array([[8.78878243e-01, 1.36853947e-44],\n",
      "       [1.18943236e-01, 7.47197234e-43],\n",
      "       [4.42477874e-45, 1.00000000e+00],\n",
      "       [2.17852136e-03, 2.03109266e-42]]), array([[   2,   -1],\n",
      "       [   0,    3],\n",
      "       [-100,  100],\n",
      "       [  -4,    4]]))\n",
      "[[   2   -1]\n",
      " [   0    3]\n",
      " [-100  100]\n",
      " [  -4    4]] (array([[8.80797078e-01, 2.68941421e-01],\n",
      "       [5.00000000e-01, 9.52574127e-01],\n",
      "       [3.72007598e-44, 1.00000000e+00],\n",
      "       [1.79862100e-02, 9.82013790e-01]]), array([[   2,   -1],\n",
      "       [   0,    3],\n",
      "       [-100,  100],\n",
      "       [  -4,    4]]))\n"
     ]
    }
   ],
   "source": [
    "z=np.array([2,-1,0,3,-100,100,-4,4]).reshape(4,2)\n",
    "print(z,relu(z))\n",
    "print(z,softmax(z))\n",
    "print(z,sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f5ca28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_count = [784,45,22,10] # Number of neurons in each count\n",
    "\n",
    "def init_parameters(list_neurons):\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(list_neurons)\n",
    "    \n",
    "    for i in range(1,L):\n",
    "        parameters[\"W\"+str(i)] = np.random.randn(list_neurons[i], list_neurons[i-1]) * 0.1\n",
    "        parameters['b' + str(i)] = np.zeros((list_neurons[i], 1))  \n",
    "        \n",
    "        assert(parameters['W' + str(i)].shape == (list_neurons[i], list_neurons[i-1]))\n",
    "        assert(parameters['b' + str(i)].shape == (list_neurons[i], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbbde074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.17971583  0.17749536 -0.12167796 ...  0.00298204 -0.06232611\n",
      "   0.16599064]\n",
      " [ 0.09587989  0.12983142  0.0457076  ... -0.06008387 -0.07041519\n",
      "  -0.0734001 ]\n",
      " [ 0.0590585   0.05864395 -0.10490222 ... -0.01370336  0.06514421\n",
      "  -0.21283107]\n",
      " ...\n",
      " [-0.10664249 -0.09398566 -0.1942275  ...  0.10216626 -0.04097427\n",
      "   0.14833526]\n",
      " [ 0.0184405   0.09614598 -0.10738451 ... -0.04451764  0.00075468\n",
      "  -0.08716545]\n",
      " [-0.07470964 -0.01355043  0.17611045 ... -0.03429762  0.04960504\n",
      "   0.00597265]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 1.72344103e-02 -1.13610539e-01  2.01949916e-02  7.89692777e-02\n",
      "   2.00842425e-01  9.85159601e-02  8.05505900e-02  5.08365088e-02\n",
      "   3.62990371e-02  1.48109756e-01  1.44019593e-01  8.09235471e-02\n",
      "  -8.93075262e-02 -2.53135009e-02 -5.76806331e-02 -2.17695433e-01\n",
      "   8.54922879e-02 -2.93873559e-01  7.28928048e-02  4.59959893e-02\n",
      "   1.04158375e-01  8.95380496e-02 -7.15157047e-02  6.36147378e-02\n",
      "  -8.71422233e-02  3.06547194e-02 -1.00800821e-01 -1.01650188e-02\n",
      "   3.43780771e-02  5.23849069e-02  9.75222561e-03 -1.17720589e-01\n",
      "   1.71436253e-01  1.39049938e-01  5.92244697e-02 -5.41370817e-03\n",
      "  -3.39794721e-03  3.76168330e-02  9.91399981e-02  1.42078650e-02\n",
      "  -5.04187146e-02 -5.14676683e-02  2.04079980e-02 -1.46881246e-01\n",
      "   1.02143685e-01]\n",
      " [ 4.41818900e-02 -8.03621750e-03  1.02047149e-02 -6.54375108e-02\n",
      "  -1.08865400e-02  1.81607238e-01  1.24228654e-02 -1.02229013e-01\n",
      "  -1.44486133e-02 -6.62507488e-02 -1.79934173e-02  1.16738896e-01\n",
      "  -4.53235789e-03 -1.28537276e-01 -1.02054459e-02 -9.91787868e-02\n",
      "  -1.45655857e-01  7.71045972e-02  1.01464262e-01 -5.45664661e-02\n",
      "  -1.28073814e-01  1.31169156e-01  1.46723056e-01  8.64357426e-02\n",
      "   2.31726421e-02  1.39426331e-01 -1.88060660e-01 -1.37901033e-01\n",
      "  -6.84013802e-04  8.15554981e-03  1.21576853e-01  3.12099183e-02\n",
      "   1.05064460e-02 -4.22783927e-02  1.19967298e-01 -2.84787623e-02\n",
      "  -3.76321109e-02 -5.46898116e-02  1.32045430e-01  9.62146119e-02\n",
      "  -1.94349640e-02 -1.30310600e-01  6.76021053e-02 -3.35291085e-02\n",
      "  -1.16602163e-01]\n",
      " [-1.00880068e-01  9.86639473e-02 -2.66457268e-02  1.08910150e-01\n",
      "   4.19152894e-02 -6.05827321e-02  6.71694984e-02 -1.21546438e-01\n",
      "  -4.05561129e-02  9.25710588e-02  4.23817542e-02  5.06475664e-02\n",
      "  -1.00231264e-01  3.56376657e-02  5.55983836e-02 -9.79379465e-02\n",
      "   9.17928591e-03  7.33027433e-02  1.35610285e-01 -1.41062455e-02\n",
      "   3.86292468e-02  4.39581804e-02  7.59750065e-02 -1.84485896e-02\n",
      "  -2.65591470e-01 -2.01373400e-02 -1.85820365e-02  1.38260012e-01\n",
      "   7.42235767e-02  1.90759663e-02 -2.30479472e-02  1.15165648e-01\n",
      "   9.41606392e-02  1.47631221e-01 -7.85278764e-02 -2.27449608e-02\n",
      "   6.03066118e-04  6.05747313e-02  1.66890026e-02 -1.16162594e-02\n",
      "  -2.53955267e-01 -5.81030566e-02  1.04722566e-01  1.13438146e-02\n",
      "   3.37585080e-02]\n",
      " [ 3.44095346e-02  7.99197088e-02  2.52635605e-02  1.65689365e-01\n",
      "   9.50698733e-03  4.84491132e-02 -1.22155228e-01 -1.05484944e-01\n",
      "  -8.82635844e-02 -9.76028705e-02 -5.63138022e-02 -3.42402033e-02\n",
      "   1.35908493e-02 -2.98256710e-03 -1.12949899e-02  8.94854316e-02\n",
      "   3.26121250e-02  6.22244490e-02 -1.04037662e-01  1.10730873e-02\n",
      "   9.13377892e-02  8.13758403e-02  4.86343469e-02 -6.70379829e-02\n",
      "   2.40451806e-02 -9.41433431e-02  8.03903122e-02 -8.57128991e-02\n",
      "  -8.99329991e-02  1.13887680e-01  2.18867153e-02  1.49334627e-02\n",
      "   2.55460181e-01  1.02045629e-01 -3.11851150e-02 -7.06311731e-02\n",
      "   5.06907507e-02  1.02824839e-01 -1.11300339e-01  7.88612372e-02\n",
      "   1.66677629e-02 -3.12160885e-02  3.94712578e-02  7.40513012e-02\n",
      "  -2.39978154e-02]\n",
      " [ 1.04600382e-03  4.76018239e-02  3.17386775e-02  1.07036909e-01\n",
      "   1.72769417e-01 -4.77321464e-02 -1.10648378e-01 -1.12912580e-02\n",
      "   7.26033524e-02 -6.14485955e-02 -1.18213827e-01  8.07053390e-02\n",
      "  -4.28429025e-02  2.36352363e-01  1.04371370e-01  6.39802109e-03\n",
      "   7.79952352e-02  3.01629370e-02 -4.48946218e-02  1.03593210e-01\n",
      "  -7.66947173e-02  2.56920655e-02  3.55223174e-02  1.57150309e-01\n",
      "  -2.62856148e-01 -1.84469643e-01  5.24919835e-02 -1.14511190e-03\n",
      "   8.74231608e-02  5.13998963e-02 -3.32599608e-02 -5.06237201e-02\n",
      "  -9.79194503e-02 -2.09379687e-01 -1.33480996e-01  1.54389013e-01\n",
      "   1.38248005e-01  5.53725135e-03  1.41191498e-01 -8.12653018e-02\n",
      "   3.05435937e-02  3.01016951e-02  3.03166882e-02 -1.80447055e-01\n",
      "  -2.44504342e-02]\n",
      " [ 8.93564349e-02 -1.39856148e-01  9.31928798e-02  1.63497653e-02\n",
      "   5.36317074e-02  5.41828133e-02  1.36512270e-02  6.62862873e-02\n",
      "  -5.59281795e-02 -1.72981441e-01  1.46506102e-01  2.31687233e-01\n",
      "   1.21734936e-01 -1.58413752e-02  3.33478861e-02  1.10689076e-01\n",
      "   1.27385111e-01 -1.71372654e-01  4.47653287e-02 -5.30091599e-02\n",
      "   5.50772304e-02  1.32854126e-01 -3.34491442e-02  4.22798791e-02\n",
      "   1.56442535e-01  4.44550077e-02  1.52070379e-03 -5.17602771e-02\n",
      "  -7.13989411e-02 -9.86018653e-02 -4.09400626e-02 -2.35722786e-03\n",
      "   6.37149370e-02 -4.88815932e-02  6.10751130e-02  3.27617077e-03\n",
      "   3.40155522e-02  5.34571465e-02 -6.38084064e-02 -1.08727492e-01\n",
      "   7.02787756e-02 -1.36495502e-01 -7.59031650e-02 -3.97926676e-02\n",
      "   5.27286814e-02]\n",
      " [-2.04800415e-01  4.39621132e-02 -1.43502915e-01  2.47864359e-02\n",
      "  -2.11389772e-02  5.49578060e-02  9.74079659e-03  4.97017745e-02\n",
      "  -8.59044000e-02 -4.41282030e-02 -9.87378201e-02  8.48963718e-02\n",
      "   1.92515702e-01 -2.99881327e-02  8.78010902e-02  1.84439503e-01\n",
      "  -9.23801670e-02  1.46800703e-01 -1.00594172e-01  2.02145309e-01\n",
      "   9.76860186e-02 -5.53526663e-02 -1.53640688e-02 -1.04368634e-02\n",
      "  -4.00672667e-02  5.15200674e-02  4.97714015e-02 -1.38726917e-01\n",
      "  -6.85908049e-03 -1.16242982e-02 -3.82336462e-02 -9.29102299e-03\n",
      "  -1.56108578e-02 -7.41243594e-02 -1.00759547e-01  1.17598312e-01\n",
      "   1.53482067e-01  3.58507598e-02  1.27867668e-01  1.73572657e-03\n",
      "   1.56503623e-01  6.83771303e-03  1.76214066e-01  1.19172875e-01\n",
      "   8.27742427e-03]\n",
      " [-8.20107001e-02 -7.19188917e-02  1.24071139e-01 -1.81310325e-01\n",
      "   7.17673495e-02  1.01403155e-01 -9.62378090e-03 -1.50647337e-01\n",
      "   6.51787538e-02 -8.85642095e-03  5.16556967e-02 -4.94925495e-02\n",
      "   1.13215162e-01  7.37242663e-03  1.85093199e-01 -4.68257735e-02\n",
      "  -5.25645611e-02  4.27213244e-02  1.60629386e-01  6.31218106e-02\n",
      "   1.08041551e-03  1.54057735e-01  4.48311464e-02 -1.42607781e-01\n",
      "  -7.41583171e-02 -1.93468755e-01 -1.49583837e-01 -8.38255781e-03\n",
      "  -4.68419808e-02  1.16208726e-01  1.68147023e-01 -1.35970549e-02\n",
      "  -1.08728306e-01 -3.26985144e-02  6.00074054e-02 -5.93231824e-02\n",
      "  -6.34762680e-02  1.56274470e-01  5.63635983e-02 -1.62085969e-01\n",
      "   1.00051552e-01  1.87106815e-02 -1.46349260e-01  1.19154056e-01\n",
      "   1.98682753e-01]\n",
      " [-1.20723397e-01 -1.24703076e-01  1.39923831e-01 -3.77069571e-01\n",
      "  -1.08491276e-01 -6.14642047e-02 -1.37553979e-01 -6.12776964e-02\n",
      "  -2.25832649e-01  2.41979949e-02  1.01771054e-01  1.79915789e-02\n",
      "  -1.86470608e-02 -6.69549144e-02 -7.43996182e-02 -8.62346938e-02\n",
      "  -1.18042514e-01 -1.58543997e-01 -1.85784549e-01  1.36028867e-02\n",
      "  -5.39313500e-02  8.17122978e-02  6.19650030e-02 -3.36769678e-02\n",
      "  -5.92094040e-02  3.80399128e-02  9.01992103e-02  6.05864749e-02\n",
      "   8.24267880e-02  2.40673923e-01  3.31309915e-02  4.58796925e-02\n",
      "  -5.31601251e-02  1.76847785e-02  1.22519353e-01  1.04822056e-02\n",
      "  -2.27337350e-02 -1.25619386e-01  1.40470442e-02 -5.07621242e-02\n",
      "   1.09295929e-01 -2.65390297e-02  4.62773354e-02 -9.11922271e-02\n",
      "   2.86180780e-01]\n",
      " [ 2.51060542e-01  1.60271471e-02  1.22462478e-01 -2.51236507e-02\n",
      "   9.83841491e-02  2.08568361e-02  1.81939601e-01  4.05351171e-02\n",
      "  -6.28377296e-02 -9.51882649e-02  1.18517170e-01 -1.69733263e-02\n",
      "  -2.07073440e-01 -8.81044900e-02 -6.98219118e-02  5.43019205e-02\n",
      "  -2.44392105e-03  1.17060261e-01 -8.18297615e-03 -3.03927016e-03\n",
      "   5.23968527e-03  9.30696528e-02  9.77202812e-02 -7.29404848e-02\n",
      "   2.81568604e-02 -7.59360541e-02 -8.30328729e-02 -1.32203743e-02\n",
      "   8.64413416e-02 -8.96441904e-03  1.06580237e-03  8.74939085e-03\n",
      "   2.45963834e-02 -8.25396166e-02  3.07297486e-02  3.27222576e-02\n",
      "  -6.91728493e-03  1.70784972e-01  1.43099522e-01 -8.57816760e-02\n",
      "  -5.00205687e-02  5.99877299e-04  7.88246080e-02 -1.52393812e-02\n",
      "   7.76914553e-02]\n",
      " [ 2.52513333e-02 -7.24539945e-02  5.65511471e-02 -1.38347106e-01\n",
      "   2.16889124e-01 -2.12889419e-03 -6.99512790e-03 -4.52290083e-03\n",
      "  -6.86780095e-02  4.57322506e-02  7.38154935e-03 -1.08701380e-01\n",
      "  -1.76516230e-02  1.23948357e-01  5.31063844e-02  4.61127193e-02\n",
      "  -2.38008665e-01  1.05407120e-01 -2.80494073e-02  5.75751065e-02\n",
      "   2.01521421e-02  5.51780311e-02  9.83070998e-02  3.73768480e-02\n",
      "   3.47132812e-03 -1.26522369e-01  5.43762827e-02  1.16393975e-01\n",
      "   2.26531770e-02  8.57111456e-02  4.91892537e-02  7.16351682e-02\n",
      "  -1.93162453e-02 -5.43307453e-04 -1.65636974e-03  4.12207824e-02\n",
      "  -1.49268726e-01  1.56429000e-02  1.44005589e-01 -4.78608929e-02\n",
      "  -1.20012645e-02 -6.62956578e-02 -1.64790769e-01  7.90035019e-02\n",
      "  -1.45041593e-01]\n",
      " [-6.76536911e-03 -4.99422497e-03 -7.74390136e-02 -2.83431643e-01\n",
      "  -3.06783852e-02  3.71422691e-02 -7.25928332e-02 -4.57020482e-02\n",
      "  -4.40725933e-03 -8.45992075e-02  9.85368670e-02  1.22666782e-01\n",
      "   7.99159203e-02 -9.57878721e-02  7.42720282e-02 -4.83329797e-02\n",
      "   1.03780526e-01  1.62817690e-01  1.00247849e-01 -7.32043940e-02\n",
      "   8.76860795e-02  1.79620448e-01  3.02459604e-01  1.15247667e-01\n",
      "  -1.01651718e-01 -3.58797192e-02  3.66170932e-02  1.59793949e-03\n",
      "   7.27227605e-02  1.67330630e-01 -1.30572706e-01  5.23021516e-02\n",
      "  -7.26962166e-02 -4.89700993e-02  6.37164773e-02 -1.17502563e-01\n",
      "   1.00249711e-01  3.68051706e-02  8.10634459e-02 -8.07382743e-02\n",
      "  -2.35587117e-01 -9.32492408e-03 -1.74130523e-02 -3.55343342e-02\n",
      "  -8.12724757e-02]\n",
      " [-1.93455112e-01 -3.40828925e-02 -6.91029833e-02  1.27260785e-02\n",
      "  -2.98601188e-02 -6.32000585e-03 -2.98349846e-02  2.23474735e-02\n",
      "  -4.79913942e-02 -2.49461442e-01 -6.58244681e-02  1.33385535e-01\n",
      "   2.66641156e-03 -1.31966288e-01 -2.16807448e-01 -2.53349140e-02\n",
      "   4.40864176e-03  5.77966191e-02 -7.27336773e-02  8.23815722e-02\n",
      "   5.45350551e-03  3.12059592e-02 -5.44117612e-02  1.21914005e-02\n",
      "   2.09607566e-02  1.03363519e-02 -3.61329997e-02 -5.61560801e-04\n",
      "   9.46484104e-02  2.51320751e-02 -7.34955498e-02 -4.24730119e-02\n",
      "  -8.80624957e-02  5.29786820e-02 -1.28222892e-01  7.74794443e-02\n",
      "  -3.52278842e-01 -3.69532190e-02 -6.81828838e-02 -1.45147265e-01\n",
      "  -4.31168462e-02 -8.73808339e-02 -4.04386091e-02  5.75011617e-02\n",
      "  -4.77894784e-02]\n",
      " [ 7.80796993e-04 -1.22975054e-01  7.37930973e-02 -1.72499338e-01\n",
      "   5.08239908e-03 -2.61947865e-02  1.84073033e-02 -1.97568856e-01\n",
      "  -8.50206525e-02 -4.09936415e-02 -8.00544986e-02  4.11965833e-02\n",
      "  -8.92617645e-02  1.83888776e-02  2.28609984e-01 -3.65437451e-02\n",
      "  -1.43446150e-01 -9.09005175e-02 -2.28818310e-02  7.90383327e-03\n",
      "  -1.45877216e-02 -1.83906788e-02  1.04443466e-01 -8.19672900e-03\n",
      "  -6.67057886e-02  2.65185236e-02 -1.79182643e-02 -1.13163789e-01\n",
      "  -1.47163583e-01  1.23708678e-02 -5.66079583e-02 -1.34143583e-01\n",
      "  -7.18838222e-02  3.76123749e-02  1.77967420e-01  1.10497419e-02\n",
      "   8.06138668e-02 -1.42681403e-01  6.92564115e-02  5.07726620e-02\n",
      "  -8.45533500e-02  8.14544229e-02 -1.21989634e-01  2.18894609e-01\n",
      "   1.12059876e-01]\n",
      " [-4.56980964e-02 -1.98358411e-01  2.47787907e-01  5.04991502e-03\n",
      "   2.43619843e-02 -3.25059858e-02  8.43843863e-02  6.60088287e-03\n",
      "   1.29942167e-02  4.43274989e-02 -1.16574122e-01 -1.10689050e-01\n",
      "   9.91236765e-02 -2.51403811e-02 -1.45939002e-01  4.18215761e-02\n",
      "   9.40853501e-02 -4.46649932e-03 -4.37108975e-02 -1.10861804e-01\n",
      "   1.15397089e-01 -5.97791638e-03 -4.83854784e-02 -8.15733966e-02\n",
      "   4.08134244e-03  1.16873401e-01  7.11028894e-02 -1.38916155e-01\n",
      "   4.49525768e-02  1.38723716e-02  1.01884789e-01 -1.60345328e-01\n",
      "  -1.15868812e-01 -5.99429373e-02 -3.25291905e-02  1.77952911e-01\n",
      "   1.62712845e-02  8.32424309e-02 -7.88927534e-02  1.27527154e-01\n",
      "   2.54207846e-01  4.85823011e-02  7.29737170e-02 -1.24195073e-01\n",
      "  -7.80791032e-03]\n",
      " [-3.26415296e-02  3.63540010e-02  1.07699300e-01 -2.39156530e-02\n",
      "  -2.74301954e-02  9.39980194e-02 -7.87253020e-03  1.13235562e-01\n",
      "  -1.00437685e-01  1.31530955e-01 -5.53351170e-02  2.42391895e-02\n",
      "   1.02144641e-01  5.02351818e-02 -4.08774735e-03 -1.13586881e-02\n",
      "   5.88326329e-02  7.20266480e-03 -1.72647000e-01 -2.82252955e-02\n",
      "   1.87660698e-01  3.82550811e-02  2.72913495e-01 -8.50684546e-02\n",
      "  -1.19299679e-01 -5.70201937e-02 -2.53826118e-01  1.22154878e-02\n",
      "  -1.78322011e-01  1.60992874e-01  3.38318377e-02 -6.04834757e-02\n",
      "  -7.63689901e-03  8.62820721e-02 -3.09515486e-01  6.17200669e-02\n",
      "  -2.94954561e-05 -6.95890746e-02 -6.28022881e-02 -5.10736504e-02\n",
      "  -3.32352116e-02 -7.06391291e-02 -1.29094633e-01  1.98462820e-01\n",
      "   5.53009008e-03]\n",
      " [-6.19994536e-02  1.04388296e-01  2.04537971e-01 -7.07521969e-02\n",
      "  -4.44883391e-02  2.80259113e-02 -8.92480720e-02  3.02456545e-02\n",
      "  -6.04847065e-02  2.02178267e-02 -2.47307732e-02 -1.22050346e-01\n",
      "   9.15543900e-02 -4.53715199e-02 -6.19308612e-02 -6.65146510e-02\n",
      "  -1.88058298e-02  9.51687343e-02 -7.57681975e-03  1.37625952e-01\n",
      "  -3.73724545e-02  1.17374771e-01  1.49312630e-01 -5.97525136e-02\n",
      "   1.84627120e-01  2.05764804e-01  2.03319926e-02 -1.14253542e-01\n",
      "   8.84497076e-02 -3.43485540e-02 -4.53006531e-02 -6.66897239e-02\n",
      "   1.20562259e-01 -4.79991856e-02 -1.34522616e-01 -3.13425823e-01\n",
      "  -8.32191270e-02  9.12172263e-02  5.55801867e-02 -8.68860204e-02\n",
      "  -1.14679748e-02 -2.76363098e-02  5.57303938e-02 -2.02979992e-01\n",
      "   1.18643655e-01]\n",
      " [ 5.22574431e-02  1.37026531e-01 -1.53795996e-01 -1.99111174e-01\n",
      "  -4.78503872e-02 -8.16602201e-02 -1.89831018e-01 -7.86916681e-02\n",
      "  -1.24274185e-01  1.72627469e-02 -4.20999850e-02 -3.02962505e-02\n",
      "  -4.74191149e-02 -5.70557261e-02 -1.54141545e-02  2.00625304e-01\n",
      "  -9.83984159e-02  4.59270405e-02  1.25635779e-01  1.86670694e-01\n",
      "   5.50160119e-02  4.64786971e-02 -2.65897963e-02  2.19114771e-01\n",
      "   1.14213398e-01 -1.06603864e-01 -7.04545878e-02 -4.90403167e-02\n",
      "   1.08223377e-02  1.19079737e-01  6.06259259e-02  1.56198259e-01\n",
      "   3.20155138e-02  2.76782880e-01  4.12572313e-03 -7.35284762e-02\n",
      "   1.17080927e-01 -2.87920625e-02  7.53323257e-02  1.81043802e-01\n",
      "   8.41209253e-02 -8.89623590e-02  2.92204721e-02  3.97809502e-02\n",
      "  -3.75879915e-02]\n",
      " [-1.05082053e-02 -8.79601338e-02  1.01070410e-01 -9.02192332e-02\n",
      "  -1.91779382e-02  1.85955285e-01  1.89753753e-01 -1.06760596e-01\n",
      "  -1.11162728e-01  1.60812465e-01 -1.10290219e-01 -2.00515657e-02\n",
      "   9.80372323e-02 -6.29512165e-02  3.02637797e-02 -7.44897308e-02\n",
      "   1.33221567e-01 -6.25981668e-02 -7.34044806e-02 -1.77439251e-01\n",
      "  -1.61470277e-01  1.65786053e-01 -3.56285142e-02  1.72240241e-01\n",
      "   8.95019491e-02  1.78899441e-02  1.54657072e-01 -3.76351984e-03\n",
      "  -1.23531653e-02 -5.50630352e-02  1.46312985e-02  1.42409771e-01\n",
      "  -1.65890970e-02  8.85770314e-02 -9.77170674e-02 -1.55501186e-01\n",
      "   6.29916311e-03 -1.36644008e-01  1.74254313e-01  4.96631590e-02\n",
      "  -3.25153354e-02  7.90329020e-02 -2.31336001e-01 -8.01989728e-02\n",
      "  -4.81016482e-02]\n",
      " [-1.05972359e-01  2.76331320e-03  1.14316278e-01  4.46116807e-02\n",
      "   1.24513190e-02 -2.43376164e-02 -2.79772059e-02  3.67456876e-02\n",
      "  -5.89194345e-02 -6.61147087e-02 -1.48684217e-01 -2.19802292e-02\n",
      "  -1.09389193e-01  7.59566852e-03 -1.16090498e-01 -2.48347662e-01\n",
      "   7.49959694e-02  7.35580663e-04  1.19513376e-01 -6.73907094e-02\n",
      "   2.19558506e-01  1.42480618e-01  1.85289533e-02 -1.22832098e-01\n",
      "  -1.82098426e-01  4.70168382e-03  5.67388653e-02  1.38605171e-01\n",
      "  -1.63783405e-02 -7.42875227e-03 -8.52299621e-02 -9.69140223e-03\n",
      "  -3.52840935e-02  7.90513529e-02  2.32582537e-02 -2.73408684e-02\n",
      "  -2.47601176e-01 -3.60197896e-02  1.79968188e-02 -4.08635193e-04\n",
      "   7.74622479e-02 -8.18251271e-02 -5.68672789e-04  5.50712602e-02\n",
      "   1.24230795e-01]\n",
      " [-1.75602035e-02 -3.54719073e-01  2.23409919e-02  5.34765198e-02\n",
      "  -2.08000579e-02  1.30270530e-01  2.03700029e-02  8.05832805e-02\n",
      "  -1.10563052e-04 -1.37082268e-02 -5.59213256e-02 -4.02333592e-02\n",
      "  -7.23707888e-02 -1.43666496e-01 -1.29191417e-02  4.53283270e-03\n",
      "   6.73004300e-02  9.23761147e-02 -1.20743539e-02 -7.10058811e-02\n",
      "  -1.97852024e-02 -3.59931249e-02  1.39068202e-02 -1.95727775e-01\n",
      "  -2.89342203e-02 -1.61882517e-01  6.63620144e-03  1.57645945e-01\n",
      "  -5.66688279e-02 -1.25922961e-02  3.07283016e-02  9.79654919e-02\n",
      "   1.11272127e-02 -2.42540470e-02  5.18907669e-02  2.98448167e-01\n",
      "   1.36840443e-01 -6.94037914e-02 -5.47416908e-02  9.89243639e-02\n",
      "  -3.05517448e-01 -1.18100181e-01  4.65101884e-02 -1.15592811e-01\n",
      "  -1.04350001e-02]\n",
      " [-8.23762969e-02  7.88516782e-02  2.00634982e-02  9.38847902e-02\n",
      "   7.22273029e-02  4.29873013e-02  3.15454033e-02  6.39033757e-03\n",
      "  -2.31544107e-01 -2.32791484e-01  2.14867109e-02  2.91325227e-02\n",
      "  -6.61078722e-02  1.52434451e-02  1.22231188e-01  4.17460088e-03\n",
      "  -3.15379392e-02 -6.68782502e-02  6.97136818e-03  1.65609462e-01\n",
      "   3.79107217e-02  5.23156843e-02 -4.70087671e-02  2.26592912e-02\n",
      "  -4.54650514e-03  6.29954392e-03 -8.02557889e-03 -9.84118905e-02\n",
      "  -9.59123072e-02  1.68212761e-01 -1.91661480e-01  1.80388162e-01\n",
      "   3.84095859e-02  9.74061322e-03 -1.42384509e-01  1.51571785e-01\n",
      "   4.64307169e-02 -1.94015861e-01 -6.23098421e-02 -1.92366897e-01\n",
      "   1.41633188e-02 -3.06763790e-02  2.18346485e-03  1.07414316e-02\n",
      "  -1.96194472e-01]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W3 = [[ 4.64491654e-02 -1.57358840e-01  8.43574173e-02 -1.95346315e-01\n",
      "   1.08618463e-01  6.87680775e-02  1.32511603e-01 -7.67142845e-02\n",
      "   9.08021239e-02  1.10023642e-01  5.48563247e-02  3.57893999e-02\n",
      "   7.15299795e-02  3.60366109e-02 -1.28336876e-01 -4.28118367e-02\n",
      "  -6.18591886e-02  1.20044434e-01  5.94277490e-02  2.51407116e-02\n",
      "  -1.62556569e-01  9.91305232e-02]\n",
      " [ 2.61295620e-02  1.46641092e-02  1.12163336e-02  7.80362578e-03\n",
      "   4.50455114e-02 -7.19408510e-02  7.10309893e-02 -2.43275277e-02\n",
      "   1.46074661e-01  2.24927761e-02  3.13048220e-02 -1.58093209e-02\n",
      "   1.04758096e-02  3.81035021e-02 -1.91969962e-01 -5.87810991e-02\n",
      "  -1.09325751e-01 -1.50816568e-01 -7.04195466e-02 -4.45197340e-02\n",
      "  -7.30412580e-02 -4.34546853e-02]\n",
      " [-8.52786036e-02  1.16520341e-01 -3.55332673e-03  1.16041467e-01\n",
      "  -1.67719590e-02  7.23564697e-02  1.59854154e-02 -3.18251403e-02\n",
      "  -2.26225332e-01 -1.87704532e-01  3.98378531e-02 -1.69309202e-01\n",
      "  -7.90339447e-02  1.27098862e-01  1.05390758e-01  1.19103269e-01\n",
      "   4.09822550e-02  1.11170576e-01  7.57681331e-02 -5.22224421e-02\n",
      "   1.55401313e-02 -4.54505622e-02]\n",
      " [-5.16412879e-02 -1.37371481e-01  4.61696914e-02  3.06797812e-02\n",
      "  -6.13387112e-02 -8.02198026e-02 -1.83514827e-01  1.68671439e-01\n",
      "   1.66564652e-01  5.51700002e-02  8.37692432e-02  1.74133107e-02\n",
      "   3.48721505e-02 -8.85918694e-02 -9.92111225e-02 -6.44141049e-02\n",
      "   1.55330579e-01  2.04716385e-04 -8.35648880e-02 -1.03409558e-01\n",
      "  -6.86265478e-02  5.95329372e-03]\n",
      " [ 1.60046075e-01 -1.26343546e-01  2.29673841e-02 -9.71703580e-02\n",
      "   4.97286857e-02  2.85835770e-02 -3.90281807e-02  2.46054612e-02\n",
      "   4.03604446e-02 -2.85465894e-02  7.25025757e-03  5.52713242e-02\n",
      "  -8.90024938e-02  1.13562360e-02  1.12924052e-01  3.91562184e-02\n",
      "  -1.86362581e-01  2.29363488e-02 -7.35934590e-02 -5.13631570e-02\n",
      "   4.08837293e-03  4.42158065e-02]\n",
      " [ 1.10694676e-03 -5.46030484e-02  3.89547945e-02  1.94974445e-01\n",
      "  -6.36321139e-02  6.06427271e-02  2.80735409e-02 -1.53218698e-01\n",
      "   2.81528027e-02  4.34442281e-02  9.02598465e-02  1.68439539e-01\n",
      "   1.39768291e-02 -6.44958720e-02  1.37765758e-02  1.75984252e-02\n",
      "   6.41472987e-02  9.44114163e-02 -5.16574739e-02  1.23427496e-01\n",
      "   8.48644048e-02 -1.28126534e-01]\n",
      " [ 6.68957445e-02 -9.37328237e-02  1.11676414e-01 -1.06495043e-01\n",
      "  -3.45913954e-02  1.06398724e-01 -7.43542900e-02 -3.93578409e-03\n",
      "   2.28575387e-02  1.28578861e-01 -1.86279168e-02  3.13828977e-02\n",
      "  -3.72054605e-02 -1.19433464e-01 -3.19046027e-02  2.30520890e-01\n",
      "   9.97715402e-03 -7.00587420e-02  2.55478816e-02 -1.46479200e-01\n",
      "   1.05005569e-02 -9.00724193e-02]\n",
      " [-2.37797657e-01  2.44616061e-02 -4.08390320e-02  4.42473881e-02\n",
      "  -1.17579555e-01 -7.62243357e-02  7.89646435e-02  6.88524579e-02\n",
      "  -7.23857172e-02  8.74490492e-03 -3.59962072e-02 -2.50746597e-02\n",
      "   8.67169355e-02 -1.64988768e-01 -1.05947344e-01 -1.09423365e-01\n",
      "   1.44652598e-02 -2.39095898e-01 -4.91707497e-02 -3.31325264e-02\n",
      "   1.23958479e-02  5.50909236e-02]\n",
      " [ 8.39039745e-03  1.09566428e-01 -2.82061159e-02 -6.41853076e-02\n",
      "  -2.86904539e-02 -7.80537600e-02 -5.41460625e-02  9.13432512e-02\n",
      "  -1.35359354e-01  1.36072532e-01  5.81414738e-02 -1.54337603e-01\n",
      "  -2.99176776e-03 -3.04021752e-02 -1.66879769e-01  3.78895757e-02\n",
      "  -5.10407274e-02  8.19375760e-02 -1.73798385e-01  1.44523885e-01\n",
      "  -9.14918731e-02 -9.81736627e-02]\n",
      " [ 3.91327062e-03  2.82182982e-01 -2.61354906e-01 -1.46714957e-01\n",
      "  -9.18217237e-02 -3.83793753e-02  7.19014001e-02 -2.66233224e-02\n",
      "   8.85397231e-02  1.02848942e-01  1.25032452e-01 -9.71032516e-02\n",
      "  -1.36755649e-01  1.24240825e-02 -3.23525656e-02  7.79428527e-03\n",
      "   2.22677160e-01  7.20480607e-02 -1.25730324e-01  9.28355989e-02\n",
      "   1.23784763e-01  9.11707878e-02]]\n",
      "b3 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = init_parameters(neuron_count)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "print(\"b3 = \" + str(parameters[\"b3\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f564c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single layer forward\n",
    "def single_layer_forward (H_prev,W,b,activation='relu'):\n",
    "    \n",
    "    # activation = sigmoid\n",
    "    if activation == \"sigmoid\":\n",
    "        Z = np.dot(W, H_prev) + b \n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = sigmoid(Z)\n",
    " \n",
    "    elif activation == \"softmax\":\n",
    "        Z = np.dot(W, H_prev) + b \n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = softmax(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        #print(\" shape w\", W.shape, \" shape H\", H_prev)\n",
    "        Z = np.dot(W, H_prev) + b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = relu(Z)\n",
    "        \n",
    "    assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
    "    memory = (linear_memory, activation_memory)\n",
    "\n",
    "    return H, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b92e3be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 30,  35,  75, 160,  40],\n",
       "       [  7,   5,  15,  25,   9],\n",
       "       [  1,   0,   5,  10,   2]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Test\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "Z = np.dot(W_sample, H_prev) + b_sample\n",
    "Z\n",
    "#H = single_layer_forward(H_prev, W_sample, b_sample)[0]\n",
    "#H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9631ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now  for L_layer\n",
    "def multi_layer_forward(X,parameters):\n",
    "    memories = []\n",
    "    H = X\n",
    "    L = len(parameters)//2\n",
    "    #print(L)\n",
    "    \n",
    "    for i in range(1,L):\n",
    "        H_prev = H\n",
    "        H,memory = single_layer_forward(H_prev,parameters[\"W\"+str(i)],parameters[\"b\"+str(i)],activation='relu')\n",
    "        memories.append(memory)\n",
    "        \n",
    "    # Implement the final softmax layer\n",
    "    # HL here is the final prediction P as specified in the lectures\n",
    "    HL, memory = single_layer_forward(H, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)],activation=\"softmax\")\n",
    "    memories.append(memory)\n",
    "\n",
    "    assert(HL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return HL, memories    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20115ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "[[0.10505824 0.10809974 0.10742884 0.10827533 0.1043315 ]\n",
      " [0.08388441 0.08798173 0.09076671 0.09444689 0.10357669]\n",
      " [0.10585231 0.10652031 0.11499845 0.1098712  0.10225505]\n",
      " [0.15150747 0.1063141  0.11056117 0.10875693 0.10831372]\n",
      " [0.11968172 0.10883656 0.12116584 0.10470745 0.10038303]\n",
      " [0.09684642 0.10892533 0.09474728 0.10444409 0.10060403]\n",
      " [0.06535662 0.07579792 0.07545279 0.07992684 0.09100451]\n",
      " [0.09350021 0.0953626  0.0958     0.09680762 0.09118882]\n",
      " [0.08193323 0.09687351 0.08271202 0.08942496 0.09510008]\n",
      " [0.09637937 0.10528821 0.10636688 0.10333869 0.10324257]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_X[:, 10:20]\n",
    "print(x_sample.shape)\n",
    "HL = multi_layer_forward(x_sample, parameters=parameters)[0]\n",
    "print(HL[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51728816",
   "metadata": {},
   "source": [
    "### Compute loss\n",
    "\n",
    "- Compute loss function after every forward pass\n",
    "- Keep checking whether it is decreasing with training\n",
    "- Loss function : Cross Entropy Loss Function (-1) * sum(y*log(p)) -P is probability matrix\n",
    "- For m data points average loss  : (-1/m) * sum(y*log(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7041c18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11 <class 'numpy.ndarray'>\n",
      "11 <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "zr = np.array([[11]])\n",
    "pt = np.squeeze(zr)\n",
    "print(np.squeeze(zr))\n",
    "print(pt,type(pt))\n",
    "assert(pt.shape==())\n",
    "print(pt,type(pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "484f371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss (HL,Y):\n",
    "    #HL is probability matrix like previous output\n",
    "    #Another argument is true label that is Y\n",
    "    # Using cross entropy loss function\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    loss =  (-1.0/m) * np.sum(np.multiply(Y, np.log(HL)))\n",
    "    \n",
    "    loss = np.squeeze(loss)\n",
    "    assert(loss.shape==())\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2c378b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]\n",
      " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]\n",
      " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]\n",
      " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]\n",
      " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]\n",
      " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]\n",
      " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]\n",
      " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]\n",
      " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]\n",
      " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "0.8964600261334037\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "# HL is (10, 5), Y is (10, 5)\n",
    "np.random.seed(2)\n",
    "HL_sample = np.random.rand(10,5)\n",
    "Y_sample = train_set_Y[:, 10:15]\n",
    "print(HL_sample)\n",
    "print(Y_sample)\n",
    "\n",
    "print(compute_loss(HL_sample, Y_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3884b0d0",
   "metadata": {},
   "source": [
    "### Backpropagation of errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1055e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need a function similar to compute dz\n",
    "def propagate_backward(dh,memory,activation):\n",
    "    # Gradient =dh \n",
    "    # Sigmoid\n",
    "    Z= memory\n",
    "    if activation==\"sigmoid\":\n",
    "        H = 1/(1+np.exp(-Z))\n",
    "        dz = dh * H * (1-H)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        dz = np.array(dh, copy=True) # dZ will be the same as dA wherever the elements of A weren't 0\n",
    "        dz[Z <= 0] = 0\n",
    "        \n",
    "    assert (dz.shape == Z.shape)\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "498d5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need backward output for single layer\n",
    "def single_layer_backward(dh, memory,activation=\"relu\"):\n",
    "    linear_memory,activation_memory = memory\n",
    "    \n",
    "    dz = propagate_backward(dh,activation_memory,activation)\n",
    "    H_prev, W, b = linear_memory\n",
    "    m = H_prev.shape[1]\n",
    "    dw = (1.0/m) * np.dot(dz, H_prev.T)\n",
    "    db = (1.0 / m) * np.sum(dz, axis=1, keepdims=True)\n",
    "    dh_prev = np.dot(linear_memory[1].T, dz)\n",
    "    \n",
    "    return dh_prev,dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbd4aa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dH_prev is \n",
      " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]\n",
      " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]\n",
      "dW is \n",
      " [[1.67565336 1.56891359]\n",
      " [1.39137819 1.4143854 ]\n",
      " [1.3597389  1.43013369]]\n",
      "db is \n",
      " [[0.37345476]\n",
      " [0.34414727]\n",
      " [0.29074635]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "H, memory = single_layer_forward(H_prev, W_sample, b_sample)\n",
    "np.random.seed(2)\n",
    "dH = np.random.rand(3,5)\n",
    "dH_prev, dW, db = single_layer_backward(dH, memory)\n",
    "print('dH_prev is \\n' , dH_prev)\n",
    "print('dW is \\n' ,dW)\n",
    "print('db is \\n', db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8dafcb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For L Layers keep on calculate gradient \n",
    "def multi_layer_backward(HL, Y, memories):\n",
    "    \n",
    "    # Takes the predicted value HL and the true target value Y and the \n",
    "    # memories calculated by L_layer_forward as input\n",
    "    \n",
    "    # returns the gradients calulated for all the layers as a dict\n",
    "\n",
    "    gradients = {}\n",
    "    L = len(memories) # the number of layers\n",
    "    #print(L)\n",
    "    m = HL.shape[1]\n",
    "    Y = Y.reshape(HL.shape) # after this line, Y is the same shape as HL\n",
    "    \n",
    "    # Perform the backprop for the last layer that is the softmax layer\n",
    "    current_memory = memories[-1]\n",
    "    linear_memory, activation_memory = current_memory\n",
    "    dZ = HL - Y\n",
    "    H_prev, W, b = linear_memory\n",
    "    gradients[\"dH\" + str(L-1)] = np.dot(linear_memory[1].T, dZ)\n",
    "    gradients[\"dW\" + str(L)] = (1.0 / m) * np.dot(dZ, H_prev.T) \n",
    "    gradients[\"db\" + str(L)] = (1.0 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "     \n",
    "    # Perform the backpropagation l-1 times\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
    "        current_memory = memories[l]\n",
    "        \n",
    "        dH_prev_temp, dW_temp, db_temp = single_layer_backward(gradients[\"dH\" + str(l + 1)], current_memory,activation=\"relu\")\n",
    "        gradients[\"dH\" + str(l)] = dH_prev_temp\n",
    "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
    "        gradients[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6771e578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79ec4a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW3 is \n",
      " [[ 0.00963716  0.          0.0285094   0.02738736  0.01424124  0.03241527\n",
      "   0.03083011  0.00013116  0.          0.01855019  0.01525012  0.08500162\n",
      "   0.01266539  0.00040786  0.01458491  0.04171005  0.01763245  0.00864049\n",
      "   0.03792124  0.00025942  0.01387686  0.00555116]\n",
      " [-0.01533125  0.          0.00775839  0.0126042  -0.00144876  0.00648266\n",
      "  -0.03044748  0.00011441  0.          0.00055676  0.00255072  0.04571754\n",
      "   0.00943349  0.00033196  0.01142653  0.03217844  0.01035296  0.00211122\n",
      "   0.01212671  0.00018164 -0.01974972  0.00454849]\n",
      " [ 0.00997107  0.          0.03092506 -0.00801676  0.01482014 -0.03621567\n",
      "   0.02316334  0.00013309  0.         -0.02103732  0.01587223  0.03172836\n",
      "   0.01459812  0.0004019   0.01536041 -0.00223577  0.01857028  0.00787309\n",
      "   0.00779063  0.00027293  0.00760524 -0.00650638]\n",
      " [-0.03996863  0.         -0.0228756  -0.03740734 -0.09259565 -0.04663876\n",
      "  -0.0811586   0.00013174  0.         -0.07131878 -0.01751133 -0.08257563\n",
      "  -0.00410823  0.00040113  0.01522453 -0.04835816 -0.06136454  0.00982725\n",
      "  -0.04010334  0.00031717 -0.0644597  -0.02285517]\n",
      " [ 0.01027818  0.          0.03080309  0.0300762   0.01583346  0.0346774\n",
      "   0.03292486  0.00012684  0.          0.0206953   0.01585417  0.09237253\n",
      "   0.01428695  0.00041064  0.01491441  0.04559082  0.01963354  0.00932102\n",
      "   0.04066111  0.00027711  0.01495454  0.00623958]\n",
      " [ 0.00892902  0.         -0.00339602  0.01223936  0.01302205 -0.00437725\n",
      "   0.00478915  0.00012652  0.          0.01695283  0.01418538  0.02272769\n",
      "   0.01199889 -0.00336204 -0.03056385  0.02601431  0.00022588  0.00805673\n",
      "  -0.00114622  0.00022727  0.01271571  0.00499406]\n",
      " [-0.00967218  0.          0.00436467  0.013145   -0.00203132 -0.0248744\n",
      "  -0.02360041 -0.00111452  0.          0.00227952 -0.04084874 -0.01316222\n",
      "  -0.01181977  0.00028599 -0.0311639  -0.03128869  0.01115354 -0.01292215\n",
      "   0.00457116  0.00014091 -0.00242128 -0.0015599 ]\n",
      " [ 0.00867205  0.         -0.03144975  0.00249524  0.01285511  0.00904545\n",
      "   0.02748528  0.00011727  0.          0.01722655 -0.00723368 -0.04086143\n",
      "   0.01163955  0.0003598   0.00238794 -0.05346097 -0.01972403 -0.01384884\n",
      "  -0.03730316 -0.00214401  0.01248694  0.00520862]\n",
      " [ 0.00803384  0.         -0.02672233 -0.03545233  0.01151522  0.02067532\n",
      "  -0.00804327  0.00010832  0.         -0.00149642  0.01024388 -0.09681754\n",
      "  -0.0485477   0.00036551 -0.00495652 -0.0149057  -0.01261262 -0.00898142\n",
      "  -0.00517674  0.00021867  0.0114084  -0.00101841]\n",
      " [ 0.00945074  0.         -0.01791692 -0.01707094  0.01378849  0.00880998\n",
      "   0.02405702  0.00012518  0.          0.01759137 -0.00836276 -0.0441309\n",
      "  -0.01014668  0.00039725 -0.00721446  0.00475567  0.01613254 -0.0100774\n",
      "  -0.01934139  0.00024889  0.01358301  0.00539796]]\n",
      "db3 is \n",
      " [[ 0.10643091]\n",
      " [-0.01386138]\n",
      " [ 0.01081822]\n",
      " [-0.08128576]\n",
      " [ 0.11342244]\n",
      " [-0.00054523]\n",
      " [-0.12468789]\n",
      " [-0.0020855 ]\n",
      " [-0.00868568]\n",
      " [ 0.00047988]]\n",
      "dW2 is \n",
      " [[ 3.78082181e-04 -4.96980767e-03 -8.04050016e-03  2.10386857e-02\n",
      "   6.17643479e-03  3.68957534e-03 -6.09885017e-05 -5.78756629e-03\n",
      "   1.06192077e-03  1.39623586e-02  1.64670987e-02  2.00302416e-02\n",
      "  -5.04305626e-03  1.45499790e-02  7.91362971e-03 -2.45085277e-03\n",
      "   2.47630285e-02 -5.88885910e-03  0.00000000e+00 -6.90679221e-03\n",
      "   0.00000000e+00  3.78798115e-03  0.00000000e+00 -4.39408069e-04\n",
      "   4.73504005e-03 -1.10161801e-03 -1.74492187e-03  2.04027521e-02\n",
      "   4.82400075e-03  1.59111549e-02  1.50383983e-02 -1.33582231e-03\n",
      "   0.00000000e+00  0.00000000e+00 -1.27403004e-03  6.99149939e-04\n",
      "  -3.63941492e-03  4.32600157e-03  0.00000000e+00 -4.84195528e-03\n",
      "   2.69643868e-02  7.16023487e-03  9.16630657e-03  0.00000000e+00\n",
      "  -4.14558491e-03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-1.69808413e-03  8.49827827e-04 -6.26045062e-03 -2.67034954e-02\n",
      "  -8.28006812e-03 -8.32265511e-03 -4.42095959e-03 -5.79879760e-03\n",
      "  -3.33353722e-03 -2.37317003e-02 -1.58883805e-02 -1.84368843e-02\n",
      "  -6.63888835e-03 -3.00787005e-02 -2.56376444e-02 -1.06231164e-03\n",
      "  -6.34373512e-02 -5.68762336e-03 -2.65867991e-03 -1.46891174e-02\n",
      "   2.15649462e-03 -3.28156936e-03  0.00000000e+00  1.99236846e-03\n",
      "  -6.59340751e-03 -1.34431154e-02  1.97876979e-03 -1.64440346e-02\n",
      "  -9.63415884e-03 -3.02670062e-02 -2.08410722e-02  7.93884538e-05\n",
      "  -1.16818608e-03 -1.71931663e-03 -3.08054173e-04 -2.50530106e-03\n",
      "  -2.28454024e-02 -7.52629585e-03  0.00000000e+00 -7.09932368e-04\n",
      "  -4.38332784e-02 -6.82685167e-03 -1.22436102e-02  0.00000000e+00\n",
      "  -5.71243502e-04]\n",
      " [ 1.54025658e-03  3.98444437e-03  7.05907682e-03  3.95605094e-02\n",
      "   3.28590049e-03 -2.05760615e-04  1.95404408e-04 -2.83114610e-03\n",
      "   2.90434051e-03  2.21970928e-02  2.11917172e-02  1.46328852e-02\n",
      "   1.18513184e-02  2.66342947e-02  2.20440499e-02 -3.64035508e-03\n",
      "   2.51791584e-02 -5.97644552e-04  1.17736255e-04  2.38923469e-02\n",
      "  -2.28518250e-03  3.08567726e-03  0.00000000e+00 -3.70367136e-03\n",
      "   3.22100212e-03  1.64255467e-03 -1.93879421e-03  1.85546597e-02\n",
      "   1.37276154e-02  1.78185918e-02  5.65500194e-04  2.06045232e-03\n",
      "  -4.72421557e-04  7.61377478e-05 -5.10458646e-04 -2.86328445e-03\n",
      "  -6.82993763e-04  2.29881373e-02  0.00000000e+00 -2.09111552e-03\n",
      "   3.82541905e-02  3.55112518e-03  2.57746236e-02  0.00000000e+00\n",
      "  -1.35373051e-03]\n",
      " [ 1.24725537e-03  2.49280289e-03  2.69210604e-03  7.24703352e-03\n",
      "   3.96062902e-03  1.00909345e-03  3.54700653e-05  1.03780520e-03\n",
      "   1.68437348e-03  4.29080172e-03  4.74890865e-03  1.76530342e-03\n",
      "   5.33061323e-03  3.05881303e-03  4.59852271e-03  1.02218842e-05\n",
      "   3.54308125e-03  1.77648556e-03  0.00000000e+00  7.20204947e-03\n",
      "   0.00000000e+00  1.10105265e-04  0.00000000e+00  0.00000000e+00\n",
      "   1.38280340e-04  3.15026749e-03  0.00000000e+00  4.38895522e-03\n",
      "   3.50804301e-03  2.37336070e-03  4.33826021e-04  6.87857018e-04\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.20093956e-03\n",
      "   0.00000000e+00  5.11042078e-03  0.00000000e+00  1.15845277e-03\n",
      "   6.72762755e-03  2.01830017e-04  5.74865169e-03  0.00000000e+00\n",
      "   2.41101459e-03]\n",
      " [ 8.01837124e-03 -1.82710307e-04  1.51612614e-02 -7.57822868e-03\n",
      "   1.70576871e-02  7.27855814e-04  2.65334280e-03  3.57068156e-02\n",
      "   2.25990872e-02 -1.75003914e-02  1.10336728e-03  2.04892272e-02\n",
      "   3.55871128e-02 -4.64667392e-03 -1.76249222e-04  1.68044389e-02\n",
      "  -3.99243579e-02  1.69369041e-02  1.42467961e-03 -6.27007711e-03\n",
      "   6.46716158e-04 -3.66126171e-03  0.00000000e+00 -8.05580480e-03\n",
      "  -1.03789496e-03  3.63577207e-02  5.17582030e-03  1.74666016e-02\n",
      "   9.84364446e-03 -7.76982380e-03 -1.79742299e-02  6.58341100e-04\n",
      "  -3.05224581e-03  9.21312618e-04  5.12242636e-03  2.94369203e-02\n",
      "   9.91721451e-03 -4.12978323e-03  0.00000000e+00  2.92495208e-02\n",
      "  -8.85118287e-03 -2.38717913e-02  5.82984642e-03  0.00000000e+00\n",
      "   2.56349947e-02]\n",
      " [-3.38914043e-03 -2.37336272e-03  5.32584772e-03 -5.37416327e-02\n",
      "  -1.96383363e-02 -4.31681769e-03 -1.61593436e-04  7.89593285e-03\n",
      "  -6.58541564e-03 -2.65759431e-02 -3.56646262e-02 -2.78791734e-02\n",
      "  -7.73592481e-03 -1.95520346e-02 -1.69398859e-02  4.97037042e-03\n",
      "  -2.58532598e-02  5.48882740e-03 -9.73642614e-05 -1.46510054e-02\n",
      "  -2.14296223e-04 -4.83672938e-03  0.00000000e+00  1.36614327e-03\n",
      "  -6.83747587e-03  1.04845476e-03  3.70517395e-03 -4.10095073e-02\n",
      "  -1.94413095e-02 -1.76980283e-02 -1.74000375e-02 -7.47534551e-04\n",
      "   3.05485319e-04 -6.29635758e-05  2.77517648e-03 -2.27994360e-03\n",
      "   1.19311813e-02 -2.43561493e-02  0.00000000e+00  5.67664570e-03\n",
      "  -4.87301715e-02 -6.50842073e-03 -3.03843847e-02  0.00000000e+00\n",
      "  -3.56238612e-04]\n",
      " [ 0.00000000e+00 -3.77939920e-04 -1.02425196e-02 -2.21358875e-03\n",
      "   0.00000000e+00 -1.49179283e-03  0.00000000e+00 -9.51437384e-03\n",
      "  -3.22199239e-03 -4.55649512e-03  0.00000000e+00 -1.11737145e-02\n",
      "  -7.83892645e-03 -1.36652401e-02 -1.00815680e-02 -6.64820171e-03\n",
      "  -1.78436573e-02 -6.40321633e-03  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.04435678e-03\n",
      "   0.00000000e+00 -1.36513214e-02 -4.14721785e-03  0.00000000e+00\n",
      "   0.00000000e+00 -9.91488407e-03  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00 -3.02803248e-03 -1.10585176e-02\n",
      "  -8.64992682e-03  0.00000000e+00  0.00000000e+00 -6.77386284e-03\n",
      "  -1.10543775e-03  0.00000000e+00 -3.92894019e-03  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-4.32757546e-03  1.88483601e-04 -3.71877068e-04 -1.39306098e-02\n",
      "   1.57329394e-02  1.97005500e-02 -9.36231388e-05  4.81113121e-03\n",
      "   1.32056971e-02  1.36845692e-02 -1.71682213e-02 -8.80686949e-03\n",
      "   1.21518555e-03  9.43328439e-03 -1.27180923e-03  6.80514250e-03\n",
      "   4.63773292e-02 -2.60190688e-03  0.00000000e+00  1.10692966e-02\n",
      "   4.19010952e-03 -3.88011481e-05  0.00000000e+00  0.00000000e+00\n",
      "  -2.41178627e-03  5.78086433e-03  0.00000000e+00 -1.19303569e-04\n",
      "   6.11327547e-03  4.58265570e-03  3.72197989e-02  1.59693298e-04\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.44439430e-02\n",
      "   2.58899304e-02 -5.78764190e-03  0.00000000e+00  4.27859306e-03\n",
      "   2.20772562e-02  9.58549607e-03 -1.18612168e-02  0.00000000e+00\n",
      "   6.01626489e-04]\n",
      " [-4.48700327e-03 -1.34128948e-02 -1.32840649e-02 -4.22893636e-02\n",
      "  -3.11253166e-02 -7.96623035e-03 -7.55359149e-05 -9.18750431e-03\n",
      "  -8.55693498e-03 -2.23573551e-02 -2.79387228e-02 -2.98942565e-02\n",
      "  -2.00868380e-02 -1.06953055e-02 -1.97516812e-02 -9.69221166e-03\n",
      "  -3.88675350e-02 -1.35549580e-02  0.00000000e+00 -1.63969874e-02\n",
      "  -4.39159020e-03 -6.47915870e-04  0.00000000e+00 -1.20529191e-02\n",
      "  -3.93653376e-03 -2.36854655e-02 -6.71196901e-03 -2.94737305e-02\n",
      "  -1.38629410e-02 -1.42083900e-02 -2.11081880e-02 -9.67806055e-04\n",
      "  -1.88123863e-03  0.00000000e+00 -2.30585764e-03 -2.86350479e-02\n",
      "   3.88937151e-03 -1.80926167e-02  0.00000000e+00 -5.96136142e-03\n",
      "  -1.15894785e-02 -5.68263769e-03 -2.72064510e-02  0.00000000e+00\n",
      "  -5.13441945e-03]\n",
      " [ 1.04040681e-03  1.39754652e-02  1.71722436e-02  1.08270464e-02\n",
      "  -1.66924993e-02 -1.05256332e-02  2.66715159e-03 -3.68951384e-04\n",
      "  -2.03082652e-02  6.32056042e-03  1.88941960e-03 -1.28617269e-02\n",
      "   1.50664512e-03  1.31813253e-02  2.07269221e-02 -2.60709787e-03\n",
      "   2.58700219e-02  1.48267231e-02  1.57134605e-03  2.97711680e-02\n",
      "  -4.06009432e-03 -7.95673440e-04  0.00000000e+00  7.37548422e-03\n",
      "   4.12380422e-04  8.42308387e-03  3.64222503e-03 -2.85181684e-02\n",
      "  -9.17252617e-03  1.76213435e-02 -2.16023690e-02  2.11621567e-03\n",
      "   2.07620680e-03  1.01615895e-03  1.90044772e-03 -1.27230110e-02\n",
      "   1.06301966e-02  9.31246544e-03  0.00000000e+00 -5.40795184e-03\n",
      "  -1.25322303e-02  8.25118452e-03  8.32301809e-03  0.00000000e+00\n",
      "  -5.70156149e-03]\n",
      " [-4.36829565e-03 -5.04619943e-04  1.32077979e-02 -5.76312981e-03\n",
      "  -5.17141244e-03  4.41316588e-03 -1.31710227e-04 -3.85365964e-03\n",
      "  -2.12724044e-03  7.74934023e-03 -5.28383221e-03 -5.63415093e-03\n",
      "  -5.59833026e-03 -1.41882201e-04  1.04836562e-02  0.00000000e+00\n",
      "   3.70782052e-02  3.68197914e-03  0.00000000e+00  2.78197161e-03\n",
      "   0.00000000e+00 -1.20056872e-03  0.00000000e+00  6.63653055e-03\n",
      "  -2.92545335e-04 -7.45669051e-03  0.00000000e+00 -1.40004088e-02\n",
      "  -2.35814590e-03 -4.54683840e-03  1.69718800e-02  0.00000000e+00\n",
      "   4.49019877e-03  0.00000000e+00  1.02737303e-03 -3.36411736e-03\n",
      "   8.06894390e-03 -3.06378548e-03  0.00000000e+00 -4.30165764e-03\n",
      "  -8.98995036e-03 -9.24381511e-04 -6.00375093e-03  0.00000000e+00\n",
      "  -8.95276841e-03]\n",
      " [ 0.00000000e+00  2.64890250e-03  0.00000000e+00  5.64050658e-03\n",
      "   3.50600554e-03  0.00000000e+00  2.86069280e-03  3.86224887e-03\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.16402866e-03\n",
      "   0.00000000e+00  6.04909941e-04  0.00000000e+00  2.18160460e-03\n",
      "   8.66265013e-03  3.07823447e-03  1.72364205e-03  6.34318605e-03\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  3.25841666e-04\n",
      "   2.45433320e-03  3.28570428e-03  0.00000000e+00  0.00000000e+00\n",
      "   2.19679124e-03  2.81880690e-03  3.70420316e-03  0.00000000e+00\n",
      "   0.00000000e+00  1.11464582e-03  0.00000000e+00  3.17130173e-03\n",
      "   8.31126837e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  3.95478285e-04  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00 -8.49051315e-03  1.64830408e-03 -2.38453494e-02\n",
      "  -7.93483638e-03  7.64294107e-03 -1.28071654e-02 -1.58944086e-02\n",
      "   2.29362529e-04  1.12783365e-02  2.67205286e-04 -4.41587452e-03\n",
      "   8.12519921e-04  7.03586869e-03  7.32859958e-03 -7.21594169e-03\n",
      "  -6.60638673e-03 -1.07588544e-02 -7.71665129e-03 -2.06402897e-02\n",
      "   1.52665677e-03  7.00836869e-04  0.00000000e+00  2.55882265e-03\n",
      "  -1.09879156e-02 -5.35964412e-03  1.88480412e-03  0.00000000e+00\n",
      "  -9.83491435e-03  2.72307789e-06 -2.00283306e-03  0.00000000e+00\n",
      "   6.13760983e-04 -4.99020843e-03  4.64928862e-04 -8.15296847e-03\n",
      "  -2.70748016e-02  0.00000000e+00  0.00000000e+00  4.82207940e-04\n",
      "   1.11931962e-02  8.00253373e-03  2.79687705e-04  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 6.45099419e-03  5.39942749e-03  1.73727263e-02 -3.02419304e-02\n",
      "   8.47552449e-03  2.05310160e-03  3.74755774e-03  3.27024489e-02\n",
      "   6.21435660e-03 -1.81527403e-02 -1.31974347e-02 -1.88212384e-04\n",
      "   2.21768193e-02 -7.83240475e-03  1.58921033e-03  1.64334192e-02\n",
      "  -1.02969193e-02  2.45774840e-02  2.08735759e-03 -2.08096489e-03\n",
      "   7.81076630e-04 -5.01012030e-03  0.00000000e+00  2.03484192e-03\n",
      "  -2.82935709e-03  4.09820939e-02  8.26403462e-03 -1.35441329e-02\n",
      "  -6.90323020e-03  1.19074189e-03 -1.44061133e-02  0.00000000e+00\n",
      "  -4.95393264e-04  1.34985359e-03  5.80426382e-03  2.25702763e-02\n",
      "   2.41261544e-02 -1.65414941e-02  0.00000000e+00  2.35329017e-02\n",
      "  -2.92389106e-02 -1.06644009e-02 -1.05303825e-02  0.00000000e+00\n",
      "   2.02688431e-02]\n",
      " [ 0.00000000e+00 -5.64994370e-03 -1.97167807e-03 -1.73395467e-02\n",
      "  -1.04448620e-02 -7.12200769e-03 -1.21654488e-03 -2.37706176e-03\n",
      "  -1.05300436e-03 -1.54662863e-02 -4.01140507e-03 -4.10158230e-03\n",
      "  -5.74213137e-03 -1.33800404e-02 -7.41370323e-03 -3.04943275e-03\n",
      "  -3.86283167e-02 -3.64377651e-03 -7.33000030e-04 -1.21356008e-02\n",
      "  -1.55895611e-03 -1.06321790e-03  0.00000000e+00 -3.71948879e-03\n",
      "  -1.58738431e-03 -8.12609807e-03 -1.62320854e-03 -1.22681594e-02\n",
      "  -4.83530496e-03 -1.66925616e-02 -1.63604111e-02 -3.24298872e-04\n",
      "  -3.25150080e-04 -4.74016875e-04 -1.85643394e-04 -1.09804007e-02\n",
      "  -1.00230339e-02 -7.92224190e-03  0.00000000e+00  0.00000000e+00\n",
      "  -2.03965111e-02 -1.14643401e-02 -7.05044052e-03  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-9.63092557e-04  6.29766409e-03  1.50450937e-02  1.80630944e-02\n",
      "  -1.45006883e-02 -7.57710756e-03 -2.90385885e-05 -6.73223646e-03\n",
      "  -9.76302737e-03  1.00484263e-02  6.64752961e-03 -6.26248848e-03\n",
      "   3.34779405e-03  1.13654045e-02  1.81117038e-02 -5.90621533e-03\n",
      "   1.58371302e-02  4.45115926e-03  0.00000000e+00  2.14423817e-02\n",
      "  -3.63718411e-03 -2.10215619e-05  0.00000000e+00  2.15142228e-03\n",
      "   0.00000000e+00 -7.16077741e-03 -5.25365440e-04 -1.09122803e-02\n",
      "   3.23827019e-04  5.12029422e-03 -1.28827998e-02  2.19274562e-03\n",
      "   2.13455246e-03  0.00000000e+00  4.52386644e-04 -1.39840045e-02\n",
      "  -1.75745115e-03  1.50692455e-02  0.00000000e+00 -6.43319311e-03\n",
      "   1.90929780e-03  5.18953181e-04  1.28756946e-02  0.00000000e+00\n",
      "  -7.18139618e-03]\n",
      " [-2.52664080e-03  1.11421638e-03  1.64571744e-02 -2.73385377e-02\n",
      "  -8.99031855e-03  2.49956288e-03  1.38544842e-03  2.23288707e-02\n",
      "   3.64828375e-03 -5.42348459e-03 -2.57961785e-02 -3.04919042e-03\n",
      "   9.08901718e-03  1.77807597e-02  5.27265937e-03  1.44922452e-02\n",
      "   2.07694555e-02  1.20316669e-02  8.34768825e-04  2.95412824e-03\n",
      "  -1.43421818e-04 -2.19450189e-03  0.00000000e+00 -1.17503511e-03\n",
      "  -3.79291419e-03  2.55110195e-02  7.89653935e-03 -2.54844092e-02\n",
      "  -8.19445359e-03  1.13039110e-02 -7.25439058e-03  1.68084785e-04\n",
      "  -1.43034263e-03  5.39828777e-04  5.91998076e-03  1.72048994e-02\n",
      "   3.21018513e-02 -1.30418773e-02  0.00000000e+00  1.50602494e-02\n",
      "  -1.44880516e-02  8.63053791e-05 -1.24827369e-02  0.00000000e+00\n",
      "   9.24015220e-04]\n",
      " [ 0.00000000e+00  1.63053243e-02  0.00000000e+00  6.09606275e-03\n",
      "   2.99272923e-02  1.65702984e-02  0.00000000e+00  3.51020519e-03\n",
      "   0.00000000e+00  1.92457152e-02  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  4.07335923e-03  3.90454875e-03  1.01383062e-02\n",
      "   5.87410289e-02  9.43019260e-03  0.00000000e+00  1.94222039e-02\n",
      "   7.44937292e-03  0.00000000e+00  0.00000000e+00  1.71112020e-02\n",
      "   0.00000000e+00  2.21743397e-02  7.75639905e-03  0.00000000e+00\n",
      "   0.00000000e+00  1.70233798e-02  3.72171849e-02  0.00000000e+00\n",
      "   1.55370904e-03  0.00000000e+00  8.87085185e-04  2.56544324e-02\n",
      "   1.43624871e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   5.83243013e-04  2.39508137e-02  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-2.37563354e-03 -7.00853224e-03 -1.62652077e-02  2.12053347e-02\n",
      "   5.19620646e-03  3.75335369e-03 -1.54961134e-04 -1.35513584e-02\n",
      "   3.95445474e-04  1.27351728e-02  1.33689316e-02  1.52479256e-02\n",
      "  -1.45827786e-02  8.35998105e-03 -8.70649613e-04 -5.64511148e-03\n",
      "   2.09974277e-02 -1.34506837e-02  0.00000000e+00 -9.57184015e-03\n",
      "   6.31666197e-04  4.35757449e-03  0.00000000e+00 -1.11474694e-03\n",
      "   5.44822893e-03 -1.46957917e-02 -4.42674236e-03  2.07749124e-02\n",
      "   5.74543029e-03  8.86039939e-03  2.19129445e-02 -1.75441646e-03\n",
      "   0.00000000e+00  0.00000000e+00 -3.23212335e-03 -3.82716746e-03\n",
      "  -7.60414329e-03  3.46305736e-03  0.00000000e+00 -1.11854933e-02\n",
      "   2.66585708e-02  8.41441735e-03  4.77416540e-03  0.00000000e+00\n",
      "  -9.48314743e-03]\n",
      " [ 1.29078368e-03  4.12536769e-04  2.89680412e-04 -1.15616789e-02\n",
      "   2.82002084e-02  2.98758990e-02  1.07675712e-04  1.16961917e-02\n",
      "   1.66388699e-02  2.38277367e-02 -1.10504846e-02 -7.58803175e-03\n",
      "   1.66684521e-02  1.65362708e-02  5.76707233e-03  7.57286331e-03\n",
      "   6.77512363e-02  3.85497661e-03  0.00000000e+00  1.00878363e-02\n",
      "   4.74247084e-03  3.05115667e-05  0.00000000e+00  0.00000000e+00\n",
      "  -3.55390882e-03  2.21565388e-02  0.00000000e+00  2.20797947e-02\n",
      "   8.06326846e-03  2.22358840e-02  4.59696060e-02  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  2.23482175e-02\n",
      "   3.24993521e-02 -2.94653072e-03  0.00000000e+00  1.18201274e-02\n",
      "   5.21579921e-02  1.68242404e-02 -8.15781760e-03  0.00000000e+00\n",
      "   1.52027841e-02]]\n",
      "db2 is \n",
      " [[-0.00071577]\n",
      " [ 0.        ]\n",
      " [-0.03007189]\n",
      " [ 0.02332698]\n",
      " [ 0.0086518 ]\n",
      " [ 0.0337232 ]\n",
      " [-0.01789838]\n",
      " [-0.01306416]\n",
      " [ 0.        ]\n",
      " [ 0.00949956]\n",
      " [-0.03678158]\n",
      " [ 0.02069259]\n",
      " [ 0.00026787]\n",
      " [ 0.00578335]\n",
      " [-0.01483394]\n",
      " [ 0.03126086]\n",
      " [-0.01713967]\n",
      " [ 0.01109991]\n",
      " [ 0.01630815]\n",
      " [ 0.02025419]\n",
      " [-0.01542929]\n",
      " [ 0.03292523]]\n"
     ]
    }
   ],
   "source": [
    "##### verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_X[:, 10:20]\n",
    "y_sample = train_set_Y[:, 10:20]\n",
    "\n",
    "HL, memories = multi_layer_forward(x_sample, parameters=parameters)\n",
    "gradients  = multi_layer_backward(HL, y_sample, memories)\n",
    "print('dW3 is \\n', gradients['dW3'])\n",
    "print('db3 is \\n', gradients['db3'])\n",
    "print('dW2 is \\n', gradients['dW2'])\n",
    "print('db2 is \\n', gradients['db2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4845cb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b844227a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dH2', 'dW3', 'db3', 'dH1', 'dW2', 'db2', 'dH0', 'dW1', 'db1'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6738bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Updates\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "\n",
    "    # parameters is the python dictionary containing the parameters W and b for all the layers\n",
    "    # gradients is the python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    # returns updated weights after applying the gradient descent update\n",
    "\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * gradients[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * gradients[\"db\" + str(l+1)]\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270704b2",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1ce9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "def multi_layer_model(X,Y, dimensions, learning_rate=0.0075,num_iterations=5000,displayLoss=True):\n",
    "    np.random.seed(2)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    parameters = init_parameters(dimensions)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Forward\n",
    "        HL, memories = multi_layer_forward(X,parameters)\n",
    "        \n",
    "        #What is the loss after cycle i\n",
    "        loss = compute_loss(HL,Y)\n",
    "        losses.append(loss)\n",
    "        if displayLoss and i%100==0 :\n",
    "            print(\"Loss after iteration : \",i,\" is: \",loss)\n",
    "        \n",
    "        # do backward\n",
    "        gradients = multi_layer_backward(HL,Y,memories)\n",
    "        #print(\" Gradients Updated\")\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        #print(\" Parameters Updated\")\n",
    "    \n",
    "    # plotting the loss\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    return parameters, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfdc7b0",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b662f413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if it runs first for taking 5000 samples\n",
    "train_set_x_check = train_set_X[:,0:50000]\n",
    "train_set_y_check = train_set_Y[:,0:50000]\n",
    "dimensions = [784, 45, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47b231d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration :  0  is:  2.4246789933912782\n",
      "Loss after iteration :  100  is:  2.140705386693057\n",
      "Loss after iteration :  200  is:  1.9021718576946665\n",
      "Loss after iteration :  300  is:  1.6426449337979832\n",
      "Loss after iteration :  400  is:  1.3946677249053185\n",
      "Loss after iteration :  500  is:  1.1900038053514148\n",
      "Loss after iteration :  600  is:  1.034682048640615\n",
      "Loss after iteration :  700  is:  0.9195562750265724\n",
      "Loss after iteration :  800  is:  0.8333800202193232\n",
      "Loss after iteration :  900  is:  0.7673887147165096\n",
      "Loss after iteration :  1000  is:  0.7155335692969371\n",
      "Loss after iteration :  1100  is:  0.6738343799240519\n",
      "Loss after iteration :  1200  is:  0.639607818150777\n",
      "Loss after iteration :  1300  is:  0.6110357086697551\n",
      "Loss after iteration :  1400  is:  0.5868269556407258\n",
      "Loss after iteration :  1500  is:  0.5660570067311895\n",
      "Loss after iteration :  1600  is:  0.5480198805341189\n",
      "Loss after iteration :  1700  is:  0.5321944271709551\n",
      "Loss after iteration :  1800  is:  0.5182085378639585\n",
      "Loss after iteration :  1900  is:  0.505744370800392\n",
      "Loss after iteration :  2000  is:  0.49455492997669853\n",
      "Loss after iteration :  2100  is:  0.4844436907742953\n",
      "Loss after iteration :  2200  is:  0.4752622961905743\n",
      "Loss after iteration :  2300  is:  0.4668811468162347\n",
      "Loss after iteration :  2400  is:  0.4591924311041027\n",
      "Loss after iteration :  2500  is:  0.4521066254532022\n",
      "Loss after iteration :  2600  is:  0.44555171052937653\n",
      "Loss after iteration :  2700  is:  0.4394604219196518\n",
      "Loss after iteration :  2800  is:  0.4337840848571996\n",
      "Loss after iteration :  2900  is:  0.4284765358639278\n",
      "Loss after iteration :  3000  is:  0.423498319519337\n",
      "Loss after iteration :  3100  is:  0.4188147898334361\n",
      "Loss after iteration :  3200  is:  0.414397117856265\n",
      "Loss after iteration :  3300  is:  0.41022035208781066\n",
      "Loss after iteration :  3400  is:  0.4062620535966226\n",
      "Loss after iteration :  3500  is:  0.40250258179029963\n",
      "Loss after iteration :  3600  is:  0.3989252830811566\n",
      "Loss after iteration :  3700  is:  0.39551579026191913\n",
      "Loss after iteration :  3800  is:  0.3922617494268411\n",
      "Loss after iteration :  3900  is:  0.38915117640225344\n",
      "Loss after iteration :  4000  is:  0.38617110076493383\n",
      "Loss after iteration :  4100  is:  0.38331330358713345\n",
      "Loss after iteration :  4200  is:  0.3805693386763827\n",
      "Loss after iteration :  4300  is:  0.377930728407936\n",
      "Loss after iteration :  4400  is:  0.37538789398280514\n",
      "Loss after iteration :  4500  is:  0.3729361979093359\n",
      "Loss after iteration :  4600  is:  0.370570469873648\n",
      "Loss after iteration :  4700  is:  0.368285106812434\n",
      "Loss after iteration :  4800  is:  0.3660747626736285\n",
      "Loss after iteration :  4900  is:  0.36393367093969414\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_parameters = multi_layer_model(train_set_x_check, train_set_y_check, dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6392a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \n",
    "    # Performs forward propogation using the trained parameters and calculates the accuracy\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = multi_layer_forward(X, parameters)\n",
    "    \n",
    "    p = np.argmax(probas, axis = 0)\n",
    "    act = np.argmax(y, axis = 0)\n",
    "    df = pd.DataFrame({\"probas\":p,\"actual\":act})\n",
    "    print(df)\n",
    "    \n",
    "    print(\"Accuracy: \"  + str(np.sum((p == act)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20be8e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      probas  actual\n",
      "0          1       5\n",
      "1          8       0\n",
      "2          8       4\n",
      "3          4       1\n",
      "4          8       9\n",
      "...      ...     ...\n",
      "4995       8       7\n",
      "4996       3       3\n",
      "4997       8       2\n",
      "4998       8       1\n",
      "4999       8       2\n",
      "\n",
      "[5000 rows x 2 columns]\n",
      "Accuracy: 0.0996\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_set_x_check, train_set_y_check, parameters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
